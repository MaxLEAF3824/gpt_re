{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分析和清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from entity_dataset2 import EntityDataset\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"/home/cs/yangyuchen/yushengliao/Medical_LLM/FastChat/checkpoints/medical_llama_13b_chatv1.3/checkpoint-4974/\")\n",
    "tok.padding_side = 'right'\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "tok.add_tokens([\"[DASH]\"])\n",
    "dst = EntityDataset(data_path='data/kg_usmle_train_subset.json', kg_path='data/bios_kg_with_def.csv', ignore_output=True, size=20,tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "target_index = 1\n",
    "print('target_index: ', target_index)\n",
    "input_ids, attention_mask, labels, hard_position_type_ids, position_ids = dst[target_index]\n",
    "prompt = tok.decode(input_ids)\n",
    "pprint(prompt)\n",
    "print('seq_len: ', input_ids.shape[-1])\n",
    "\n",
    "tokens = tok.batch_decode(input_ids)\n",
    "tokens = [t.replace('\\n', '\\\\n') for t in tokens]\n",
    "tokens = [f\"{t}-{int(hard_position_type_ids[i])}-{position_ids[i]}-{'label' if int(labels[i])!=-100 else 'not'}\" for i,t in enumerate(tokens)]\n",
    "attention_mask = attention_mask.numpy().tolist()\n",
    "attention_mask = [[int(i) for i in row] for row in attention_mask]\n",
    "\n",
    "matrix = attention_mask\n",
    "row_index = tokens\n",
    "col_index = tokens\n",
    "\n",
    "max_row_length = max(len(row) for row in row_index)\n",
    "max_col_length = max(len(col) for col in col_index)\n",
    "\n",
    "# 打开文件\n",
    "with open(f'dst_{target_index}.txt', 'w') as f:\n",
    "    f.write(prompt+ '\\n\\n')\n",
    "    f.write('attention_mask:\\n0:non-entity tokens，1:entity tokens, 2:triplet tokens, 3:triplet target tokens\\n\\n')\n",
    "    # 写入列索引\n",
    "    f.write(\" \" * max_row_length + \"  \" + \"  \".join(col.ljust(max_col_length) for col in col_index) + '\\n')\n",
    "    # 写入行索引和每行的值\n",
    "    for j, row in enumerate(matrix):\n",
    "        f.write(row_index[j].ljust(max_row_length) + \"  \" + \"  \".join(str(val).ljust(max_col_length) for val in row) + '\\n')\n",
    "        \n",
    "# 创建DataFrame对象\n",
    "df = pd.DataFrame(matrix, index=row_index, columns=col_index)\n",
    "\n",
    "# 将DataFrame写入CSV文件\n",
    "df.to_csv(f'dst_{target_index}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取BIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import marisa_trie\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"Loading BIOS...\")\n",
    "# relations = open(\"data/bios_v2.2_release/CoreData/Relations.txt\").readlines()\n",
    "uni_relations = open(\"data/bios_v2.2_release/CoreData/UniRelations.txt\").readlines()\n",
    "concept_terms = open(\"data/bios_v2.2_release/CoreData/ConceptTerms.txt\").readlines()\n",
    "definitions = open(\"data/bios_v2.2_release/CoreData/Definitions.txt\").readlines()\n",
    "semantic_types = open(\"data/bios_v2.2_release/CoreData/SemanticTypes.txt\").readlines()\n",
    "\n",
    "print(\"Building concept2term...\")\n",
    "concept2term = defaultdict(list)\n",
    "for line in tqdm(concept_terms):\n",
    "    ls = line.strip().split('|')\n",
    "    concept2term[ls[0]].append(ls[2])\n",
    "\n",
    "print(\"Building term2concept...\")\n",
    "if os.path.exists('data/term2concept.marisa'):\n",
    "    term2concept = marisa_trie.BytesTrie()\n",
    "    term2concept.load(\"data/term2concept.marisa\")\n",
    "else:\n",
    "    keys = []\n",
    "    values = []\n",
    "    for line in tqdm(concept_terms):\n",
    "        ls = line.strip().split('|')\n",
    "        keys.append(ls[2])\n",
    "        values.append(bytes(ls[0], encoding='utf-8'))\n",
    "    term2concept = marisa_trie.BytesTrie(zip(keys,values))\n",
    "    term2concept.save('data/term2concept.marisa')\n",
    "\n",
    "print(\"Building node2idx...\")\n",
    "node2idx = {}\n",
    "for line in tqdm(uni_relations):\n",
    "    rid, head, tail, relid, rel = line.strip().split(\"|\")\n",
    "    if not node2idx.get(head):\n",
    "        node2idx[head] = len(node2idx)\n",
    "    if not node2idx.get(tail):\n",
    "        node2idx[tail] = len(node2idx)\n",
    "\n",
    "print(\"Building idx2node...\")\n",
    "idx2node = [x[0] for x in sorted(node2idx.items(), key=lambda x: x[1])]\n",
    "\n",
    "print(\"Building edges...\")\n",
    "edges = {}\n",
    "for rl in tqdm(uni_relations):\n",
    "    ls = rl.strip().split(\"|\")\n",
    "    edges[(node2idx[ls[1]],node2idx[ls[2]])]=ls[-1]\n",
    "\n",
    "print(\"Building graph...\")\n",
    "g = ig.Graph(n=len(node2idx), edges=list(edges.keys()), directed=True)\n",
    "g.simplify()\n",
    "\n",
    "print(\"Adding node descriptions...\")\n",
    "contains_chinese = lambda text:  bool(re.compile(r'[\\u4e00-\\u9fa5]').search(text))\n",
    "for i in tqdm(range(len(g.vs))):\n",
    "    g.vs[i][\"cid\"] = idx2node[i]\n",
    "    g.vs[i][\"name\"] = concept2term[idx2node[i]][0]\n",
    "    color = \"blue\"\n",
    "    neighbors_num = len(g.neighbors(i))\n",
    "    if neighbors_num > 10:\n",
    "        color = \"green\"\n",
    "    if neighbors_num > 50:\n",
    "        color = \"orange\"\n",
    "    if neighbors_num > 100:\n",
    "        color = \"red\"\n",
    "    if neighbors_num > 1000:\n",
    "        color = \"purple\"\n",
    "    g.vs[i][\"color\"] = color\n",
    "    for term in concept2term[idx2node[i]]:\n",
    "        if contains_chinese(term):\n",
    "            g.vs[i][\"name\"] = term\n",
    "            break\n",
    "\n",
    "print(\"Adding edge descriptions...\")\n",
    "for i in tqdm(range(len(g.es))):\n",
    "    s,t = g.es[i].source, g.es[i].target\n",
    "    g.es[i][\"name\"] = edges[(s,t)]\n",
    "\n",
    "undi_g = g.as_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize']=(15,15)\n",
    "\n",
    "def get_cid(str):\n",
    "    if term2concept.get(str.lower()):\n",
    "        return term2concept[str.lower()][0].decode('utf-8')\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_n_hop_neighbors_subgraph(str,n=2):\n",
    "    cid = get_cid(str)\n",
    "    if not cid:\n",
    "        print(\"Entity Not found in KG\")\n",
    "        return\n",
    "    print(f'cid: {cid} idx: {node2idx[cid]} term: {g.vs[node2idx[cid]][\"name\"]}')\n",
    "\n",
    "    root = node2idx[cid]\n",
    "    g.vs[root]['hop'] = 0\n",
    "    all_neighbors = [root]\n",
    "    for i in range(1,n+1):\n",
    "        i_hop_neighbors = []\n",
    "        for node in all_neighbors:\n",
    "            node_neighbors = g.neighbors(node)\n",
    "            if len(node_neighbors) > 100:\n",
    "                continue\n",
    "            node_neighbors = node_neighbors[:min(len(node_neighbors), 5)]\n",
    "            i_hop_neighbors.extend(node_neighbors)\n",
    "        i_hop_neighbors = list(set(i_hop_neighbors))\n",
    "        print(f'{i}_hop_neighbors num: {len(i_hop_neighbors)}')\n",
    "        for x in i_hop_neighbors:\n",
    "            if 'hop' not in g.vs[x].attributes().keys() or g.vs[x]['hop'] is None:\n",
    "                g.vs[x]['hop'] = i\n",
    "        all_neighbors.extend(i_hop_neighbors)\n",
    "        all_neighbors = list(set(all_neighbors))\n",
    "    print('all_neighbors: ', all_neighbors)\n",
    "    subgraph = g.subgraph(all_neighbors)\n",
    "    for edge in subgraph.es:\n",
    "        if abs(subgraph.vs[edge.source]['hop'] - subgraph.vs[edge.target]['hop']) != 1:\n",
    "            subgraph.delete_edges(edge)\n",
    "    for node in all_neighbors:\n",
    "        if 'hop' in g.vs[node].attributes():\n",
    "            del g.vs[node]['hop']\n",
    "    return subgraph\n",
    "\n",
    "def get_n_str_subgraph(str_list):\n",
    "    node_idxs = list(set([node2idx[get_cid(str)] for str in str_list if get_cid(str)]))\n",
    "    print('node_idxs: ', node_idxs)\n",
    "    old_colors = [g.vs[i]['color'] for i in node_idxs]\n",
    "    for i in node_idxs:\n",
    "        g.vs[i]['color'] = 'white'\n",
    "    paths = []\n",
    "    for i in range(len(node_idxs)):\n",
    "        for j in range(i+1, len(node_idxs)):\n",
    "            paths.extend(g.get_shortest_paths(node_idxs[i], node_idxs[j], mode=ig.ALL))\n",
    "    all_nodes = list(set([n for path in paths for n in path]))\n",
    "    print('node num: ', len(all_nodes))\n",
    "    subgraph = g.subgraph(all_nodes)\n",
    "    for i,color in zip(node_idxs,old_colors):\n",
    "        g.vs[i]['color'] = color\n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cid(\"SAH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept2term.get(get_cid(\"PNA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = get_n_str_subgraph([\"periodontal ligament\",\n",
    "            \"Hyalinization\",\n",
    "            \"Osteoclastic activity\",\n",
    "            \"tooth\",\n",
    "            \"Crest bone resorption\"])\n",
    "ig.plot(subgraph,vertex_label=subgraph.vs[\"name\"], edge_label=subgraph.es[\"name\"], layout=subgraph.layout(\"kk\"),backend='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = get_n_hop_neighbors_subgraph(\"tropicamide\", 2)\n",
    "ig.plot(subgraph,vertex_label=subgraph.vs[\"name\"], edge_label=subgraph.es[\"name\"], layout=subgraph.layout(\"kk\"),backend='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = g.pagerank(directed=True, damping=0.85, )\n",
    "sorted_indices = sorted(range(len(rank)), key=lambda i: rank[i], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print('Rank: %d, Score: %.4f, Node: %s' % (i+1, rank[sorted_indices[i]], g.vs[sorted_indices[i]]['name']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
