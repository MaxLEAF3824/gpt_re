{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model.model_interface import LLM\n",
    "from dataset.knowns import Knowns\n",
    "import torch.utils.data as tud\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "from gpthook import TraceDict\n",
    "import os \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Config\n",
    "llm_config = {\n",
    "    \"model_name\": \"gpt2\",\n",
    "}\n",
    "\n",
    "# Dataset config\n",
    "dl_config = {\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "data_dir= \"data\"\n",
    "size = 1000\n",
    "\n",
    "# Trainer config\n",
    "trainer_config = {\n",
    "    \"precision\" : 16,\n",
    "    \"accelerator\" : \"auto\",\n",
    "    \"devices\" : 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1000 elements\n"
     ]
    }
   ],
   "source": [
    "mt = LLM(**llm_config)\n",
    "dst = Knowns(data_dir, mt.tokenizer, size)\n",
    "dl = tud.DataLoader(dst, **dl_config, collate_fn=dst.collate_fn)\n",
    "trainer = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,a,l in dl:\n",
    "    with torch.no_grad(), TraceDict(mt.model, device=\"cpu\") as clean_td:\n",
    "        logits = mt.predict(input_ids, attention_mask=attention_mask, output_attentions=True)['logits'] # [bsz, seq, vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comp_flow(mt:LLM, batch:tuple, comp_key, comp_kind):\n",
    "    \"\"\"batch_size equal 1\"\"\"\n",
    "    model, tokenizer = mt.model, mt.tokenizer\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_tokens = [tokenizer.decode([t]) for t in input_ids.squeeze()]\n",
    "    # get clean td\n",
    "    with torch.no_grad(), TraceDict(mt.model, device=\"cpu\") as clean_td:\n",
    "        logits = mt.predict_step(input_ids, attention_mask=attention_mask, output_attentions=True)['logits'] # [bsz, seq, vocab]\n",
    "    clean_prob = F.softmax(logits, dim=-1)\n",
    "    gt_idx = torch.argmax(clean_prob[:,-1,:], dim=-1)\n",
    "    answer = mt.tokenizer.decode(gt_idx)\n",
    "    gt_prob = clean_prob[:,-1,gt_idx]\n",
    "    \n",
    "    x0 = clean_td[\"block_0\"].input\n",
    "    table = []\n",
    "    attn_weight_diff = []\n",
    "    for layer in range(1, model.config.n_layer):\n",
    "        if comp_key == \"attn\":\n",
    "            comp = clean_td[f\"{comp_key}_{layer - 1}\"].output[0] \n",
    "        else:\n",
    "            comp = clean_td[f\"{comp_key}_{layer - 1}\"].output\n",
    "        comp = comp.to(device)\n",
    "        column = []\n",
    "        for t_idx in range(len(inp['input_ids'][0])):\n",
    "            prob, td = trace_comp_patch(model, inp, x0, layer, [t_idx], comp, comp_kind)\n",
    "            column.append(gt_prob - prob[:,-1,gt_idx])\n",
    "        column = torch.vstack(column)\n",
    "        table.append(column)\n",
    "        # corrupt all tokens\n",
    "        t_idxs = list(range(len(inp['input_ids'][0])))\n",
    "        prob, td = trace_comp_patch(model, inp, x0, layer, t_idxs, comp, comp_kind, output_attentions=True)\n",
    "        attn_weight_o, attn_weight_fixed = td[f'attn_{layer}'].output[2]\n",
    "        device2 = attn_weight_fixed.device\n",
    "        # pdb.set_trace()\n",
    "        attn_weight_diff.append((attn_weight_o-attn_weight_fixed).abs().sum(dim=-1).sum(dim=-1))\n",
    "    attn_weight_diff = torch.vstack(attn_weight_diff)\n",
    "    table = torch.stack(table).squeeze()\n",
    "    return {\"table\":table.transpose(0,1).cpu(),\n",
    "            \"comp_key\":comp_key,\n",
    "            \"comp_kind\":comp_kind,\n",
    "            \"input_tokens\": input_tokens,  \n",
    "            \"answer\":answer,\n",
    "            \"attn_weight_diff\":attn_weight_diff.cpu()}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
