{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model.model_interface import LLM\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.my_utils import *\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import regex as re\n",
    "from dataset import *\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Union, List\n",
    "\n",
    "pl.seed_everything(42)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "\n",
    "model_list = list({\n",
    "    \"gpt2\": \"/nvme/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8\",\n",
    "    \"gpt2_xl\": \"/nvme/guoyiqiu/coding/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8\",\n",
    "    \"llama_7b\": \"/nvme/share/guoyiqiu/llama-7b\",\n",
    "    \"llama_13b\": \"/nvme/share/guoyiqiu/llama-13b\",\n",
    "    \"vicuna_7b\": \"/nvme/share/guoyiqiu/vicuna-7b\",\n",
    "    \"vicuna_13b\": \"/nvme/share/guoyiqiu/vicuna-13b-v1.1\",\n",
    "}.items())\n",
    "\n",
    "llm_config = {\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"lr\": 1e-4,\n",
    "}\n",
    "\n",
    "hook_config = {\n",
    "    \"retain_output\": True,\n",
    "    \"retain_input\": False,\n",
    "    \"edit_output\": None,\n",
    "    \"clone\": True,\n",
    "    \"float\": True,\n",
    "    \"detach\": True,\n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "def init_mt():\n",
    "    global mt\n",
    "    mt = LLM(model_name=mt_dropdown.value, fp16=precision_tbtn.value == \"half\", **llm_config)\n",
    "\n",
    "\n",
    "def init_modules():\n",
    "    global n_layer\n",
    "    global lm_head\n",
    "    global embedding\n",
    "    global ln_f\n",
    "    global blocks\n",
    "    global ATTN\n",
    "    global MLP\n",
    "    global LN1\n",
    "    global LN2\n",
    "    if \"gpt2\" in mt.model.__class__.__name__.lower():\n",
    "        # gpt2 config\n",
    "        n_layer = mt.model.config.num_hidden_layers\n",
    "        lm_head = mt.model.lm_head\n",
    "        embedding = mt.model.transformer.wte\n",
    "        ln_f = mt.model.transformer.ln_f\n",
    "        blocks = mt.model.transformer.h\n",
    "        ATTN = 'attn'\n",
    "        MLP = 'mlp'\n",
    "        LN1 = 'ln_1'\n",
    "        LN2 = 'ln_2'\n",
    "    elif \"llama\" in mt.model.__class__.__name__.lower():\n",
    "        # llama config\n",
    "        n_layer = mt.model.config.num_hidden_layers\n",
    "        lm_head = mt.model.lm_head\n",
    "        embedding = mt.model.model.embed_tokens\n",
    "        ln_f = mt.model.model.norm\n",
    "        blocks = mt.model.model.layers\n",
    "        ATTN = 'self_attn'\n",
    "        MLP = 'mlp'\n",
    "        LN1 = 'input_layernorm'\n",
    "        LN2 = 'post_self_attn_layernorm'\n",
    "        \n",
    "\n",
    "\n",
    "def init_hook(mt):\n",
    "    mt.clear_hook()\n",
    "    for i in range(n_layer):\n",
    "        mt.add_hook(module=blocks[i], name=f\"block_{i}\", **hook_config)\n",
    "        mt.add_hook(module=getattr(blocks[i], ATTN), name=f\"attn_{i}\", **hook_config)\n",
    "        mt.add_hook(module=getattr(blocks[i], MLP), name=f\"mlp_{i}\", **hook_config)\n",
    "\n",
    "\n",
    "def setup(btn):\n",
    "    time_st = time.time()\n",
    "    btn.description = \"Loading model...\"\n",
    "    init_mt()\n",
    "    btn.description = \"init modules...\"\n",
    "    init_modules()\n",
    "    btn.description = \"init hooks...\"\n",
    "    init_hook(mt)\n",
    "    btn.description = \"Everything is ready.\"\n",
    "    device_tbtn.value = 'cpu'\n",
    "    print(f\"Time cost: {time.time() - time_st:.2f}s\")\n",
    "\n",
    "# setup widgets\n",
    "\n",
    "\n",
    "# model dropdown\n",
    "mt_dropdown = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# setup button\n",
    "setup_btn = widgets.Button(\n",
    "    description=\"Setup everything\",\n",
    "    disabled=False,\n",
    ")\n",
    "setup_btn.on_click(setup)\n",
    "\n",
    "# switch deivce\n",
    "device_tbtn = widgets.ToggleButtons(\n",
    "    options=['cpu', f'cuda',],\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def switch_device(change):\n",
    "    device_tbtn.disabled = True\n",
    "    mt.model.to(change.new)\n",
    "    torch.cuda.empty_cache() if change.new == 'cpu' else None\n",
    "    device_tbtn.disabled = False\n",
    "\n",
    "\n",
    "device_tbtn.observe(switch_device, names='value')\n",
    "\n",
    "# switch precision\n",
    "\n",
    "precision_tbtn = widgets.ToggleButtons(\n",
    "    options=['float', 'half'],\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def switch_precision(change):\n",
    "    precision_tbtn.disabled = True\n",
    "    if mt is not None:\n",
    "        mt.model = mt.model.half() if change.new == 'half' else mt.model.float()\n",
    "        init_modules()\n",
    "    precision_tbtn.disabled = False\n",
    "\n",
    "\n",
    "precision_tbtn.observe(switch_precision, names='value')\n",
    "\n",
    "\n",
    "mnt_slider = widgets.IntSlider(\n",
    "    value=128,\n",
    "    min=1,\n",
    "    max=512,\n",
    "    step=1,\n",
    "    description='new token:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "\n",
    "input_textarea = widgets.Textarea(\n",
    "    value='',\n",
    "    description='Input:',\n",
    "    layout=widgets.Layout(width='30%', height='250px'),\n",
    "    disabled=False\n",
    ")\n",
    "output_textarea = widgets.Textarea(\n",
    "    value='',\n",
    "    description='Output:',\n",
    "    layout=widgets.Layout(width='30%', height='250px'),\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "submit_btn = widgets.Button(\n",
    "    description=\"generate\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def generate(btn):\n",
    "    input_text = input_textarea.value\n",
    "    max_new_tokens = mnt_slider.value\n",
    "    btn.disabled = True\n",
    "    submit_btn.description = \"Generating...\"\n",
    "    result = mt.generate(input_text, max_new_tokens=max_new_tokens)\n",
    "    btn.disabled = False\n",
    "    submit_btn.description = \"generate\"\n",
    "    output_text = result[0]\n",
    "    output_textarea.value = output_text\n",
    "\n",
    "\n",
    "submit_btn.on_click(generate)\n",
    "\n",
    "control_panel = widgets.HBox([mt_dropdown, setup_btn, precision_tbtn, device_tbtn])\n",
    "talk_panel = widgets.HBox([input_textarea, widgets.VBox([mnt_slider, submit_btn]), output_textarea])\n",
    "all_panel = widgets.VBox([control_panel, talk_panel])\n",
    "display(all_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mt\u001b[39m.\u001b[39mmodel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mt' is not defined"
     ]
    }
   ],
   "source": [
    "mt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_tbtn.value = 'float'\n",
    "mt_dropdown.index = 4\n",
    "setup_btn.click()\n",
    "# precision_tbtn.value = 'half'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LORA Tune MedQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "mt.model = get_peft_model(mt.model, peft_config)\n",
    "mt.model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lite Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_all(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def set_module_requires_grad(model, layers: Union[int, List[int]], names: Union[str, List[str]], requires_grad: bool):\n",
    "    layers = [layers] if isinstance(layers, int) else layers\n",
    "    names = [names] if isinstance(names, str) else names\n",
    "    for layer in layers:\n",
    "        for name in names:\n",
    "            assert name in [ATTN, MLP, LN1, LN2]\n",
    "            module = getattr(blocks[layer], name)\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "def my_training_step(self, batch, batch_idx):\n",
    "    '''batch: (input_ids, attention_mask, labels) **padding already** '''\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.unsqueeze(0) if len(input_ids.shape) == 1 else input_ids\n",
    "    attention_mask = attention_mask.unsqueeze(0) if len(attention_mask.shape) == 1 else attention_mask\n",
    "    labels = labels.unsqueeze(0) if len(labels.shape) == 1 else labels\n",
    "    gt_id = labels[0, -1].item()\n",
    "\n",
    "    bsz = input_ids.shape[0]\n",
    "    assert bsz == 1\n",
    "    \n",
    "    def set_require_grad(module, input, output):\n",
    "        ''' output: (bsz, seq_len, hidden_size) '''\n",
    "        with torch.no_grad():\n",
    "            topk_logits, topk_indices = torch.topk(lm_head(ln_f(output[0])), k=10, dim=-1) # [bsz, seq_len, k]\n",
    "        is_important = gt_id in topk_indices\n",
    "        if is_important:\n",
    "            print(f'{module.name} is_important')\n",
    "            # for param in module.parameters():\n",
    "            #     param.requires_grad = True\n",
    "        return output\n",
    "    \n",
    "    self.clear_hook()\n",
    "    hook_config = {\n",
    "        \"retain_output\": False,\n",
    "        \"retain_input\": False,\n",
    "        \"edit_output\": set_require_grad,\n",
    "        \"clone\": False,\n",
    "        \"float\": False,\n",
    "        \"detach\": False,\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "    for i in range(n_layer):\n",
    "        self.add_hook(module=getattr(blocks[i], ATTN), name=f\"attn_{i}\", **hook_config)\n",
    "    print(batch_idx)\n",
    "    res = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    lm_logits = res['logits']\n",
    "    shift_logits = lm_logits[..., :-1, :].contiguous()  # Shift so that tokens < n predict n\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    if isinstance(res.get('loss'), torch.Tensor):\n",
    "        loss = res['loss']\n",
    "    else:\n",
    "        loss = self.loss_func(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    acc = self._acc(shift_logits, shift_labels)\n",
    "\n",
    "    self.log('train_loss', loss, on_step=True, on_epoch=True, sync_dist=True, prog_bar=True)\n",
    "    self.log('train_acc', acc, on_step=False, sync_dist=True, on_epoch=True, prog_bar=True)\n",
    "    print(\"\\n\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 1\n",
    "\n",
    "train_dst = MedQA('/nvme/guoyiqiu/coding/datasets/MedQA/data_clean/questions/US/train.jsonl',tokenizer=mt.tokenizer, max_len=512,size=1000)\n",
    "train_dl = DataLoader(train_dst, batch_size=bsz, shuffle=True, collate_fn=train_dst.collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"devices\": [1],\n",
    "    \"enable_checkpointing\":False,\n",
    "    'accumulate_grad_batches': 32,\n",
    "    \"max_epochs\":3,\n",
    "}\n",
    "\n",
    "mt.clear_hook()\n",
    "mt.set_func('training_step', my_training_step)\n",
    "freeze_all(mt.model)\n",
    "set_module_requires_grad(mt.model, list(range(n_layer)), ATTN, True)\n",
    "# trainer = pl.Trainer(**trainer_config, logger=WandbLogger(project='tune medqa', name='litetune_5ep_vicuna7b'))\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "trainer.fit(mt, train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
