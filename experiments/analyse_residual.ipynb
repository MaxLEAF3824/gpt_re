{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185e5d62920b4a9082c1db48652d6b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', '/mnt/workspace/guoyiqiu/coding…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model import *\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.my_utils import *\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import regex as re\n",
    "from dataset import *\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Union, List\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model_list = [\n",
    "    (\"gpt2\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8\"),\n",
    "    (\"gpt2-xl\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8\"),\n",
    "    (\"gpt2-medium\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d\"),\n",
    "    (\"llama_7b\", \"/nvme/share/guoyiqiu/llama-7b\"),\n",
    "    (\"llama_13b\", \"/nvme/share/guoyiqiu/llama-13b\"),\n",
    "    (\"vicuna_7b\", \"/mnt/workspace/guoyiqiu/coding/vicuna_7b\"),\n",
    "    (\"vicuna_13b\", \"/mnt/workspace/guoyiqiu/coding/vicuna-13b-v1.1\"),\n",
    "    (\"book_7b\", \"/mnt/workspace/guoyiqiu/coding/Book_7B/checkpoint-4968\"),\n",
    "    (\"book_13b\", \"/home/cs/yangyuchen/guoyiqiu/kg_llm/output/full_book_13b_bsz1_epoch3_lr1e-05\"),\n",
    "]\n",
    "\n",
    "\n",
    "def setup_widgets(model_list):\n",
    "    global mt_dropdown\n",
    "    global setup_btn\n",
    "    global device_tbtn\n",
    "    global precision_tbtn\n",
    "    global mnt_slider\n",
    "    global input_textarea\n",
    "    global output_textarea\n",
    "    global submit_btn\n",
    "    global chat_checkbox\n",
    "    global sample_checkbox\n",
    "    global model\n",
    "    global tok\n",
    "    global mt\n",
    "    \n",
    "    def setup_llm(btn):\n",
    "        global mt\n",
    "        global vis\n",
    "        global model\n",
    "        global tok\n",
    "        time_st = time.time()\n",
    "        btn.description = \"Loading model...\"\n",
    "        mt = LLM.from_pretrained(model_name=mt_dropdown.value, fp16=(precision_tbtn.value == \"half\"),)\n",
    "        btn.description = \"Everything is ready.\"\n",
    "        device_tbtn.value = 'cpu'\n",
    "        model = mt.model\n",
    "        tok = mt.tokenizer\n",
    "        print(f\"Time cost: {time.time() - time_st:.2f}s\")\n",
    "    \n",
    "    def switch_device(change):\n",
    "        device_tbtn.disabled = True\n",
    "        mt.to(change.new)\n",
    "        torch.cuda.empty_cache() if change.new == 'cpu' else None\n",
    "        device_tbtn.disabled = False\n",
    "\n",
    "    def switch_precision(change):\n",
    "        precision_tbtn.disabled = True\n",
    "        if mt is not None:\n",
    "            mt.model = mt.model.half() if change.new == 'half' else mt.model.float()\n",
    "        precision_tbtn.disabled = False\n",
    "\n",
    "    def generate(btn):\n",
    "        CHAT_TEMPLATE = \"You are a friendly assistant, chat with human.###Human: {}###Assistant:\"\n",
    "        btn.disabled = True\n",
    "        submit_btn.description = \"Generating...\"\n",
    "        input_text = CHAT_TEMPLATE.format(input_textarea.value) if chat_checkbox.value else input_textarea.value\n",
    "        gen_kwargs = {\n",
    "            \"input_texts\":input_text,\n",
    "            \"max_new_tokens\":mnt_slider.value,\n",
    "            \"do_sample\": sample_checkbox.value,\n",
    "        }\n",
    "        result = mt.generate(**gen_kwargs)\n",
    "        btn.disabled = False\n",
    "        submit_btn.description = \"generate\"\n",
    "        output_text = result[0].replace(input_text, \"\") if chat_checkbox.value else result[0]\n",
    "        output_textarea.value = output_text\n",
    "\n",
    "    # model dropdown\n",
    "    mt_dropdown = widgets.Dropdown(options=model_list, description='Model:', disabled=False,)\n",
    "\n",
    "    # setup button\n",
    "    setup_btn = widgets.Button(description=\"Setup everything\", disabled=False,)\n",
    "    setup_btn.on_click(setup_llm)\n",
    "\n",
    "    # switch deivce\n",
    "    device_tbtn = widgets.ToggleButtons(options=['cpu', f'cuda',], disabled=False,)\n",
    "    device_tbtn.observe(switch_device, names='value')\n",
    "\n",
    "    # switch precision\n",
    "    precision_tbtn = widgets.ToggleButtons(options=['float', 'half'], disabled=False,)\n",
    "    precision_tbtn.observe(switch_precision, names='value')\n",
    "\n",
    "    # max new token slider\n",
    "    mnt_slider = widgets.IntSlider(value=64,min=1,max=512,step=1,description='new token:',disabled=False,)\n",
    "    \n",
    "    # sample checkbox\n",
    "    sample_checkbox = widgets.Checkbox(value=False,description='do sample',disabled=False,)\n",
    "    \n",
    "    # input and output textarea\n",
    "    input_textarea = widgets.Textarea(value='',description='Input:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "    output_textarea = widgets.Textarea(value='',description='Output:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "\n",
    "    # submit button\n",
    "    submit_btn = widgets.Button(description=\"generate\",disabled=False,)\n",
    "    submit_btn.on_click(generate)\n",
    "\n",
    "    # chat mode checkbox\n",
    "    chat_checkbox = widgets.Checkbox(value=False,description='chat mode',disabled=False,)\n",
    "    \n",
    "    # pannel layout\n",
    "    control_panel = widgets.HBox([mt_dropdown, setup_btn, precision_tbtn, device_tbtn])\n",
    "    generate_panel = widgets.HBox([input_textarea, widgets.VBox([mnt_slider, sample_checkbox, chat_checkbox, submit_btn]), output_textarea])\n",
    "    all_panel = widgets.VBox([control_panel, generate_panel])\n",
    "    display(all_panel)\n",
    "\n",
    "setup_widgets(model_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt num: 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/hug42/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a2dfb72f5f472b89378707323e5843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "606151"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute = [\"body temperature\", \n",
    "             \"diastolic blood pressure\", \n",
    "             \"systolic blood pressure\",\n",
    "             \"heart rate\",\n",
    "             \"respiratory rate\"\n",
    "             \"hemoglobin for male\",\n",
    "             \"hemoglobin for female\",\n",
    "             \"body mass index\",\n",
    "             \"pulse rate\", \n",
    "             \"platelet count\",\n",
    "             \"white blood cell count\",\n",
    "             \"fasting blood glucose\"\n",
    "             ]\n",
    "              \n",
    "test_range = [[round(i,1) for i in np.arange(0,50,1)], # 体温\n",
    "              list(range(0,200,4)), # 舒张压\n",
    "              list(range(0,200,4)), # 收缩压\n",
    "              list(range(0,200,4)), # 心率\n",
    "              list(range(0,100,2)), # 呼吸率\n",
    "              list(range(0,200,4)), # 血红蛋白 男\n",
    "              list(range(0,200,4)), # 血红蛋白 女\n",
    "              [round(i,1) for i in np.arange(5,35,0.6)], # BMI\n",
    "              list(range(0,200,4)), # 脉搏\n",
    "              list(range(0,500,10)), # 血小板\n",
    "              [round(i,1) for i in np.arange(0,20,0.4)], # 白细胞\n",
    "              [round(i,1) for i in np.arange(0,10,0.2)], # 空腹血糖\n",
    "              ]\n",
    "\n",
    "unit = [\"degree celsius\", \n",
    "        \"mmHg\", \n",
    "        \"mmHg\", \n",
    "        \"bpm\", \n",
    "        \"breaths/min\",\n",
    "        \"g/L\",\n",
    "        \"g/L\",\n",
    "        \"kg/m²\",\n",
    "        \"bpm\",\n",
    "        \"×10^9/L\",\n",
    "        \"×10^9/L\",\n",
    "        \"mmol/L\",]\n",
    "\n",
    "import json\n",
    "\n",
    "dst = json.load(open(\"/mnt/workspace/guoyiqiu/coding/my_rome/numeric_attributes.json\"))\n",
    "\n",
    "\n",
    "reason_prompt = \"Analyse the medical attribute. A person's {attribute} is {num} {unit}.\\n\\nAnalysis:\"\n",
    "\n",
    "dst = []\n",
    "for (attribute, test_range, unit) in zip(attribute, test_range, unit):\n",
    "    for num in test_range:\n",
    "        dst.append(reason_prompt.format(attribute=attribute, num=num, unit=unit))\n",
    "\n",
    "\n",
    "print(f\"prompt num: {len(dst)}\")\n",
    "\n",
    "def batch_text2text(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "    batch = batch if isinstance(batch, list) else [batch]\n",
    "    inps = self.tokenizer(batch, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask = inps['input_ids'], inps['attention_mask']\n",
    "    len_input_ids = input_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(self.model.device)\n",
    "        attention_mask = attention_mask.to(self.model.device)\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"do_sample\": False,\n",
    "        }\n",
    "        output_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n",
    "        output_ids = output_ids[:, len_input_ids:]\n",
    "    output_text = self.tokenizer.batch_decode(output_ids)\n",
    "    return list(output_text)\n",
    "\n",
    "mt.set_func(\"predict_step\", batch_text2text)\n",
    "mt.clear_hook()\n",
    "trainer_config = {\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"accelerator\": \"auto\",\n",
    "    # \"strategy\": \"auto\",\n",
    "    # \"devices\" : [0,1,2,3,4,5,6,7],\n",
    "    \"devices\" : [0],\n",
    "}\n",
    "dl = DataLoader(dst, batch_size=16, shuffle=False, num_workers=0)\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "res = trainer.predict(mt, dl)\n",
    "\n",
    "res = [item for sublist in res for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "open(\"result2.txt\", \"w\").write(\"\\n-------------------------------------------\\n\".join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llb = 0\n",
    "lb = 30\n",
    "ub = 50\n",
    "uub = 100\n",
    "prompt_tmp = \"A patient's diastolic blood pressure is {} mmHg, which is considered to be\"\n",
    "\n",
    "normal_probs = []\n",
    "low_probs = []\n",
    "high_probs = []\n",
    "from tqdm.auto import tqdm\n",
    "for i in tqdm(range(llb,uub)):\n",
    "    prob = torch.softmax(model(tok(prompt_tmp.format(i), return_tensors=\"pt\").input_ids.to(model.device),)['logits'][0,-1],dim=-1)\n",
    "    low_probs.append(prob[tok('low').input_ids[1]].item())\n",
    "    normal_probs.append(prob[tok('normal').input_ids[1]].item())\n",
    "    high_probs.append(prob[tok('high').input_ids[1]].item())\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('/mnt/workspace/guoyiqiu/coding')\n",
    "from gpt_re.utils.my_utils import *\n",
    "print(f\"llb-lb: low {np.mean(low_probs[:lb]):.4f} normal {np.mean(normal_probs[:lb]):.4f} high {np.mean(high_probs[:lb]):.4f}\")\n",
    "print(f\"lb-ub:  low {np.mean(low_probs[lb:ub]):.4f} normal {np.mean(normal_probs[lb:ub]):.4f} high {np.mean(high_probs[lb:ub]):.4f}\")\n",
    "print(f\"ub-uub: low {np.mean(low_probs[ub:]):.4f} normal {np.mean(normal_probs[ub:]):.4f} high {np.mean(high_probs[ub:]):.4f}\")\n",
    "\n",
    "plotly_bar(low_probs,f\"low{np.mean(high_probs)}\")\n",
    "plotly_bar(normal_probs,f\"normal{np.mean(normal_probs)}\")\n",
    "plotly_bar(high_probs,f\"high{np.mean(high_probs)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.charts import Bar, Timeline, Tab, Page, Line\n",
    "from pyecharts.faker import Faker\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "SAVED_MODULES = ['layer', 'attn', 'mlp']\n",
    "\n",
    "\n",
    "class Unembedding(nn.Module):\n",
    "    def __init__(self, lm_head, ln_f):\n",
    "        super().__init__()\n",
    "        self.lm_head = lm_head\n",
    "        self.ln_f = ln_f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.ln_f(x)\n",
    "            x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "class FlowVisualizer:\n",
    "    def __init__(self, mt: LLM):\n",
    "        self.mt = mt\n",
    "        self.idx2token = [f\"{i}-{self.mt.tokenizer.decode(i)}\" for i in range(self.mt.tokenizer.vocab_size)]\n",
    "        self.unembedding = Unembedding(deepcopy(mt.lm_head).to('cpu').float(), deepcopy(mt.ln_f).to('cpu').float())\n",
    "        self.init_save_hook()\n",
    "        self.sentences = [] # generated sentences\n",
    "        self.next_tokens = [] # next token of sentences\n",
    "        self.prompt_lengths = [] # prompt length of sentences\n",
    "        self.utokens = [] # 对于每个句子，都有seq_len个token，每个token都有一个vocab_size大小的utoken list [bsz, seq_len, vocab_size]\n",
    "        self.uprobs = [] # [bsz, 3, seq_len, n_layer, vocab_size]\n",
    "        self.infos = [] # 对于每个句子，每个模块每一层每个token的uprob信息熵 [bsz, 3, n_layer, seq_len]\n",
    "        self.diffs = [] # 对于每个句子，每个模块每一层每个token的uprob关于上一层的uprob的交叉熵 [bsz, 3, n_layer, seq_len]\n",
    "        \n",
    "    def init_save_hook(self):\n",
    "        self.mt.clear_hook()\n",
    "        hook_config = {\n",
    "            \"retain_output\": True,\n",
    "            \"retain_input\": False,\n",
    "            \"edit_output\": None,\n",
    "            \"clone\": True,\n",
    "            \"float\": True,\n",
    "            \"detach\": True,\n",
    "            \"device\": \"cpu\"\n",
    "        }\n",
    "        for h in SAVED_MODULES:\n",
    "            for l in range(self.mt.n_layer):\n",
    "                hook_config['retain_input'] = (l == 0 and h == 'layer') # 只保留Layer第一层的输入\n",
    "                self.mt.add_hook(module=getattr(self.mt, h+'s')[l], name=f'{h}_{l}', **hook_config)\n",
    "\n",
    "    def get_sentence_matrix(self, sidx):\n",
    "        '''return matrix of sentence sidx with shape of [3, n_layer, seq_len, hidden_size]'''\n",
    "        cur_matrix = torch.stack([torch.cat([self.mt.hooks[f'{h}_{l}'].outputs[sidx] for l in range(self.mt.n_layer)], dim=0) for h in SAVED_MODULES])\n",
    "        return cur_matrix\n",
    "    \n",
    "    def get_x0(self, sidx):\n",
    "        return self.mt.hooks['layer_0'].inputs[sidx]# [1, seq_len, hidden_size]\n",
    "    \n",
    "    def generate(self, input_texts, **gen_wargs):\n",
    "        input_texts = input_texts if isinstance(input_texts, list) else [input_texts]\n",
    "        inps = [self.mt.tokenizer(text, return_tensors='pt') for text in input_texts]\n",
    "        \n",
    "        for inp in tqdm(inps, total=len(inps)):\n",
    "            input_ids, attention_mask = inp['input_ids'], inp['attention_mask']\n",
    "            self.prompt_lengths.append(input_ids.shape[1])\n",
    "\n",
    "            # model generate\n",
    "            hook_idxs = [len(h.outputs) for h in self.mt.hooks.values()]\n",
    "            with torch.no_grad():\n",
    "                input_ids = input_ids.to(self.mt.model.device)\n",
    "                attention_mask = attention_mask.to(self.mt.model.device)\n",
    "                gen_wargs['max_new_tokens'] = 10 if 'max_new_tokens' not in gen_wargs else gen_wargs['max_new_tokens']\n",
    "                output_ids = self.mt.model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_wargs)\n",
    "            \n",
    "            # 模型会在generate的过程中多次forward产生多个hook中间值，需要把hook的输出拼接起来得到完整的句子的matrix\n",
    "            for (hook, idx) in zip(self.mt.hooks.values(), hook_idxs):\n",
    "                hook.outputs[idx] = torch.cat([o for o in hook.outputs[idx:]], dim=1)\n",
    "                hook.outputs = hook.outputs[:idx+1]\n",
    "                if hook.retain_input:\n",
    "                    hook.inputs[idx] = torch.cat([o for o in hook.inputs[idx:]], dim=1)\n",
    "                    hook.inputs = hook.inputs[:idx+1]\n",
    "            \n",
    "            # 保存generate的句子和下一个token\n",
    "            out_tokens = self.mt.tokenizer.batch_decode(output_ids[0])\n",
    "            self.sentences.append(out_tokens[:-1])\n",
    "            self.next_tokens.append(out_tokens[-1])\n",
    "            \n",
    "            # 获取当前句子的关于每一层，每一个模块合并后的完整matrix [3, n_layer, seq_len, hidden_size]\n",
    "            cur_matrix = self.get_sentence_matrix(-1)\n",
    "            seq_len = cur_matrix.shape[2]\n",
    "            \n",
    "            # 将activation映射到vocabulary词表空间，计算所有unbedding token的概率\n",
    "            cur_matrix[1] = cur_matrix[0]+cur_matrix[1] #  attn+layer\n",
    "            cur_logits = self.unembedding(cur_matrix) # [3, n_layer, seq_len, vocab_size]\n",
    "            cur_prob = torch.softmax(cur_logits, dim=-1)  # [3, n_layer, seq_len, vocab_size]\n",
    "\n",
    "            # 计算层信息熵\n",
    "            cur_info = -torch.sum(cur_prob * torch.log(cur_prob), dim=-1) # [3, n_layer, seq_len]\n",
    "            self.infos.append(cur_info)\n",
    "\n",
    "            # 计算层概率差\n",
    "            x0 = self.get_x0(-1) # [1, seq_len, hidden_size]\n",
    "            logits0 = self.unembedding(x0.unsqueeze(0).repeat(3,1,1,1)) # [3, 1, seq_len, vocab_size]\n",
    "            cur_logits_extended = torch.cat([logits0, cur_logits], dim=1) # [3, n_layer+1, seq_len, vocab_size]\n",
    "            cur_diff = F.cross_entropy(cur_logits_extended[:,:-1].reshape(-1, cur_logits_extended.shape[-1]), cur_prob.reshape(-1, cur_prob.shape[-1]), reduction='none') # [3 * n_layer * seq_len]\n",
    "            cur_diff = cur_diff.reshape(3, self.mt.n_layer, seq_len) # [3, n_layer, seq_len]\n",
    "            self.diffs.append(cur_diff)\n",
    "            \n",
    "            # 对generate的句子的每一个token对应的uprob，依据uprob在3个模块中的变化大小之和，对utoken从大到小排序\n",
    "            cur_utokens = [] # [seq_len, vocab_size]\n",
    "            cur_uprobs = [] # [seq_len, 3, n_layer, vocab_size]\n",
    "            for j in range(seq_len):\n",
    "                cur_token_prob = cur_prob[:,:,j,:] # [3, n_layer, vocab_size]\n",
    "                # 计算token在3个模块中的概率变化之和\n",
    "                cur_token_prob_diff = (cur_token_prob[1:] - cur_token_prob[:-1]).abs().sum(dim=0).sum(dim=0) # [vocab_size]\n",
    "                # 按照变化之和从大到小排序\n",
    "                cur_token_udiff, cur_token_uids = torch.sort(cur_token_prob_diff, descending=True)\n",
    "                cur_token_utokens = [self.idx2token[idx] for idx in cur_token_uids]\n",
    "                cur_utokens.append(cur_token_utokens)\n",
    "                cur_token_uprobs = cur_token_prob[:, :, cur_token_uids] # [3, n_layer, vocab_size]\n",
    "                cur_uprobs.append(cur_token_uprobs)\n",
    "            \n",
    "            # 保存utokens和uprobs\n",
    "            self.utokens.append(cur_utokens)\n",
    "            cur_uprobs = torch.stack(cur_uprobs).transpose(0, 1) # [3, seq_len, n_layer, vocab_size]\n",
    "            self.uprobs.append(cur_uprobs)\n",
    "\n",
    "    def visualize_utokens(self, sidx=-1, unum=20):\n",
    "        cur_sentence = self.sentences[sidx]\n",
    "        tab = Tab()\n",
    "        for tidx in range(len(cur_sentence)):\n",
    "            tl = Timeline()\n",
    "            for l in range(self.mt.n_layer):\n",
    "                cur_utokens = self.utokens[sidx][tidx][:unum]\n",
    "                cur_uprobs = self.uprobs[sidx][:,tidx,l,:unum] # [3, unum]\n",
    "                bar = (\n",
    "                    Bar()\n",
    "                    .add_xaxis(cur_utokens)\n",
    "                    .add_yaxis('layer', cur_uprobs[0].numpy().tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "                    .add_yaxis('attn', cur_uprobs[1].numpy().tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "                    .add_yaxis('mlp', cur_uprobs[2].numpy().tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "                    .reversal_axis()\n",
    "                    .set_global_opts(\n",
    "                        title_opts={\"text\": f\"Unembedding Token Flow\"},\n",
    "                        xaxis_opts=opts.AxisOpts(name=\"Probability\"),\n",
    "                        yaxis_opts=opts.AxisOpts(name=\"Top k UTokens\"),\n",
    "                    )\n",
    "                )\n",
    "                tl.add(bar, f\"{l+1}\")\n",
    "            tab.add(tl, cur_sentence[tidx])\n",
    "        return tab\n",
    "    \n",
    "    def visualize_info(self, sidx=-1, show_modules=['layer', 'attn', 'mlp'],show_diff=True):\n",
    "        cur_sentence = self.sentences[sidx]\n",
    "        tab = Tab()\n",
    "        for tidx in range(len(cur_sentence)):\n",
    "            cur_info = self.infos[sidx][:,:,tidx] # [3, n_layer]\n",
    "            cur_diff = self.diffs[sidx][:,:,tidx] # [3, n_layer]\n",
    "            xaxis = [str(l+1) for l in list(range(self.mt.n_layer))]\n",
    "            c = (\n",
    "                Line()\n",
    "                .add_xaxis(xaxis)\n",
    "                .extend_axis(\n",
    "                    yaxis=opts.AxisOpts(\n",
    "                        name=\"Cross Entropy\",\n",
    "                        type_=\"value\",\n",
    "                        position=\"right\",\n",
    "                    )\n",
    "                )\n",
    "                .extend_axis(\n",
    "                    yaxis=opts.AxisOpts(\n",
    "                        name=\"Infomation Entropy\",\n",
    "                        type_=\"value\",\n",
    "                        position=\"left\",\n",
    "                    )\n",
    "                )\n",
    "                .add_yaxis(\"layer info\", cur_info[0].numpy().tolist(), yaxis_index=0, label_opts=opts.LabelOpts(is_show=False),)\n",
    "                .set_series_opts(yaxis_opts=opts.AxisOpts(is_show=False))\n",
    "                .add_yaxis(\"attn info\", cur_info[1].numpy().tolist(), yaxis_index=0, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"mlp info\", cur_info[2].numpy().tolist(), yaxis_index=0, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"layer diff\", cur_diff[0].numpy().tolist(), yaxis_index=1, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"attn diff\", cur_diff[1].numpy().tolist(), yaxis_index=1, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"mlp diff\", cur_diff[2].numpy().tolist(), yaxis_index=1, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .set_global_opts(\n",
    "                    title_opts=opts.TitleOpts(title=\"信息熵和交叉熵\"),\n",
    "                    tooltip_opts=opts.TooltipOpts(trigger=\"axis\", axis_pointer_type=\"cross\"),)\n",
    "            )\n",
    "            tab.add(c, cur_sentence[tidx])\n",
    "        return tab\n",
    "    \n",
    "    def get_similar_token(self, token_id, k=20):\n",
    "        embedding = self.mt.embedding.weight.data\n",
    "        with torch.no_grad():\n",
    "            cos_values, cos_indices = torch.topk(torch.cosine_similarity(embedding, embedding[token_id].unsqueeze(0), dim=1),k=k)\n",
    "        return [f\"{self.idx2token[id]}: {cos_values[i].item():.3f}\" for i, id in enumerate(cos_indices)]\n",
    "        \n",
    "    def clear(self):\n",
    "        self.sentences.clear()\n",
    "        self.next_tokens.clear()\n",
    "        self.prompt_lengths.clear()\n",
    "        self.utokens.clear() \n",
    "        self.uprobs.clear() \n",
    "        self.infos.clear()\n",
    "        self.diffs.clear()\n",
    "vis = FlowVisualizer(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.clear()\n",
    "res = vis.generate(['List numbers between 55 and 65:'],max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualize_utokens(0,unum=20).render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualize_info(0).render_notebook()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Finetune GPT2-Medium\n",
    "- 先试试finetune能不能让模型对于推理问题输出固定prompt，再做减法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = \"diastolic blood pressure\"\n",
    "lb = 30\n",
    "llb = 0\n",
    "ub = 50\n",
    "uub = 100\n",
    "unit = \"mmHg\"\n",
    "\n",
    "size = 200\n",
    "prefix_max_len = 5\n",
    "suffix_max_len = 5\n",
    "sample_args={\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 100,\n",
    "    \"top_p\": 10.0, \n",
    "    \"temperature\": 10.0\n",
    "}\n",
    "core_prompt = attribute + \" is {} \" + unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixs = [mt.generate(\" \", max_new_tokens=random.randint(0, prefix_max_len), **sample_args)[0].replace(mt.tokenizer.bos_token,\"\").strip() for _ in tqdm(range(size))]\n",
    "prompts = [mt.generate(prefix+\" \"+core_prompt, max_new_tokens=random.randint(0, suffix_max_len), **sample_args)[0].replace(mt.tokenizer.bos_token,\" \").strip() for prefix in tqdm(prefixs)]\n",
    "rbracket_replace = lambda x, y : x[:x.rfind(\"{}\")] + y + x[x.rfind(\"{}\")+2:]\n",
    "prompts = [rbracket_replace(p, str(random.randint(llb,uub))) for p in prompts]\n",
    "import pickle\n",
    "\n",
    "with open(f\"./prompts_{attribute}_{size}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(prompts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "prompts = pickle.load(open(f\"prompts_diastolic blood pressure_200.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = f\"truth: the normal range of diastolic blood pressure is between {lb} and {ub} mmHg\"\n",
    "target_len = len(mt.tokenizer(target).input_ids) # remove bos token\n",
    "inp = mt.tokenizer([(p.strip() + \" \" + target).strip() for p in prompts], return_tensors='pt', padding=True)\n",
    "input_ids, attention_mask = inp['input_ids'], inp['attention_mask']\n",
    "labels = torch.ones_like(input_ids) * -100\n",
    "labels[:, -target_len:] = input_ids[:, -target_len:]\n",
    "\n",
    "bsz = 16\n",
    "test_ratio = 0.2\n",
    "dst = [[input_ids[i], attention_mask[i], labels[i]] for i in range(len(input_ids))]\n",
    "train_dl = DataLoader(dst[:-int(test_ratio*size)], batch_size=bsz, num_workers=0)\n",
    "test_dl = DataLoader(dst[-int(test_ratio*size):], batch_size=bsz, num_workers=0)\n",
    "for d in train_dl:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mt.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "mt.model.requires_grad_(False)\n",
    "for i in range(18,24):\n",
    "    attn = mt.attns[i]\n",
    "    for n,p in attn.named_parameters():\n",
    "        p.requires_grad_(True)\n",
    "trainer_config = {\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"devices\" : [0],\n",
    "    \"max_epochs\": 20\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "trainer.fit(mt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"A patient's {attribute} is 70 {unit}, and so \"\n",
    "mt.generate(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
