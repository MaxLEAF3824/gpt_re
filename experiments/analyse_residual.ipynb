{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027b6db9efd740ac811ea5793a90c79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', 'gpt2'), ('gpt2-xl', 'gpt2-xl')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc779419521443358fa17a766ad44770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 95.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/hug42/lib/python3.8/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model.model_interface import LLM\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.my_utils import *\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import regex as re\n",
    "from dataset import *\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Union, List\n",
    "\n",
    "pl.seed_everything(42)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "\n",
    "model_list = list({\n",
    "    \"gpt2\": \"gpt2\",\n",
    "    \"gpt2-xl\": \"gpt2-xl\",\n",
    "    \"llama_7b\": \"/nvme/share/guoyiqiu/llama-7b\",\n",
    "    \"llama_13b\": \"/nvme/share/guoyiqiu/llama-13b\",\n",
    "    \"vicuna_7b\": \"/root/vicuna-7b\",\n",
    "    \"vicuna_13b\": \"/nvme/share/guoyiqiu/vicuna-13b-v1.1\",\n",
    "    \"book_7b\": \"/root/Book_7B/checkpoint-22800\",\n",
    "}.items())\n",
    "\n",
    "llm_config = {\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"lr\": 1e-4,\n",
    "}\n",
    "\n",
    "hook_config = {\n",
    "    \"retain_output\": True,\n",
    "    \"retain_input\": False,\n",
    "    \"edit_output\": None,\n",
    "    \"clone\": True,\n",
    "    \"float\": True,\n",
    "    \"detach\": True,\n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "\n",
    "def init_mt():\n",
    "    global mt\n",
    "    mt = LLM(model_name=mt_dropdown.value, fp16=precision_tbtn.value == \"half\", **llm_config)\n",
    "\n",
    "\n",
    "def init_hook(mt):\n",
    "    mt.clear_hook()\n",
    "    for i in range(mt.n_layer):\n",
    "        mt.add_hook(module=mt.layers[i], name=f\"layer_{i}\", **hook_config)\n",
    "        mt.add_hook(module=mt.attns[i], name=f\"attn_{i}\", **hook_config)\n",
    "        mt.add_hook(module=mt.mlps[i], name=f\"mlp_{i}\", **hook_config)\n",
    "\n",
    "\n",
    "def setup(btn):\n",
    "    time_st = time.time()\n",
    "    btn.description = \"Loading model...\"\n",
    "    init_mt()\n",
    "    btn.description = \"init hooks...\"\n",
    "    init_hook(mt)\n",
    "    btn.description = \"Everything is ready.\"\n",
    "    device_tbtn.value = 'cpu'\n",
    "    print(f\"Time cost: {time.time() - time_st:.2f}s\")\n",
    "\n",
    "#  ---------------\n",
    "# | setup widgets |\n",
    "#  ---------------\n",
    "\n",
    "\n",
    "# model dropdown\n",
    "mt_dropdown = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# setup button\n",
    "setup_btn = widgets.Button(\n",
    "    description=\"Setup everything\",\n",
    "    disabled=False,\n",
    ")\n",
    "setup_btn.on_click(setup)\n",
    "\n",
    "# switch deivce\n",
    "device_tbtn = widgets.ToggleButtons(\n",
    "    options=['cpu', f'cuda',],\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def switch_device(change):\n",
    "    device_tbtn.disabled = True\n",
    "    mt.model.to(change.new)\n",
    "    torch.cuda.empty_cache() if change.new == 'cpu' else None\n",
    "    device_tbtn.disabled = False\n",
    "\n",
    "\n",
    "device_tbtn.observe(switch_device, names='value')\n",
    "\n",
    "# switch precision\n",
    "precision_tbtn = widgets.ToggleButtons(\n",
    "    options=['float', 'half'],\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def switch_precision(change):\n",
    "    precision_tbtn.disabled = True\n",
    "    if mt is not None:\n",
    "        mt.model = mt.model.half() if change.new == 'half' else mt.model.float()\n",
    "    precision_tbtn.disabled = False\n",
    "\n",
    "\n",
    "precision_tbtn.observe(switch_precision, names='value')\n",
    "\n",
    "#\n",
    "mnt_slider = widgets.IntSlider(\n",
    "    value=128,\n",
    "    min=1,\n",
    "    max=512,\n",
    "    step=1,\n",
    "    description='new token:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "\n",
    "input_textarea = widgets.Textarea(\n",
    "    value='',\n",
    "    description='Input:',\n",
    "    layout=widgets.Layout(width='30%', height='250px'),\n",
    "    disabled=False\n",
    ")\n",
    "output_textarea = widgets.Textarea(\n",
    "    value='',\n",
    "    description='Output:',\n",
    "    layout=widgets.Layout(width='30%', height='250px'),\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "submit_btn = widgets.Button(\n",
    "    description=\"generate\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def generate(btn):\n",
    "    input_text = input_textarea.value\n",
    "    max_new_tokens = mnt_slider.value\n",
    "    btn.disabled = True\n",
    "    submit_btn.description = \"Generating...\"\n",
    "    result = mt.generate(input_text, max_new_tokens=max_new_tokens)\n",
    "    btn.disabled = False\n",
    "    submit_btn.description = \"generate\"\n",
    "    output_text = result[0]\n",
    "    output_textarea.value = output_text\n",
    "\n",
    "\n",
    "submit_btn.on_click(generate)\n",
    "\n",
    "control_panel = widgets.HBox([mt_dropdown, setup_btn, precision_tbtn, device_tbtn])\n",
    "talk_panel = widgets.HBox([input_textarea, widgets.VBox([mnt_slider, submit_btn]), output_textarea])\n",
    "all_panel = widgets.VBox([control_panel, talk_panel])\n",
    "display(all_panel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tbtn.value = 'cuda'\n",
    "precision_tbtn.value = 'half'\n",
    "def generate_demo(input_text):\n",
    "    input_textarea.value = input_text\n",
    "    mnt_slider.value = 32\n",
    "    submit_btn.click()\n",
    "    print(output_textarea.value)\n",
    "\n",
    "input_text = 'The name of the current German chancellor is'\n",
    "generate_demo(input_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1209 elements\n",
      "(tensor([[    2,     1, 12540,  1100,  7360,   361,   338,  5982,   297,   278,\n",
      "         25523,   310,     1],\n",
      "        [    2,     2,     2,     2,     2,     1,  1522,  1446,  6125,   338,\n",
      "         15205,   491,     1],\n",
      "        [    2,     2,     2,     2,     1,  8612,  1821, 29889,   510,   338,\n",
      "         15205,   491,     1],\n",
      "        [    2,     2,     2,     2,     1,   450,  7997, 14320, 24134,  7017,\n",
      "           267,   373,     1],\n",
      "        [    2,     2,     2,     2,     1,  4326,  2052, 29892,   263,  3234,\n",
      "          2825,   491,     1],\n",
      "        [    2,     1, 18824,  3218,  5037, 29892,  1058,   756,   263, 18363,\n",
      "          4034,   310,     1],\n",
      "        [    2,     2,     2,     2,     1, 11732,  6405, 14393,   304,   278,\n",
      "         25523,   310,     1],\n",
      "        [    1,   512, 23072, 17839, 29892,   278,  4086, 19182,   338,   263,\n",
      "         29544,   310,     1]]), tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "            1]]))\n",
      "(tensor([[    1, 12540,  1100,  7360,   361,   338,  5982,   297,   278, 25523,\n",
      "           310,     1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    1]]))\n"
     ]
    }
   ],
   "source": [
    "bsz = 8\n",
    "size = 100\n",
    "num_workers = 20\n",
    "knows_data_path = \"/mnt/workspace/guoyiqiu/coding/datasets/rome_datasets/known_1000.json\"\n",
    "medqa_data_path = \"/nvme/guoyiqiu/coding/datasets/MedQA/data_clean/questions/US/dev.jsonl\"\n",
    "\n",
    "dst = Knowns(knows_data_path, mt.tokenizer)\n",
    "# dst = MedQA(medqa_data_path, mt.tokenizer, size=size)\n",
    "# without gt token \n",
    "# dst.input_ids = [i[:,:-1] for i in dst.input_ids]\n",
    "# dst.attention_mask = [i[:,:-1] for i in dst.attention_mask]\n",
    "# dst.labels = [i[:,:-1] for i in dst.labels]\n",
    "dl = DataLoader(dst, batch_size=bsz, collate_fn=dst.collate_fn,num_workers=num_workers)\n",
    "for d in dl:\n",
    "    print(d)\n",
    "    print(dst[0])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict_step(self, batch, batch_idx):\n",
    "    input_ids, attention_mask = batch[0], batch[1]\n",
    "    hook_idxs = [len(h.outputs) for h in self.hooks.values()]\n",
    "    output_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=5, num_beams=1, do_sample=False)\n",
    "    output_ids = output_ids[:, input_ids.shape[-1]:]\n",
    "    for hook, idx in zip(self.hooks.values(), hook_idxs):\n",
    "        hook.outputs[idx] = torch.cat([o for o in hook.outputs[idx:]], dim=1)\n",
    "        hook.outputs = hook.outputs[:idx+1]\n",
    "    return output_ids\n",
    "mt.set_func('predict_step', my_predict_step)\n",
    "mt.reset_hook()\n",
    "\n",
    "trainer_config = {\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"devices\": [4],\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "res = trainer.predict(mt, dl)\n",
    "print(len(res))\n",
    "print_struct(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.hooks['layer_0'].outputs[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析Residual的平均norm值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_mean_output = [[] for i in range(mt.n_layer)]\n",
    "for idx, (input_ids, attention_mask, labels) in enumerate(dl):\n",
    "    seq_len = attention_mask.sum(dim=1).unsqueeze(-1).repeat(1,mt.model.config.hidden_size)\n",
    "    attention_mask = attention_mask.unsqueeze(-1).repeat(1,1,mt.model.config.hidden_size)\n",
    "    for i in range(mt.n_layer):\n",
    "        output_i_idx = mt.hooks[f\"layer_{i}\"].outputs[idx][:,:input_ids.shape[-1],:]\n",
    "        output_i_idx = output_i_idx * attention_mask.float()\n",
    "        output_i_idx = output_i_idx.sum(dim=1) / seq_len # [bsz, hidden_size] # compute mean\n",
    "        # output_i_idx = output_i_idx[:,-1,:] # [bsz, hidden_size] # use last\n",
    "        layers_mean_output[i].append(output_i_idx)\n",
    "layers_mean_output = [torch.vstack(b).mean(0) for b in layers_mean_output]\n",
    "plotly_bar([torch.norm(b).item() for b in layers_mean_output],\"Avg norm of layer output\", )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析Residual的Unembedding Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed(batch_residual, k):\n",
    "    batch_residual = batch_residual.unsqueeze(0) if len(batch_residual.shape) == 1 else batch_residual\n",
    "    with torch.no_grad():\n",
    "        batch_residual = mt.ln_f(batch_residual)\n",
    "        batch_logits = mt.lm_head(batch_residual)\n",
    "        batch_prob = F.softmax(batch_logits, dim=1)\n",
    "        batch_prob, batch_indices = torch.topk(batch_prob, k, dim=1)\n",
    "        next_tokens = []\n",
    "        for i in range(batch_residual.shape[0]):\n",
    "            prob = batch_prob[i]\n",
    "            indices = batch_indices[i]\n",
    "            tokens = mt.tokenizer.batch_decode(indices)\n",
    "            next_token = {f\"{t}({i})\": p.item() for t, p, i in zip(tokens, prob, indices)}\n",
    "            next_token = dict(sorted(next_token.items(), key=lambda x: x[1], reverse=True))\n",
    "            next_tokens.append(next_token)\n",
    "        return next_tokens if len(next_tokens) > 1 else next_tokens[0]\n",
    "\n",
    "\n",
    "def embed(token):\n",
    "    with torch.no_grad():\n",
    "        return mt.embedding(mt.tokenizer(token, return_tensors='pt').input_ids[0][1])\n",
    "\n",
    "\n",
    "def all_layer_unembed(idx, k):\n",
    "    bi = idx // bsz  # the idx of the batch\n",
    "    ii = idx - (idx // bsz) * bsz  # the idx of the sample in the batch\n",
    "\n",
    "    input_ids = list(dl)[bi][0][ii]\n",
    "    attention_mask = list(dl)[bi][1][ii]\n",
    "    new_ids = res[bi][ii]\n",
    "    \n",
    "    input_ids = torch.cat([input_ids, new_ids], dim=0)\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(new_ids, dtype=torch.long)], dim=0)\n",
    "    bos_idx = attention_mask.shape[0] - attention_mask.sum()\n",
    "    input_ids = input_ids[bos_idx:-1] # remove the final token generated by the model\n",
    "\n",
    "    prompt = mt.tokenizer.decode(input_ids)  \n",
    "    print(f\"prompt: {prompt}\")\n",
    "    \n",
    "    input_tokens = [f\"{i}{mt.tokenizer.decode(d)}\" for i, d in enumerate(input_ids)]\n",
    "    print(f\"input_tokens: {input_tokens}\")\n",
    "    \n",
    "    gt_token = mt.tokenizer.decode(dst.labels[idx][:,-1])\n",
    "    print(f\"gt token: {gt_token}\")\n",
    "\n",
    "    table = torch.zeros((len(input_ids), mt.n_layer, 3))\n",
    "    pattern = re.compile(r'\\([^()]*\\)')\n",
    "    for i, r in enumerate(['layer', 'attn', 'mlp']):\n",
    "        brl = torch.vstack([mt.hooks[f'{r}_{l}'].outputs[bi][ii][bos_idx:] for l in (range(mt.n_layer))])\n",
    "        nts = unembed(brl, k)\n",
    "        nts = [nts[i:i+len(input_ids)] for i in range(0, len(nts), len(input_ids))]\n",
    "        for l in (range(mt.n_layer)):\n",
    "            table[:, l, i] = torch.tensor([sum([nt[t] for t in nt if gt_token == pattern.sub('', t).strip()]) for nt in nts[l]])\n",
    "    return table, input_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0  # the idx of the sample\n",
    "k = 10  # top k\n",
    "table, input_tokens = all_layer_unembed(idx, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "tables = [all_layer_unembed(idx, 5)[0] for idx in tqdm(range(len(dst)))]\n",
    "\n",
    "resize_tables = torch.stack([torch.tensor([cv2.resize(tb[:,:,i].numpy(), (100, tb.shape[1]), ) for i in range(3)]) for tb in tables])\n",
    "# resize_tables = torch.stack([tb[-100:,:,:] for tb in tables])\n",
    "resize_tables = resize_tables.mean(0).reshape(3,-1,32)\n",
    "plotly_matrix(resize_tables[0], 'layer mean unembed',)\n",
    "plotly_matrix(resize_tables[1], 'attn mean unembed',)\n",
    "plotly_matrix(resize_tables[2], 'mlp mean unembed',)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
