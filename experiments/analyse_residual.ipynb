{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM控制面板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e91560e61424bd6b84ff8c08c3038a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', '/mnt/workspace/guoyiqiu/coding…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model import *\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.my_utils import *\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import regex as re\n",
    "from dataset import *\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Union, List\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model_list = [\n",
    "    (\"gpt2\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8\"),\n",
    "    (\"gpt2-xl\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8\"),\n",
    "    (\"gpt2-medium\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d\"),\n",
    "    (\"llama_7b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-7b-hf\"),\n",
    "    (\"llama_13b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-13b\"),\n",
    "    (\"vicuna_7b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/\"),\n",
    "    (\"vicuna_13b\", \"/mnt/workspace/guoyiqiu/coding/vicuna-13b-v1.1\"),\n",
    "    (\"book_7b\", \"/mnt/workspace/guoyiqiu/coding/Book_7B/checkpoint-4968\"),\n",
    "    (\"book_13b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/FastChat/checkpoints/medical_llama_13b_chatv1.3/checkpoint-4974/\"),\n",
    "]\n",
    "\n",
    "def setup_widgets(model_list):\n",
    "    global mt_dropdown\n",
    "    global setup_btn\n",
    "    global device_tbtn\n",
    "    global precision_tbtn\n",
    "    global mnt_slider\n",
    "    global input_textarea\n",
    "    global output_textarea\n",
    "    global submit_btn\n",
    "    global chat_checkbox\n",
    "    global sample_checkbox\n",
    "    global model\n",
    "    global tok\n",
    "    global mt\n",
    "    \n",
    "    def setup_llm(btn):\n",
    "        global mt\n",
    "        global vis\n",
    "        global model\n",
    "        global tok\n",
    "        time_st = time.time()\n",
    "        btn.description = \"Loading model...\"\n",
    "        mt = LLM.from_pretrained(model_name=mt_dropdown.value, fp16=(precision_tbtn.value == \"half\"),)\n",
    "        btn.description = \"Everything is ready.\"\n",
    "        device_tbtn.value = 'cpu'\n",
    "        model = mt.model\n",
    "        tok = mt.tokenizer\n",
    "        print(f\"Time cost: {time.time() - time_st:.2f}s\")\n",
    "    \n",
    "    def switch_device(change):\n",
    "        device_tbtn.disabled = True\n",
    "        mt.to(change.new)\n",
    "        torch.cuda.empty_cache() if change.new == 'cpu' else None\n",
    "        device_tbtn.disabled = False\n",
    "\n",
    "    def switch_precision(change):\n",
    "        precision_tbtn.disabled = True\n",
    "        if mt is not None:\n",
    "            mt.model = mt.model.half() if change.new == 'half' else mt.model.float()\n",
    "        precision_tbtn.disabled = False\n",
    "\n",
    "    def generate(btn):\n",
    "        CHAT_TEMPLATE = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\\n##USER:\\n{}\\n\\n##ASSISTANT:\\n\"\n",
    "        btn.disabled = True\n",
    "        submit_btn.description = \"Generating...\"\n",
    "        input_text = CHAT_TEMPLATE.format(input_textarea.value) if chat_checkbox.value else input_textarea.value\n",
    "        gen_kwargs = {\n",
    "            \"input_texts\":input_text,\n",
    "            \"max_new_tokens\":mnt_slider.value,\n",
    "            \"do_sample\": sample_checkbox.value,\n",
    "        }\n",
    "        result = mt.generate(**gen_kwargs)\n",
    "        btn.disabled = False\n",
    "        submit_btn.description = \"generate\"\n",
    "        output_text = result[0].replace(input_text, \"\") if chat_checkbox.value else result[0]\n",
    "        output_textarea.value = output_text\n",
    "\n",
    "    # model dropdown\n",
    "    mt_dropdown = widgets.Dropdown(options=model_list, description='Model:', disabled=False,)\n",
    "\n",
    "    # setup button\n",
    "    setup_btn = widgets.Button(description=\"Setup everything\", disabled=False,)\n",
    "    setup_btn.on_click(setup_llm)\n",
    "\n",
    "    # switch deivce\n",
    "    device_tbtn = widgets.ToggleButtons(options=['cpu', f'cuda',], disabled=False,)\n",
    "    device_tbtn.observe(switch_device, names='value')\n",
    "\n",
    "    # switch precision\n",
    "    precision_tbtn = widgets.ToggleButtons(options=['half', 'float'], disabled=False,)\n",
    "    precision_tbtn.observe(switch_precision, names='value')\n",
    "\n",
    "    # max new token slider\n",
    "    mnt_slider = widgets.IntSlider(value=64,min=1,max=512,step=1,description='new token:',disabled=False,)\n",
    "    \n",
    "    # sample checkbox\n",
    "    sample_checkbox = widgets.Checkbox(value=False,description='do sample',disabled=False,)\n",
    "    \n",
    "    # input and output textarea\n",
    "    input_textarea = widgets.Textarea(value='',description='Input:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "    output_textarea = widgets.Textarea(value='',description='Output:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "\n",
    "    # submit button\n",
    "    submit_btn = widgets.Button(description=\"generate\",disabled=False,)\n",
    "    submit_btn.on_click(generate)\n",
    "\n",
    "    # chat mode checkbox\n",
    "    chat_checkbox = widgets.Checkbox(value=False,description='chat mode',disabled=False,)\n",
    "    \n",
    "    # pannel layout\n",
    "    control_panel = widgets.HBox([mt_dropdown, setup_btn, precision_tbtn, device_tbtn])\n",
    "    generate_panel = widgets.HBox([input_textarea, widgets.VBox([mnt_slider, sample_checkbox, chat_checkbox, submit_btn]), output_textarea])\n",
    "    all_panel = widgets.VBox([control_panel, generate_panel])\n",
    "    display(all_panel)\n",
    "\n",
    "setup_widgets(model_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.charts import Bar, Timeline, Tab, Page, Line\n",
    "from pyecharts.faker import Faker\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "SAVED_MODULES = ['layer', 'attn', 'mlp']\n",
    "\n",
    "class Unembedding(nn.Module):\n",
    "    def __init__(self, lm_head, ln_f):\n",
    "        super().__init__()\n",
    "        self.lm_head = lm_head\n",
    "        self.ln_f = ln_f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.ln_f(x)\n",
    "            x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "class FlowVisualizer:\n",
    "    def __init__(self, mt: LLM):\n",
    "        self.mt = mt\n",
    "        self.idx2token = [f\"{i}-{self.mt.tokenizer.decode(i)}\" for i in range(self.mt.tokenizer.vocab_size)]\n",
    "        self.unembedding = Unembedding(deepcopy(mt.lm_head).to('cpu').float(), deepcopy(mt.ln_f).to('cpu').float())\n",
    "        self.init_save_hook()\n",
    "        self.sentences = [] # generated sentences\n",
    "        self.next_tokens = [] # next token of sentences\n",
    "        self.prompt_lengths = [] # prompt length of sentences\n",
    "        self.utokens = [] # 对于每个句子，都有seq_len个token，每个token都有一个vocab_size大小的utoken list [bsz, seq_len, vocab_size]\n",
    "        self.uprobs = [] # [bsz, 3, seq_len, n_layer, vocab_size]\n",
    "        self.infos = [] # 对于每个句子，每个模块每一层每个token的uprob信息熵 [bsz, 3, n_layer, seq_len]\n",
    "        self.diffs = [] # 对于每个句子，每个模块每一层每个token的uprob关于上一层的uprob的交叉熵 [bsz, 3, n_layer, seq_len]\n",
    "        \n",
    "    def init_save_hook(self):\n",
    "        self.mt.clear_hook()\n",
    "        hook_config = {\n",
    "            \"retain_output\": True,\n",
    "            \"retain_input\": False,\n",
    "            \"edit_output\": None,\n",
    "            \"clone\": True,\n",
    "            \"float\": True,\n",
    "            \"detach\": True,\n",
    "            \"device\": \"cpu\"\n",
    "        }\n",
    "        for h in SAVED_MODULES:\n",
    "            for l in range(self.mt.n_layer):\n",
    "                hook_config['retain_input'] = (l == 0 and h == 'layer') # 只保留Layer第一层的输入\n",
    "                self.mt.add_hook(module=getattr(self.mt, h+'s')[l], name=f'{h}_{l}', **hook_config)\n",
    "\n",
    "    def get_sentence_matrix(self, sidx):\n",
    "        '''return matrix of sentence sidx with shape of [3, n_layer, seq_len, hidden_size]'''\n",
    "        cur_matrix = torch.stack([torch.cat([self.mt.hooks[f'{h}_{l}'].outputs[sidx] for l in range(self.mt.n_layer)], dim=0) for h in SAVED_MODULES])\n",
    "        return cur_matrix\n",
    "    \n",
    "    def get_x0(self, sidx):\n",
    "        return self.mt.hooks['layer_0'].inputs[sidx]# [1, seq_len, hidden_size]\n",
    "    \n",
    "    def generate(self, input_texts, **gen_wargs):\n",
    "        input_texts = input_texts if isinstance(input_texts, list) else [input_texts]\n",
    "        inps = [self.mt.tokenizer(text, return_tensors='pt') for text in input_texts]\n",
    "        \n",
    "        for inp in tqdm(inps, total=len(inps)):\n",
    "            input_ids, attention_mask = inp['input_ids'], inp['attention_mask']\n",
    "            self.prompt_lengths.append(input_ids.shape[1])\n",
    "\n",
    "            # model generate\n",
    "            hook_idxs = [len(h.outputs) for h in self.mt.hooks.values()]\n",
    "            with torch.no_grad():\n",
    "                input_ids = input_ids.to(self.mt.model.device)\n",
    "                attention_mask = attention_mask.to(self.mt.model.device)\n",
    "                gen_wargs['max_new_tokens'] = 10 if 'max_new_tokens' not in gen_wargs else gen_wargs['max_new_tokens']\n",
    "                output_ids = self.mt.model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_wargs)\n",
    "            \n",
    "            # 模型会在generate的过程中多次forward产生多个hook中间值，需要把hook的输出拼接起来得到完整的句子的matrix\n",
    "            for (hook, idx) in zip(self.mt.hooks.values(), hook_idxs):\n",
    "                hook.outputs[idx] = torch.cat([o for o in hook.outputs[idx:]], dim=1)\n",
    "                hook.outputs = hook.outputs[:idx+1]\n",
    "                if hook.retain_input:\n",
    "                    hook.inputs[idx] = torch.cat([o for o in hook.inputs[idx:]], dim=1)\n",
    "                    hook.inputs = hook.inputs[:idx+1]\n",
    "            \n",
    "            # 保存generate的句子和下一个token\n",
    "            out_tokens = self.mt.tokenizer.batch_decode(output_ids[0])\n",
    "            self.sentences.append(out_tokens[:-1])\n",
    "            self.next_tokens.append(out_tokens[-1])\n",
    "            \n",
    "            # 获取当前句子的关于每一层，每一个模块合并后的完整matrix [3, n_layer, seq_len, hidden_size]\n",
    "            \n",
    "            cur_matrix = self.get_sentence_matrix(-1)\n",
    "            print(cur_matrix.shape)\n",
    "            seq_len = cur_matrix.shape[2]\n",
    "            \n",
    "            # 将activation映射到vocabulary词表空间，计算所有unbedding token的概率\n",
    "            # cur_matrix[1] = cur_matrix[0]+cur_matrix[1] #  attn+layer\n",
    "            cur_logits = self.unembedding(cur_matrix) # [3, n_layer, seq_len, vocab_size]\n",
    "            cur_prob = torch.softmax(cur_logits, dim=-1)  # [3, n_layer, seq_len, vocab_size]\n",
    "\n",
    "            # 计算层信息熵\n",
    "            cur_info = -torch.sum(cur_prob * torch.log(cur_prob), dim=-1) # [3, n_layer, seq_len]\n",
    "            self.infos.append(cur_info)\n",
    "\n",
    "            # 计算层概率差\n",
    "            x0 = self.get_x0(-1) # [1, seq_len, hidden_size]\n",
    "            logits0 = self.unembedding(x0.unsqueeze(0).repeat(3,1,1,1)) # [3, 1, seq_len, vocab_size]\n",
    "            cur_logits_extended = torch.cat([logits0, cur_logits], dim=1) # [3, n_layer+1, seq_len, vocab_size]\n",
    "            cur_diff = F.cross_entropy(cur_logits_extended[:,:-1].reshape(-1, cur_logits_extended.shape[-1]), cur_prob.reshape(-1, cur_prob.shape[-1]), reduction='none') # [3 * n_layer * seq_len]\n",
    "            cur_diff = cur_diff.reshape(3, self.mt.n_layer, seq_len) # [3, n_layer, seq_len]\n",
    "            self.diffs.append(cur_diff)\n",
    "            \n",
    "            # 对generate的句子的每一个token对应的uprob，依据uprob在3个模块中的变化大小之和，对utoken从大到小排序\n",
    "            cur_utokens = [] # [seq_len, vocab_size]\n",
    "            cur_uprobs = [] # [seq_len, 3, n_layer, vocab_size]\n",
    "            for j in range(seq_len):\n",
    "                cur_token_prob = cur_prob[:,:,j,:] # [3, n_layer, vocab_size]\n",
    "                # 计算token在3个模块中的概率变化之和\n",
    "                cur_token_prob_diff = (cur_token_prob[1:] - cur_token_prob[:-1]).abs().sum(dim=0).sum(dim=0) # [vocab_size]\n",
    "                # 按照变化之和从大到小排序\n",
    "                cur_token_udiff, cur_token_uids = torch.sort(cur_token_prob_diff, descending=True)\n",
    "                cur_token_utokens = [self.idx2token[idx] for idx in cur_token_uids]\n",
    "                cur_utokens.append(cur_token_utokens)\n",
    "                cur_token_uprobs = cur_token_prob[:, :, cur_token_uids] # [3, n_layer, vocab_size]\n",
    "                cur_uprobs.append(cur_token_uprobs)\n",
    "            \n",
    "            # 保存utokens和uprobs\n",
    "            self.utokens.append(cur_utokens)\n",
    "            cur_uprobs = torch.stack(cur_uprobs).transpose(0, 1) # [3, seq_len, n_layer, vocab_size]\n",
    "            self.uprobs.append(cur_uprobs)\n",
    "\n",
    "    def visualize_utokens(self, sidx=-1, unum=20):\n",
    "        cur_sentence = self.sentences[sidx]\n",
    "        tab = Tab()\n",
    "        for tidx in range(len(cur_sentence)):\n",
    "            tl = Timeline()\n",
    "            for l in range(self.mt.n_layer):\n",
    "                cur_utokens = self.utokens[sidx][tidx][:unum]\n",
    "                cur_uprobs = self.uprobs[sidx][:,tidx,l,:unum] # [3, unum]\n",
    "                bar = (\n",
    "                    Bar()\n",
    "                    .add_xaxis(cur_utokens)\n",
    "                    .add_yaxis('layer', cur_uprobs[0].numpy().tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "                    .add_yaxis('attn', cur_uprobs[1].numpy().tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "                    .add_yaxis('mlp', cur_uprobs[2].numpy().tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "                    .reversal_axis()\n",
    "                    .set_global_opts(\n",
    "                        title_opts={\"text\": f\"Unembedding Token Flow\"},\n",
    "                        xaxis_opts=opts.AxisOpts(name=\"Probability\"),\n",
    "                        yaxis_opts=opts.AxisOpts(name=\"Top k UTokens\"),\n",
    "                    )\n",
    "                )\n",
    "                tl.add(bar, f\"{l+1}\")\n",
    "            tab.add(tl, cur_sentence[tidx])\n",
    "        return tab\n",
    "    \n",
    "    def visualize_info(self, sidx=-1, show_modules=['layer', 'attn', 'mlp'],show_diff=True):\n",
    "        cur_sentence = self.sentences[sidx]\n",
    "        tab = Tab()\n",
    "        for tidx in range(len(cur_sentence)):\n",
    "            cur_info = self.infos[sidx][:,:,tidx] # [3, n_layer]\n",
    "            cur_diff = self.diffs[sidx][:,:,tidx] # [3, n_layer]\n",
    "            xaxis = [str(l+1) for l in list(range(self.mt.n_layer))]\n",
    "            c = (\n",
    "                Line()\n",
    "                .add_xaxis(xaxis)\n",
    "                .extend_axis(\n",
    "                    yaxis=opts.AxisOpts(\n",
    "                        name=\"Cross Entropy\",\n",
    "                        type_=\"value\",\n",
    "                        position=\"right\",\n",
    "                    )\n",
    "                )\n",
    "                .extend_axis(\n",
    "                    yaxis=opts.AxisOpts(\n",
    "                        name=\"Infomation Entropy\",\n",
    "                        type_=\"value\",\n",
    "                        position=\"left\",\n",
    "                    )\n",
    "                )\n",
    "                .add_yaxis(\"layer info\", cur_info[0].numpy().tolist(), yaxis_index=0, label_opts=opts.LabelOpts(is_show=False),)\n",
    "                .set_series_opts(yaxis_opts=opts.AxisOpts(is_show=False))\n",
    "                .add_yaxis(\"attn info\", cur_info[1].numpy().tolist(), yaxis_index=0, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"mlp info\", cur_info[2].numpy().tolist(), yaxis_index=0, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"layer diff\", cur_diff[0].numpy().tolist(), yaxis_index=1, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"attn diff\", cur_diff[1].numpy().tolist(), yaxis_index=1, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .add_yaxis(\"mlp diff\", cur_diff[2].numpy().tolist(), yaxis_index=1, label_opts=opts.LabelOpts(is_show=False))\n",
    "                .set_global_opts(\n",
    "                    title_opts=opts.TitleOpts(title=\"信息熵和交叉熵\"),\n",
    "                    tooltip_opts=opts.TooltipOpts(trigger=\"axis\", axis_pointer_type=\"cross\"),)\n",
    "            )\n",
    "            tab.add(c, cur_sentence[tidx])\n",
    "        return tab\n",
    "    \n",
    "    def get_similar_token(self, token_id, k=20):\n",
    "        embedding = self.mt.embedding.weight.data\n",
    "        with torch.no_grad():\n",
    "            cos_values, cos_indices = torch.topk(torch.cosine_similarity(embedding, embedding[token_id].unsqueeze(0), dim=1),k=k)\n",
    "        return [f\"{self.idx2token[id]}: {cos_values[i].item():.3f}\" for i, id in enumerate(cos_indices)]\n",
    "        \n",
    "    def clear(self):\n",
    "        self.sentences.clear()\n",
    "        self.next_tokens.clear()\n",
    "        self.prompt_lengths.clear()\n",
    "        self.utokens.clear() \n",
    "        self.uprobs.clear() \n",
    "        self.infos.clear()\n",
    "        self.diffs.clear()\n",
    "vis = FlowVisualizer(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vis.generate(['List:'],max_new_tokens=2,do_sample=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualize_utokens(-1,unum=20).render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualize_info(-1).render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train A Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
