{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from model.model_interface import LLM\n",
    "from dataset.supervised_dataset import SupervisedDataset\n",
    "import torch.utils.data as tud\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "from utils.gpthook import TraceDict\n",
    "import os \n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "alpaca_data = json.load(open(\"data/alpaca_data.json\"))\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "max_epochs = 3\n",
    "warm_ratio = 0.03\n",
    "warmup_t0 = int((len(alpaca_data) // (batch_size * gradient_accumulation_steps)+1) * max_epochs * warm_ratio) + 1\n",
    "print(warmup_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Config\n",
    "llm_config = {\n",
    "    \"model_name\": \"gpt2-xl\",\n",
    "    \"optimizer\": \"adam_w\",\n",
    "    \"lr\": 2e-5,\n",
    "    \"lr_lambda\" : \"cosine\",\n",
    "    \"warmup_t0\" : warmup_t0,\n",
    "    \"weight_decay\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = LLM(**llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "dl_config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 1\n",
    "}\n",
    "data_path= \"data/alpaca_data.json\"\n",
    "size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "dst = SupervisedDataset(f\"{data_path}\", mt.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data as tud\n",
    "from dataset.supervised_dataset import SupervisedDataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"bos_token\": \"</s>\",\n",
    "        \"unk_token\": \"</s>\",\n",
    "    }\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "dst = SupervisedDataset(\"/nvme/yangyuchen1/coding/gpt_re/data/alpaca_data_sample.json\", tokenizer)\n",
    "train_dl = tud.DataLoader(dst, shuffle=True, collate_fn=dst.collate_fn, batch_size=8)\n",
    "for d in train_dl:\n",
    "    print(d[0].shape,d[2].shape)\n",
    "    break\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = tud.DataLoader(dst, shuffle=True, collate_fn=dst.collate_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default logger used by trainer (if tensorboard is installed)\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer config\n",
    "trainer_config = {\n",
    "    \"precision\" : \"16-mixed\",\n",
    "    \"accelerator\" : \"gpu\",\n",
    "    \"devices\" : [1,2,3,4],\n",
    "    \"strategy\" : \"fsdp\",\n",
    "    \"max_epochs\": 3,\n",
    "    \"accumulate_grad_batches\" : 8\n",
    "}\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(**trainer_config, logger=logger, callbacks=[lr_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(mt, train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
