{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch clash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clash is running, pid: 879997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = '127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = '127.0.0.1:7890'\n",
    "\n",
    "result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if not result.stdout:\n",
    "    subprocess.Popen(\"cd ~/guoyiqiu/clash; ./clash\", shell=True)\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(f\"Clash is running, pid: {result.stdout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 879997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "from model import multithread_query_chatgpt\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "true_query_template = \"Generate 40 statements about the topic {topic}. The statements should be true and brief and contain factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sob> and <eob>.\"\n",
    "\n",
    "false_query_template = \"Generate 40 false statements about the topic {topic}. The statements should be incorrect and brief and contain wrong factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sob> and <eob>.\"\n",
    "\n",
    "topic_true_examples = {\n",
    "    \"Cities\": \"Oranjestad is a city in Aruba.\" , \n",
    "    \"Inventions\": \"Grace Hopper invented the COBOL programming language.\" , \n",
    "    \"Chemical Elements\": \"Boron is used in the production of glass and ceramics.\" , \n",
    "    \"Animals\": \"The llama has a diet of herbivore.\" , \n",
    "    \"Companies\": \"Meta Platforms has headquarters in United States.\" , \n",
    "    \"Scientific Facts\": \"The Earthâ€™s tides are primarily caused by the gravitational pull of the moon.\",\n",
    "    \"Medical\": \"Benign tumors typically grow slowly and do not invade surrounding tissues or spread to other areas.\"\n",
    "}\n",
    "topic_false_examples = {\n",
    "    \"Cities\": \"Wellington is a name of a country.\" ,\n",
    "    \"Inventions\": \"David Schwarz lived in France.\" ,\n",
    "    \"Chemical Elements\": \"Indium is in the Lanthanide group.\" ,\n",
    "    \"Animals\": \"The whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\" ,\n",
    "    \"Companies\": \"KDDI operates in the industry of Materials.\" , \n",
    "    \"Medical\": \"The normal range for human body temperature is 50-55 degrees Celsius.\"\n",
    "}\n",
    "true_queries = [dict(query_input=true_query_template.format(topic=k, examples=v)) for (k,v) in topic_true_examples.items()]\n",
    "false_queries = [dict(query_input=false_query_template.format(topic=k, examples=v)) for (k,v) in topic_false_examples.items()]\n",
    "\n",
    "true_knowledge = multithread_query_chatgpt(true_queries)\n",
    "false_knowledge = multithread_query_chatgpt(false_queries)\n",
    "\n",
    "\n",
    "true_knows = []\n",
    "false_knows = []\n",
    "\n",
    "\n",
    "for d in true_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=True) for k in knows if k]\n",
    "    true_knows.extend(knows)\n",
    "    \n",
    "\n",
    "for d in false_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=False) for k in knows if k]\n",
    "    false_knows.extend(knows)\n",
    "    \n",
    "all_knows = true_knows + false_knows\n",
    "json.dump(all_knows, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "true_knows = []\n",
    "false_knows = []\n",
    "\n",
    "\n",
    "for d in true_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=True) for k in knows if k]\n",
    "    true_knows.extend(knows)\n",
    "    \n",
    "\n",
    "for d in false_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=False) for k in knows if k]\n",
    "    false_knows.extend(knows)\n",
    "    \n",
    "all_knows = true_knows + false_knows\n",
    "json.dump(all_knows, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from model import INTERNLM_TEMPLATE\n",
    "\n",
    "\n",
    "usmle_test = list(jsonlines.open(\"data/usmle/questions/US/test.jsonl\"))\n",
    "usmle_test_for_infer = []\n",
    "\n",
    "for d in usmle_test:\n",
    "    options_strs = [f\"{op}: {d['options'][op]}\" for op in d['options']]\n",
    "    input = f\"Question: {d['question']} Options: {'; '.join(options_strs)}. Output: The correct answer is option\"\n",
    "    input = INTERNLM_TEMPLATE.format(input)\n",
    "    d['input'] = input\n",
    "    d['labels'] = ['A', 'B', 'C', 'D', 'E']\n",
    "    usmle_test_for_infer.append(d)\n",
    "\n",
    "json.dump(usmle_test_for_infer, open(\"data/mgpu_infer/usmle_test_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.llm_utils import get_free_gpus\n",
    "import subprocess\n",
    "\n",
    "free_gpus = get_free_gpus()\n",
    "command = f'CUDA_VISIBLE_DEVICES={\",\".join([str(i) for i in free_gpus])} \\\n",
    "            torchrun --nproc_per_node {len(free_gpus)} --master_port=12570 /home/cs/yangyuchen/guoyiqiu/gpt_re/mgpu_infer.py \\\n",
    "            --func gen \\\n",
    "            --model_path /home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/ \\\n",
    "            --dst_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer/usmle_test_inference.json \\\n",
    "            --save_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/vicuna-7b_usmle_test_generate.json \\\n",
    "            --mnt 8'\n",
    "\n",
    "process = subprocess.Popen(command, shell=True)\n",
    "print(f\"shell process id: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "internlm_chat_7b_usmle_test_generate = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_generate_chat.json'))\n",
    "internlm_chat_7b_usmle_test_inference = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_inference_chat.json'))\n",
    "\n",
    "inference_acc = 0\n",
    "generate_acc = 0\n",
    "i_g = 0\n",
    "for di,dg in zip(internlm_chat_7b_usmle_test_inference, internlm_chat_7b_usmle_test_generate):\n",
    "    pred_i = ['A','B','C','D','E'][np.argmin(di['label_loss'])]\n",
    "    pred_g = dg['output'].replace(\"<eoa>\",\"\").replace(\"</s>\",\"\").strip()[0]\n",
    "    gt = di['answer_idx']\n",
    "    inference_acc += (pred_i == gt)\n",
    "    generate_acc += (pred_g == gt)\n",
    "    i_g += (pred_i == pred_g)\n",
    "    \n",
    "print(f\"inference_acc: {inference_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"generate_acc: {generate_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"i_g: {i_g/len(internlm_chat_7b_usmle_test_inference)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_llm_gyq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
