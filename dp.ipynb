{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Clash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"Start initial compatible provider 主站加速\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"Start initial compatible provider Bilibili哔哩哔哩\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"Start initial compatible provider 广告屏蔽\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"Start initial compatible provider 海外游戏平台\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"Start initial compatible provider 选择节点\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"Start initial compatible provider NETFLIX\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"RESTful API listening at: [::]:9090\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"HTTP proxy listening at: [::]:7890\"\n",
      "time=\"2023-11-13T16:15:42+08:00\" level=info msg=\"SOCKS proxy listening at: [::]:7891\"\n",
      "Clash is running, pid: 142378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if not result.stdout:\n",
    "    subprocess.Popen(\"~/tools/clash/clash\", shell=True)\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(f\"Clash is running, pid: {result.stdout}\")\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-13 16:17:13--  http://www.google.com/\n",
      "Connecting to 10.1.8.50:33128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘index.html.1’\n",
      "\n",
      "    [ <=>                                   ] 19,656      --.-K/s   in 0.03s   \n",
      "\n",
      "2023-11-13 16:17:13 (579 KB/s) - ‘index.html.1’ saved [19656]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget www.google.com\n",
    "!rm -f index.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clash: no process found\n"
     ]
    }
   ],
   "source": [
    "!killall clash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_name = \"internlm/internlm-7b\"\n",
    "# model_name = \"baichuan-inc/Baichuan2-7B-Base\"\n",
    "\n",
    "local_dir = os.path.join(os.environ['my_models_dir'], model_name.split(\"/\")[-1])\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import snapshot_download as huggingface_snapshot_download\n",
    "huggingface_snapshot_download(model_name, \n",
    "                              cache_dir=local_dir,\n",
    "                              local_dir=local_dir, \n",
    "                              local_dir_use_symlinks=False, \n",
    "                              ignore_patterns=[\"*.h5\",\"*safetensors\",\"*msgpack\"],\n",
    "                              etag_timeout=60,\n",
    "                              force_download=True, \n",
    "                              resume_download=False)\n",
    "\n",
    "# from modelscope import snapshot_download as modelscope_snapshot_download; modelscope_snapshot_download('Shanghai_AI_Laboratory/internlm-7b', revision='v1.0.2',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/guoyiqiu/miniconda3/envs/med_llm_gyq/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from model import multithread_query_chatgpt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5a46a256d4412b8c5a5c54a2072b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:15:59,696 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer'))': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:15:59+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:34118 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.31:19984->119.29.29.29:53: i/o timeout\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:04,698 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer'))': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:04+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:34616 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: context deadline exceeded\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:09,701 - root - INFO - An exception occurred: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\n",
      "2023-11-13 16:16:09,703 - root - INFO - round: 1 error_inputs: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:09+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:35040 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.31:39113->223.5.5.5:53: i/o timeout\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3addc2b94c466ca26089c7acbdad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:14,715 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer'))': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:14+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:35522 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: context deadline exceeded\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:19,718 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer'))': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:19+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:35986 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: Post \\\"https://dns.alidns.com/dns-query\\\": context deadline exceeded\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:24,723 - root - INFO - An exception occurred: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\n",
      "2023-11-13 16:16:24,725 - root - INFO - round: 2 error_inputs: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:24+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:36440 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.31:41423->119.29.29.29:53: i/o timeout\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb7db23e2f14e7097e7ac12e6d37dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:29,736 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer'))': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:29+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:36864 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.31:62989->223.5.5.5:53: i/o timeout\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:16:34,740 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer'))': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-13T16:16:34+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:37364 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.31:42904->223.5.5.5:53: i/o timeout\"\n",
      "time=\"2023-11-13T16:16:39+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:37820 --> api.openai.com:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: Post \\\"https://doh.pub/dns-query\\\": context deadline exceeded\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/petrelfs/guoyiqiu/coding/gpt_re/dp.ipynb 单元格 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bs1/mnt/petrelfs/guoyiqiu/coding/gpt_re/dp.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m multithread_query_chatgpt([\u001b[39mdict\u001b[39;49m(query_input\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhello\u001b[39;49m\u001b[39m\"\u001b[39;49m)])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bs1/mnt/petrelfs/guoyiqiu/coding/gpt_re/dp.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39moutput: \u001b[39m\u001b[39m'\u001b[39m, output)\n",
      "File \u001b[0;32m~/coding/gpt_re/model/llm_utils.py:38\u001b[0m, in \u001b[0;36mmultithread_query_chatgpt\u001b[0;34m(inputs, thread_num, max_round, model_name, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(thread_num) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m     37\u001b[0m     future_to_input \u001b[39m=\u001b[39m {executor\u001b[39m.\u001b[39msubmit(get_output, i[\u001b[39m'\u001b[39m\u001b[39mquery_input\u001b[39m\u001b[39m'\u001b[39m]): i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs}\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m tqdm(concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mas_completed(future_to_input),total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(inputs)):\n\u001b[1;32m     39\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m future_to_input[future]\n\u001b[1;32m     40\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/med_llm_gyq/lib/python3.9/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/med_llm_gyq/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/med_llm_gyq/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m wait_timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m (of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) futures unfinished\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[39mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(wait_timeout)\n\u001b[1;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m waiter\u001b[39m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m~/miniconda3/envs/med_llm_gyq/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/med_llm_gyq/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = multithread_query_chatgpt([dict(query_input=\"hello\")])\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_query_template = \"Generate 10 statements about the topic {topic}. The statements should be true and brief and contain factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "false_query_template = \"Generate 10 false statements about the topic {topic}. The statements should be incorrect and brief and contain wrong factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "topic_true_examples = {\n",
    "    \"Cities\": \"Oranjestad is a city in Aruba.\" , \n",
    "    \"Inventions\": \"Grace Hopper invented the COBOL programming language.\" , \n",
    "    \"Chemical Elements\": \"Boron is used in the production of glass and ceramics.\" , \n",
    "    \"Animals\": \"The llama has a diet of herbivore.\" , \n",
    "    \"Companies\": \"Meta Platforms has headquarters in United States.\" , \n",
    "    \"Scientific Facts\": \"The Earth’s tides are primarily caused by the gravitational pull of the moon.\",\n",
    "    \"Medical\": \"Benign tumors typically grow slowly and do not invade surrounding tissues or spread to other areas.\"\n",
    "}\n",
    "topic_false_examples = {\n",
    "    \"Cities\": \"Wellington is a name of a country.\" ,\n",
    "    \"Inventions\": \"David Schwarz lived in France.\" ,\n",
    "    \"Chemical Elements\": \"Indium is in the Lanthanide group.\" ,\n",
    "    \"Animals\": \"The whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\" ,\n",
    "    \"Companies\": \"KDDI operates in the industry of Materials.\" , \n",
    "    \"Scientific Facts\": \"Ice sinks in water due to its higher density.\",\n",
    "    \"Medical\": \"The normal range for human body temperature is 50-55 degrees Celsius.\"\n",
    "}\n",
    "true_queries = [dict(query_input=true_query_template.format(topic=k, examples=v),topic=k,label=True) for (k,v) in topic_true_examples.items()]\n",
    "false_queries = [dict(query_input=false_query_template.format(topic=k, examples=v),topic=k,label=False) for (k,v) in topic_false_examples.items()]\n",
    "inputs = true_queries + false_queries\n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    outputs.extend(multithread_query_chatgpt(inputs, thread_num=8, model_name='gpt-4-1106-preview'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "九院数预处理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import fitz\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import re\n",
    "\n",
    "def get_data(i):\n",
    "    broken_files = []\n",
    "    advice_path = os.path.join(base_dir,\"多科室医嘱\",i+\".json\")\n",
    "    with open(advice_path, \"r\", encoding=\"gbk\") as f:\n",
    "        try:\n",
    "            advice = json.load(f)['Data']\n",
    "        except Exception as e:\n",
    "            broken_files.append(advice_path)\n",
    "            # print(f\"{i}: broken advice {advice_path}.\")\n",
    "            advice=None\n",
    "    \n",
    "    all_checks = []\n",
    "    check_path = os.path.join(base_dir,\"多科室检验报告\",i)\n",
    "    for fname in os.listdir(check_path):\n",
    "        if not fname.endswith(\".html\"):\n",
    "            continue\n",
    "        try:\n",
    "            with open(os.path.join(check_path, fname), \"r\") as f:\n",
    "                soup = BeautifulSoup(f, features=\"lxml\")\n",
    "        except Exception as e:\n",
    "            broken_files.append(os.path.join(check_path, fname))\n",
    "            # print(f\"{i}: broken check: {fname}.\")\n",
    "            continue\n",
    "            \n",
    "        if not soup.table:\n",
    "            continue\n",
    "        check_title = soup.h4.get_text(strip=True).replace(\"共 1 项\",\"\").strip()\n",
    "        # print('check_title: ', check_title)\n",
    "        info = dict(标题=check_title)\n",
    "        info_text = soup.div.get_text(strip=True).replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "        # print('info_text: ', info_text)\n",
    "        info_keys = [\"工作组:\",\"送检医生:\",\"报告时间:\",\"检验医生:\",\"送检时间:\",\"标本名称:\",\"性别:\",\"年龄:\",\"诊断:\",\"生理周期:\",\"申请号:\"]\n",
    "        for k1,k2 in zip(info_keys[:-1],info_keys[1:]):\n",
    "            value = re.search(f\"{k1}(.*?){k2}\", info_text)\n",
    "            if value:\n",
    "                info[k1[:-1]] = value.group(1).strip()\n",
    "        # print('info: ', info)\n",
    "        \n",
    "        table_head = []\n",
    "        for row in soup.table.find_all('th'):\n",
    "            table_head.append(row.get_text(strip=True))\n",
    "        \n",
    "        table_data = []\n",
    "        for row in soup.table.find_all('tr'):\n",
    "            row_data = [i.get_text(strip=True) for i in row.find_all('td')]\n",
    "            if row_data:\n",
    "                table_data.append(row_data)\n",
    "        check_df = pd.DataFrame(table_data, columns=table_head)\n",
    "        check_dict = check_df[[c for c in check_df.columns if c]].to_dict(orient=\"records\")\n",
    "        all_checks.append(dict(filename=fname, info=info, values=check_dict))\n",
    "    \n",
    "    all_emrs = []\n",
    "    emr_path = os.path.join(base_dir,\"多科室EMR\",i)\n",
    "    for fname in os.listdir(emr_path):\n",
    "        if not fname.endswith(\".pdf\"):\n",
    "            continue\n",
    "        try:\n",
    "            texts = []\n",
    "            doc = fitz.open(os.path.join(emr_path,fname))\n",
    "            # print(os.path.join(emr_path,fname))\n",
    "            for page in doc:\n",
    "                texts.append(page.get_text())\n",
    "        except Exception as e:\n",
    "            broken_files.append(os.path.join(emr_path,fname))\n",
    "            # print(f\"{i}: broken emr: {fname}.\")\n",
    "            continue\n",
    "        finally:\n",
    "            all_emrs.append(dict(filename=fname, texts=texts))\n",
    "    \n",
    "    return dict(zid=i,advice=advice, checks=all_checks, emrs=all_emrs), broken_files\n",
    "\n",
    "base_dir = os.path.join(os.environ['my_datasets_dir'], \"ninth\")\n",
    "check_ids = os.listdir(os.path.join(base_dir,\"多科室检验报告\"))\n",
    "advice_ids = [i.replace(\".json\",\"\") for i in os.listdir(os.path.join(base_dir,\"多科室医嘱\"))]\n",
    "emr_ids = os.listdir(os.path.join(base_dir,\"多科室EMR\"))\n",
    "overlap_ids = sorted(list(set(check_ids) & set(advice_ids) & set(emr_ids)))\n",
    "\n",
    "all_data = []\n",
    "all_broken_files = []\n",
    "process_num = cpu_count()\n",
    "print(f\"Total {len(overlap_ids)} files. process_num={process_num}\")\n",
    "\n",
    "with Pool(process_num) as p:\n",
    "    for data, broken_files in tqdm(p.imap(get_data, overlap_ids), total=len(overlap_ids)):\n",
    "        all_data.append(data)\n",
    "        all_broken_files.extend(broken_files)\n",
    "\n",
    "all_data = sorted(all_data, key=lambda x: x[\"zid\"])\n",
    "all_data = [d for d in all_data if d[\"advice\"] and d[\"checks\"] and d[\"emrs\"]]\n",
    "json.dump(all_broken_files, open(os.path.join(base_dir, \"ninth_broken_files.json\"), \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)\n",
    "json.dump(all_data[-10:], open(os.path.join(base_dir, \"ninth_data_sample.json\"), \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)\n",
    "json.dump(all_data, open(os.path.join(base_dir, \"ninth_data.json\"), \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pylcs\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "\n",
    "default_tmp = ('性别', '年龄', '入院时间', '出院时间', '门诊诊断', '入院诊断', '出院诊断', '入院时主要症状及体征', '主要化验结果', '特殊检查及重要会诊', '病程与治疗结果', '合并症', '出院时情况', '出院后建议', '治疗结果', \"主治医师\")\n",
    "tmp_orders = [(0,       1,        2,          3,        4,          5,          6,          7,                  8,              9,              10,             11,         12,          13,         14,         15),\n",
    "              (0,       1,        2,          3,        4,          5,          6,          7,                  10,              8,              9,             11,         12,          13,         14,         15)]\n",
    "    \n",
    "specaial_variants = dict(\n",
    "    入院时间=['入院日期',],\n",
    "    出院时间=['出院日期','死亡时间'],\n",
    "    门诊诊断=['门诊诊',],\n",
    "    入院诊断=['入院',],\n",
    "    出院诊断=['死亡诊断'],\n",
    "    入院时主要症状及体征=['入院情况'],\n",
    "    主要化验结果=['主要结果','检验结果'],\n",
    "    特殊检查及重要会诊=['特殊检查结果','特殊检验及重要会诊','动态心电图结果','术中冰冻结果','术中冰冻','病理结果'],\n",
    "    病程与治疗结果=['诊疗经过','经过','病程及治疗结果'],\n",
    "    出院时情况=['院时情况',\"死亡时情况\"],\n",
    "    出院后建议=['出院后用药及建议','出院后用药建议','其他'],\n",
    "    治疗结果=['治果'],\n",
    ")\n",
    "\n",
    "variants = lambda key :[key] + specaial_variants.get(key, []) + [(k[:i] + \"\\n\" + k[i:]).strip() for k in ([key] + specaial_variants.get(key, [])) for i in range(1,len(k))]\n",
    "is_checkout = lambda text: all(any(kv in text for kv in variants(key)) for key in default_tmp)\n",
    "\n",
    "\n",
    "\n",
    "my_datasets_dir = os.environ['my_datasets_dir']\n",
    "input_path = os.path.join(my_datasets_dir,\"ninth/ninth_data.json\")\n",
    "output_path = os.path.join(my_datasets_dir, \"ninth/checkout_data_with_checks.json\")\n",
    "\n",
    "data = json.load(open(input_path))\n",
    "checkout_data = []\n",
    "num_error = 0\n",
    "not_checkout = 0\n",
    "for d in tqdm(data):\n",
    "    for emr in d['emrs']:\n",
    "        checkout_id = d['zid']+ \"/\" +emr['filename']\n",
    "        try:\n",
    "            if not emr['texts']:\n",
    "                continue\n",
    "            \n",
    "            texts = [re.sub(r'第(\\s)*\\d+(\\s)*页','',t) for t in emr['texts']]\n",
    "            text = ' '.join([t.replace(os.path.commonprefix(texts),'') for t in texts]) if len(texts) > 1 else texts[0]\n",
    "            \n",
    "            if not is_checkout(text):\n",
    "                if (\"出院小结\" in checkout_id and \"小时入出院记录\" not in text):\n",
    "                    not_checkout += 1\n",
    "                    # print(checkout_id)\n",
    "                    # print(text)\n",
    "                continue\n",
    "            \n",
    "            tmp = []\n",
    "            for key in default_tmp:\n",
    "                for kv in variants(key):\n",
    "                    if kv in text:\n",
    "                        tmp.append((kv,key))\n",
    "                        break\n",
    "            \n",
    "            for order in tmp_orders:\n",
    "                new_tmp = [tmp[i] for i in order]\n",
    "                new_tmp_str = ''.join([t[0] for t in new_tmp])\n",
    "                if pylcs.lcs_sequence_length(new_tmp_str,text) == len(new_tmp_str):\n",
    "                    tmp = new_tmp\n",
    "                    break\n",
    "            \n",
    "            info_dict = dict(文件ID=checkout_id)\n",
    "            for (kv,k),(nkv,nk) in zip(tmp[:-1],tmp[1:]):\n",
    "                # print(f'kv:{kv} nkv:{nkv}')\n",
    "                match = re.search(f\"{kv}(.*?){nkv}\", text, re.DOTALL)\n",
    "                span = match.span()\n",
    "                # print(f'span:{span}')\n",
    "                text = text[span[0]:]\n",
    "                info_dict[k] = match.group(1).replace(\"：\",\"\").strip()\n",
    "            \n",
    "            advice = info_dict['出院后建议']\n",
    "            advice = re.sub(\"预约[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"下次来院时间[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"健康宣教[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"请输入出院后建议[\\s\\S]*\",\"\",advice).strip()\n",
    "            info_dict['出院后建议'] = advice\n",
    "            \n",
    "            related_checks = []\n",
    "            time_span = []\n",
    "            default_time_stamp = ['2023','12','31','23','59']\n",
    "            for string in [info_dict['入院时间'],info_dict['出院时间']]:\n",
    "                # print('string: ', string)\n",
    "                time_stamp = re.findall(\"[\\d]+\",string)\n",
    "                # print('time_stamp: ', time_stamp)\n",
    "                time_stamp += default_time_stamp[len(time_stamp):]\n",
    "                time_stamp = ' '.join(time_stamp)\n",
    "                time_span.append(datetime.strptime(time_stamp, \"%Y %m %d %H %M\"))\n",
    "            # print('time_span: ', time_span)\n",
    "            \n",
    "            key_set = set(['检验项', '药品名称', '细菌名称'])\n",
    "            value_set = set(['异常标识', '结果标识', '结果'])\n",
    "            \n",
    "            value_map = [('^N$','[正常]'),(\"^敏感[\\s\\S]*\",\"[敏感]\"),(\"^耐药[\\s\\S]*\",\"[耐药]\"),(\"^中介[\\s\\S]*\",\"[中介]\"),(\"^$\",'[异常]'),(\"阴.*性.*\",\"[阴性]\"),(\"阳.*性.*\",\"[阳性]\"),(\"S\",\"[阴性]\"),(\"R\",\"[阳性]\"),(\"T\",\"[中介]\"),(\"弱阳性\",\"[弱阳性]\"),(\"极弱阳性\",\"[极弱阳性]\")]\n",
    "            for check in d['checks']:\n",
    "                check_time = datetime.strptime(check['info']['报告时间'], \"%Y/%m/%d %H:%M\")\n",
    "                if time_span[0] < check_time < time_span[1]:\n",
    "                    check_dict = {}\n",
    "                    check_dict['header'] = check['info']['标本名称']\n",
    "                    check_dict['values'] = {}\n",
    "                    key_name = '检验项'\n",
    "                    value_name = '异常标识'\n",
    "                    if not check['values']:\n",
    "                        continue\n",
    "                    for key in key_set:\n",
    "                        if key in check['values'][0]:\n",
    "                            key_name = key\n",
    "                            break\n",
    "                    for value in value_set:\n",
    "                        if value in check['values'][0]:\n",
    "                            value_name = value\n",
    "                            break\n",
    "                    for c in check['values']:\n",
    "                        key_item = c[key_name]\n",
    "                        value_item = ''\n",
    "                        if c[value_name] is None:\n",
    "                            continue\n",
    "                        \n",
    "                        for pattern,flag in value_map:\n",
    "                            if re.match(pattern, c[value_name]):\n",
    "                                value_item = flag\n",
    "                                break\n",
    "                        if not key_item or not value_item:\n",
    "                            print(c)\n",
    "                            continue\n",
    "                        check_dict['values'][key_item] = value_item\n",
    "                    related_checks.append(check_dict)\n",
    "            if not related_checks:\n",
    "                continue\n",
    "            info_dict['完整化验结果'] = related_checks\n",
    "            checkout_data.append(info_dict)\n",
    "        except Exception as e:\n",
    "            tb_str = traceback.format_tb(e.__traceback__)\n",
    "            print(f\"ERROR: {checkout_id} LINE:{tb_str} {e}\")\n",
    "            num_error += 1\n",
    "        break\n",
    "print(f\"num error:{num_error}\")\n",
    "print(f\"not_checkout:{not_checkout}\")\n",
    "print(f\"success num :{len(checkout_data)}\")\n",
    "json.dump(checkout_data, open(output_path,'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm_tok = AutoTokenizer.from_pretrained(os.path.join(os.environ['my_models_dir'],'internlm-7b'), trust_remote_code=True)\n",
    "data_path = os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_with_checks.json\")\n",
    "eval_ratio = 0.01\n",
    "max_length = 768\n",
    "max_length2 = 2048\n",
    "\n",
    "df = pd.read_json(data_path)\n",
    "print('full df size: ', df.shape[0])\n",
    "\n",
    "data_df = df.apply(lambda r: r if (r['门诊诊断'] and r['出院诊断'] and r['门诊诊断'] not in r['出院诊断']) else None,axis=1).dropna()\n",
    "data_df = data_df.apply(lambda r: pd.Series(dict(\n",
    "    input=\"性别:\"+r['性别']+\"\\n年龄:\"+r['年龄']+\"\\n入院时主要症状及体征:\"+r['入院时主要症状及体征']+\"\\n特殊检查及重要会诊:\"+r['特殊检查及重要会诊']+\"\\n出院诊断:\",\n",
    "    data=r['完整化验结果'],\n",
    "    output=r['出院诊断'])),axis=1)\n",
    "# data_df['input'] = data_df.apply(lambda r: r['input'].replace(r['output','未知']), axis=1)\n",
    "print(f\"filterd by 门诊and出院: {data_df.shape[0]} left\")\n",
    "\n",
    "data_df['num_tokens'] = data_df.apply(lambda r : len(llm_tok(r['input']+r['output'])['input_ids']),axis=1)\n",
    "data_df = data_df[data_df['num_tokens'] < max_length]\n",
    "print(f\"filterd by length: {data_df.shape[0]} left\")\n",
    "\n",
    "data_df = data_df.sample(frac=1)\n",
    "data_df_text_dicts = data_df.apply(lambda r: pd.Series(dict(input=r['input']+'完整化验结果: '+', '.join([f\"{k}:{v.replace('[','').replace(']','')}\" for dict in r['data'] for (k,v) in dict['values'].items()]),output=r['output'])), axis=1)\n",
    "data_df_text_dicts['num_tokens'] = data_df_text_dicts.apply(lambda r : len(llm_tok(r['input']+r['output'])['input_ids']),axis=1)\n",
    "\n",
    "data_df = data_df[data_df_text_dicts['num_tokens'] < max_length2]\n",
    "data_df_text_dicts = data_df_text_dicts[data_df_text_dicts['num_tokens'] < max_length2]\n",
    "data_df_no_dicts = data_df.drop(columns=['data'])\n",
    "print(f'filterd by length 2: {data_df.shape[0]} left')\n",
    "\n",
    "train_df = data_df.iloc[:int(len(data_df)*(1-eval_ratio))]\n",
    "eval_df = data_df.iloc[int(len(data_df)*(1-eval_ratio)):]\n",
    "train_df_text_dicts = data_df_text_dicts.iloc[:int(len(data_df_text_dicts)*(1-eval_ratio))]\n",
    "eval_df_text_dicts = data_df_text_dicts.iloc[int(len(data_df_text_dicts)*(1-eval_ratio)):]\n",
    "train_df_no_dicts = data_df_no_dicts.iloc[:int(len(data_df_no_dicts)*(1-eval_ratio))]\n",
    "eval_df_no_dicts = data_df_no_dicts.iloc[int(len(data_df_no_dicts)*(1-eval_ratio)):]\n",
    "\n",
    "json.dump(train_df.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_train.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(eval_df.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_eval.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(train_df_no_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_train_no_dicts.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(eval_df_no_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_eval_no_dicts.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(train_df_text_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_train_text_dicts.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(eval_df_text_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_eval_text_dicts.json\"),'w'), ensure_ascii=False)\n",
    "\n",
    "print(data_df['num_tokens'].describe())\n",
    "print(data_df_text_dicts['num_tokens'].describe())\n",
    "data_df_text_dicts['num_tokens'].hist(bins=1000,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../my_datasets/ninth/checkout_data.json\")\n",
    "print(f\"有效出院小结数量：{df.shape[0]}\")\n",
    "print(f\"门诊 not in 出院：{df.apply(lambda r : r if r['门诊诊断'] and r['出院诊断'] and (r['门诊诊断'] not in r['出院诊断']) else None,axis=1).dropna().shape[0]}\")\n",
    "print(f\"门诊 != 出院：{df.apply(lambda r : r if r['门诊诊断'] and r['出院诊断'] and (r['门诊诊断'] != r['出院诊断']) else None,axis=1).dropna().shape[0]}\")\n",
    "advice_len = df.apply(lambda r : len(r['出院后建议']) ,axis=1)\n",
    "print(f\"出院建议平均长度：{advice_len.mean()}\")\n",
    "advice_len.hist(bins=advice_len.max(),log=True)\n",
    "print(advice_len.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仁济住院数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_name_list = [name for name in os.listdir('./') if name.endswith('.csv')]\n",
    "dfs = {file_name:pd.read_csv(file_name, low_memory=False) for file_name in file_name_list}\n",
    "for df in dfs.values():\n",
    "    df.rename(columns={'jzzsy': '住院号'}, inplace=True)\n",
    "print(\"文件名:行数\")\n",
    "[(i,len(dfs[i])) for i in dfs]\n",
    "print(\"文件名:住院号数量\")\n",
    "[(i,len(set(dfs[i]['住院号']))) for i in dfs]\n",
    "intersection = set(dfs['202303_medical_history_enter.csv']['住院号'])\n",
    "black_list = set([\n",
    "    '202303_medical_history_leave_24h.csv',\n",
    "    '202303_medical_history_op_first_disease.csv',\n",
    "    '202303_medical_history_operation.csv',\n",
    "    '202303_medical_history_routine.csv'\n",
    "])\n",
    "union = set()\n",
    "for i in dfs:\n",
    "    union = union.union(set(dfs[i]['住院号']))\n",
    "\n",
    "for i in set(file_name_list) - black_list:\n",
    "    new_set = set(dfs[i]['住院号'])\n",
    "    intersection = new_set & intersection\n",
    "print(f\"住院号交集数量:{len(intersection)}, 重合比例:{len(intersection)}/{len(union)} {len(intersection)/len(union)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "advice = pd.read_csv(\"data/202303出院/202303出院有放射报告-医嘱.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "check = pd.read_csv(\"data/202303出院/202303出院有放射报告-检验.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "test = pd.read_csv(\"data/202303出院/202303出院有放射报告-检查.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "xlsx = pd.ExcelFile('data/202303出院/202303出院有放射报告-病史相关.xlsx')\n",
    "history = {sheet_name : xlsx.parse(sheet_name).rename(columns={'jzzsy': '住院号'}) for sheet_name in xlsx.sheet_names}\n",
    "\n",
    "\n",
    "zids = set(history['病案首页']['住院号'])\n",
    "chuyuan = []\n",
    "for zid in tqdm(zids):\n",
    "    d = {}\n",
    "    d['住院号'] = zid\n",
    "    for sheet_name in history:\n",
    "        sheet = history[sheet_name]\n",
    "        if \"住院号\" in sheet.columns:\n",
    "            d[sheet_name] = sheet[sheet['住院号'] == zid].to_dict(orient='records')\n",
    "    d['医嘱'] = advice[advice['住院号'] == zid].to_dict(orient='records')\n",
    "    d['检验'] = check[check['住院号'] == zid].to_dict(orient='records')\n",
    "    d['检查'] = test[test['住院号'] == zid].to_dict(orient='records')\n",
    "    chuyuan.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "chuyuan_sample = chuyuan[:20]\n",
    "json.dump(chuyuan_sample, open(\"data/chuyuan_data_sample.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)\n",
    "# json.dump(chuyuan, open(\"data/chuyuan_data.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chuyuan = json.load(open(\"data/chuyuan_data.json\"))\n",
    "check_nums = [len(d['检验']) for d in chuyuan]\n",
    "check_normal = [len([c for c in d['检验'] if c['结果值异常标志'] == 'NO']) for d in chuyuan]\n",
    "check_abnormal = [(i-j) for i,j in zip(check_nums, check_normal)]\n",
    "print(f\"average_check_num: {(sum(check_nums)/len(check_nums))}\\n\" \\\n",
    "    f\"average normal: {sum(check_normal)/len(check_normal)}\\n\" \\\n",
    "    f\"average abnormal: {sum(check_abnormal)/len(check_abnormal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Stdout2File\n",
    "from datetime import datetime\n",
    "\n",
    "int2date = lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M%S\").strftime(\"%Y/%m/%d %H:%M:%S\").replace(r\"2023/3\",r\"2023/03\")\n",
    "date2int = lambda x: int(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\").strftime(\"%Y%m%d%H%M%S\"))\n",
    "chuyuan_sample = json.load(open(\"data/chuyuan_data_sample.json\"))\n",
    "\n",
    "def time_stamps(d):\n",
    "    time_stamps = []\n",
    "    time_stamps.append((int2date(str(d['出院记录'][0]['入院日期时间'])),  f\"入院时间 {d['病案首页'][0]['入院诊断名称']}\"))\n",
    "    time_stamps.append((int2date(str(d['出院记录'][0]['出院日期时间'])[:14]),  '出院时间'))\n",
    "    if d.get(\"手术记录\"):\n",
    "        for i,op in enumerate(d['手术记录']):\n",
    "            time_stamps.append((int2date(str(op['手术开始日期时间'])),  f\"手术{i}:{op['手术及操作编码对应名称']} 开始时间\"))\n",
    "            if op.get('手术结束日期时间').strip():\n",
    "                time_stamps.append((int2date(str(op['手术结束日期时间'][:14])),  f\"手术{i}:{op['手术及操作编码对应名称']} 结束时间\"))\n",
    "    if d.get(\"日常病程记录\"):\n",
    "        for i,rec in enumerate(d['日常病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"日常病程{i} 记录时间:\"))\n",
    "    if d.get('首次病程记录'):\n",
    "        for i, rec in enumerate(d['首次病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"首次病程记录{i} 记录时间:\"))\n",
    "    if d.get('术后首次病程记录'):\n",
    "        for i, rec in enumerate(d['术后首次病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"术后首次病程记录{i} 记录时间:\"))\n",
    "    if d.get('医嘱'):\n",
    "        for i, rec in enumerate(d['医嘱']):\n",
    "            time_stamps.append((rec['开始时间'].replace(r\"2023/3\",r\"2023/03\"),  f\"医嘱{i}:{rec['医嘱名称']} 开始时间:\"))\n",
    "            if rec.get('结束时间'):\n",
    "                time_stamps.append((rec['结束时间'].replace(r\"2023/3\",r\"2023/03\"),  f\"医嘱{i}:{rec['医嘱名称']} 结束时间:\"))\n",
    "    if d.get('检验'):\n",
    "        for i, rec in enumerate(d['检验']):\n",
    "            time_stamps.append((rec['报告日期'].replace(r\"2023/3\",r\"2023/03\"),  f\"检验{i}:{rec['报告名称']} {rec['检验项目']} 报告日期:\"))\n",
    "    if d.get('检查'):\n",
    "        for i, rec in enumerate(d['检查']):\n",
    "            time_stamps.append((rec['报告日期'].replace(r\"2023/3\",r\"2023/03\"), f\"检查{i}:{rec['报告名称']} 报告日期:\"))\n",
    "    sorted_time_stamps = sorted(time_stamps, key=lambda x: date2int(x[0]))\n",
    "    return sorted_time_stamps\n",
    "\n",
    "with Stdout2File(\"data/chuyuan_time_stamps.txt\"):\n",
    "    for d in chuyuan_sample:\n",
    "        for s in time_stamps(d):\n",
    "            print(s)\n",
    "        print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "date2int = lambda x: int(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\").strftime(\"%Y%m%d%H%M%S\"))\n",
    "data = json.load(open(\"data/chuyuan/chuyuan_data.json\"))\n",
    "\n",
    "def advice_type(advice):\n",
    "    if advice.get(\"规格\") and advice['单次用量']!=1.0:\n",
    "        return \"药物\"\n",
    "    # if \"会诊\" in advice['医嘱名称']:\n",
    "    #     return \"会诊\"\n",
    "    keywords = [\"平扫\",\"脑电\",\"心电\",\"CT\",\"检查\",\"分析\",\"试验\",\"检测\",\"MRI\",\"静脉血\",\"超声\"]\n",
    "    for k in keywords:\n",
    "        if k in advice['医嘱名称']:\n",
    "            return \"检查\"\n",
    "    return False\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for d in tqdm(data):\n",
    "    if (not d.get(\"入院记录\") and not d.get(\"24小时出入院记录\")) or (not d.get(\"医嘱\") or not d.get(\"检验\")):\n",
    "        continue\n",
    "    text_dict =dict(\n",
    "        病人年龄=d['检验'][0]['年龄'],\n",
    "        病人性别=d['病案首页'][0]['性别'],\n",
    "        入院诊断名称=d['病案首页'][0]['入院诊断名称'],\n",
    "        主诉=d['入院记录'][0]['主诉'] if d.get('入院记录') else d['24小时出入院记录'][0]['主诉'],\n",
    "    )\n",
    "    \n",
    "    checks = []\n",
    "    for c in d['检验']:\n",
    "        try:\n",
    "            c['报告日期'] = date2int(c['报告日期'])\n",
    "            checks.append(c)\n",
    "        except:\n",
    "            continue\n",
    "    checks = sorted(checks, key=lambda x: x['报告日期'])\n",
    "    operations = sorted(d['手术记录'], key=lambda x: x['手术开始日期时间'])\n",
    "    # tests = sorted(d['检查'], key=lambda x: x['报告日期'])\n",
    "    \n",
    "    advices = []\n",
    "    for a in d['医嘱']:\n",
    "        try:\n",
    "            a['开始时间'] = date2int(a['开始时间'])\n",
    "            advices.append(a)\n",
    "        except:\n",
    "            continue\n",
    "    advices = sorted(advices, key=lambda x: x['开始时间'])\n",
    "    \n",
    "    # 如果有手术记录，过滤掉在第一次手术之后的所有信息\n",
    "    if operations:\n",
    "        first_op_time = operations[0]['手术开始日期时间']\n",
    "        checks = [c for c in checks if c['报告日期'] < first_op_time]\n",
    "        advices = [a for a in advices if a['开始时间'] < first_op_time]\n",
    "    \n",
    "    if not checks or not advices:\n",
    "        continue\n",
    "    \n",
    "    # 过滤掉在第一次检验报告出来之前的医嘱\n",
    "    advices = [a for a in advices if a['开始时间'] > checks[0]['报告日期']]\n",
    "    \n",
    "    # 过滤掉在最后一次检验报告出来之前的医嘱\n",
    "    # advices = [a for a in advices if a['开始时间'] > checks[-1]['报告日期']]\n",
    "    \n",
    "    # 只保留药物，会诊，检查三种医嘱\n",
    "    advices = [a for a in advices if advice_type(a)]\n",
    "    \n",
    "    # 对医嘱按时间聚类\n",
    "    new_advices = [dict(开始时间=a['开始时间'], 医嘱名称=a['医嘱名称'], 医嘱类型=advice_type(a)) for a in advices]\n",
    "    check_times = sorted(list(set([c['报告日期'] for c in checks])))\n",
    "    advice_groups = [[a for a in new_advices if check_times[i] < a['开始时间'] < check_times[i+1]] for i in range(len(check_times)-1)]\n",
    "    advice_groups = [a for a in advice_groups if a]\n",
    "\n",
    "    # 生成医嘱训练数据\n",
    "    text = \",\".join([f\"{k}: {v}\" for (k,v) in text_dict.items()])\n",
    "    for advice_group in advice_groups:\n",
    "        check_list = [dict(检验项目=c['检验项目'], 检验结果=c['检验结果'], 正常值范围=c['正常值范围'], 结果值异常标志=c['结果值异常标志'] if c['结果值异常标志']!=\"NO\" else \"正常\") for c in checks if c['报告日期'] < advice_group[0]['开始时间']]\n",
    "        advice_text = \",\".join([f\"{a['医嘱类型']}:{a['医嘱名称']}\" for a in advice_group])\n",
    "        train_data.append(dict(text=text,data=check_list,output=advice_text))\n",
    "\n",
    "json.dump(train_data, open(\"data/chuyuan/chuyuan_train.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_data[:50], open(\"data/chuyuan/chuyuan_train_sample.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HONESTY_EXTRACTION_TEMPLATE_EXP = \"\"\"USER:Pretend you are an honest person making statements about the world.\\nAssistant:\"\"\"\n",
    "HONESTY_EXTRACTION_TEMPLATE_REF = \"\"\"USER:Pretend you are a dishonest person making statements about the world.\\nAssistant:\"\"\"\n",
    "\n",
    "knows = {topic:{True:[],False:[]}for topic in topic_true_examples}\n",
    "\n",
    "for o in outputs:\n",
    "    ks = o['query_output'].split(\"<sep>\")\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"<sep\"))\n",
    "    ks = new_ks\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"\\n\"))\n",
    "    ks = new_ks\n",
    "    ks = [re.sub(\"[0-9]+. \",\"\", k).strip() for k in ks]\n",
    "    ks = [k for k in ks if k]\n",
    "    for k in ks:\n",
    "        if k not in knows[o['topic']][o['label']]:\n",
    "            knows[o['topic']][o['label']].append(k)\n",
    "    \n",
    "dst = []\n",
    "for topic in knows:\n",
    "    for i in range(min(len(knows[topic][True]),len(knows[topic][False]))):\n",
    "        true_input = HONESTY_EXTRACTION_TEMPLATE_EXP\n",
    "        false_input = HONESTY_EXTRACTION_TEMPLATE_REF\n",
    "        true_output = knows[topic][True][i]\n",
    "        false_output = knows[topic][False][i]\n",
    "        dst.append([dict(input=true_input,output=true_output,topic=topic,label=True),dict(input=false_input,output=false_output,topic=topic,label=False)])\n",
    "print('True-False Dst Size: ', len(dst))\n",
    "json.dump(knows, open(\"data/knows.json\", \"w\"), indent=4)\n",
    "json.dump(dst, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from model import INTERNLM_TEMPLATE\n",
    "\n",
    "\n",
    "usmle_test = list(jsonlines.open(\"data/usmle/questions/US/test.jsonl\"))\n",
    "usmle_test_for_infer = []\n",
    "\n",
    "for d in usmle_test:\n",
    "    options_strs = [f\"{op}: {d['options'][op]}\" for op in d['options']]\n",
    "    input = f\"Question: {d['question']} Options: {'; '.join(options_strs)}. Output: The correct answer is option\"\n",
    "    input = INTERNLM_TEMPLATE.format(input)\n",
    "    d['input'] = input\n",
    "    d['labels'] = ['A', 'B', 'C', 'D', 'E']\n",
    "    usmle_test_for_infer.append(d)\n",
    "\n",
    "json.dump(usmle_test_for_infer, open(\"data/mgpu_infer/usmle_test_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from model import *\n",
    "import os\n",
    "\n",
    "tqa = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/TruthfulQA-main/data/mc_task.json\"))\n",
    "print(tqa[0])\n",
    "tqa_dst = [dict(input=INTERNLM_TEMPLATE.format(d['question']),labels=list(d['mc1_targets'].keys()),gt=list(d['mc1_targets'].values()).index(1)) for d in tqa]\n",
    "print(tqa_dst[0])\n",
    "json.dump(tqa_dst, open(\"data/mgpu_infer/truthfulqa_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "medqa_test = json.load(open(\"data/usmle/questions/US/test.json\"))\n",
    "print('medqa_test: ', medqa_test[0])\n",
    "medqa_test_infer = []\n",
    "for d in medqa_test:\n",
    "    options = d['options']\n",
    "    option_text = \";\".join([f\"{k}: {options[k]}\" for k in options])\n",
    "    # input_text = INTERNLM_TEMPLATE.format(\"Question:\"+d['question']+\"; Options:\"+option_text) + \"The correct answer is option\"\n",
    "    input_text = INTERNLM_TEMPLATE.format(\"Options:\"+option_text) + \"The correct answer is option\"\n",
    "    medqa_test_infer.append(dict(input=input_text, labels=list(options.keys()), gt=d['answer']))\n",
    "print('medqa_test_infer: ', medqa_test_infer[0])\n",
    "json.dump(medqa_test_infer, open(\"data/mgpu_infer/medqa_test_noquestion_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.llm_utils import get_free_gpus\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "free_gpus = get_free_gpus()\n",
    "command = f'CUDA_VISIBLE_DEVICES={\",\".join([str(i) for i in free_gpus])} \\\n",
    "            torchrun --nproc_per_node {len(free_gpus)} --master_port=12570 /home/cs/yangyuchen/guoyiqiu/gpt_re/mgpu_infer.py \\\n",
    "            --func infer \\\n",
    "            --model_path /home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms \\\n",
    "            --dst_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer/medqa_test_noquestion_inference.json \\\n",
    "            --save_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_noquestion_inference.json \\\n",
    "            --mnt 8 '\n",
    "\n",
    "process = subprocess.Popen(command, shell=True)\n",
    "print(f\"shell process id: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "internlm_chat_7b_usmle_test_generate = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_generate.json'))\n",
    "internlm_chat_7b_usmle_test_inference = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_inference.json'))\n",
    "\n",
    "inference_acc = 0\n",
    "generate_acc = 0\n",
    "i_g = 0\n",
    "for di,dg in zip(internlm_chat_7b_usmle_test_inference, internlm_chat_7b_usmle_test_generate):\n",
    "    pred_i = ['A','B','C','D','E'][np.argmin(di['label_loss'])]\n",
    "    pred_g = dg['output'].replace(\"<eoa>\",\"\").replace(\"</s>\",\"\").strip()[0]\n",
    "    gt = di['answer_idx']\n",
    "    inference_acc += (pred_i == gt)\n",
    "    generate_acc += (pred_g == gt)\n",
    "    i_g += (pred_i == pred_g)\n",
    "    \n",
    "print(f\"inference_acc: {inference_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"generate_acc: {generate_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"i_g: {i_g/len(internlm_chat_7b_usmle_test_inference)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "medqa_test_withquestion_result = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_inference.json\"))\n",
    "acc_w = [['A','B','C','D','E'][np.argmin(d['label_loss'])]==d['gt'] for d in medqa_test_withquestion_result]\n",
    "print(f\"acc_w: {sum(acc_w)/len(acc_w)}\")\n",
    "medqa_test_noquestion_result = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_noquestion_inference.json\"))\n",
    "acc_wo = [['A','B','C','D','E'][np.argmin(d['label_loss'])]==d['gt'] for d in medqa_test_noquestion_result]\n",
    "print(f\"acc_wo: {sum(acc_wo)/len(acc_wo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_llm_gyq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
