{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-33b-v1.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-33b-v1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch clash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"Start initial compatible provider Bilibili哔哩哔哩\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"Start initial compatible provider 海外游戏平台\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"Start initial compatible provider NETFLIX\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"Start initial compatible provider 广告屏蔽\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"Start initial compatible provider 选择节点\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"Start initial compatible provider 主站加速\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"HTTP proxy listening at: [::]:7890\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"SOCKS proxy listening at: [::]:7891\"\n",
      "time=\"2023-10-19T11:00:24+08:00\" level=info msg=\"RESTful API listening at: [::]:9090\"\n",
      "Clash is running, pid: 629244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if not result.stdout:\n",
    "    subprocess.Popen(\"cd ~/guoyiqiu/clash; ./clash\", shell=True)\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(f\"Clash is running, pid: {result.stdout}\")\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 2555334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "from model import multithread_query_chatgpt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "true_query_template = \"Generate 10 statements about the topic {topic}. The statements should be true and brief and contain factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "false_query_template = \"Generate 10 false statements about the topic {topic}. The statements should be incorrect and brief and contain wrong factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "topic_true_examples = {\n",
    "    \"Cities\": \"Oranjestad is a city in Aruba.\" , \n",
    "    \"Inventions\": \"Grace Hopper invented the COBOL programming language.\" , \n",
    "    \"Chemical Elements\": \"Boron is used in the production of glass and ceramics.\" , \n",
    "    \"Animals\": \"The llama has a diet of herbivore.\" , \n",
    "    \"Companies\": \"Meta Platforms has headquarters in United States.\" , \n",
    "    \"Scientific Facts\": \"The Earth’s tides are primarily caused by the gravitational pull of the moon.\",\n",
    "    \"Medical\": \"Benign tumors typically grow slowly and do not invade surrounding tissues or spread to other areas.\"\n",
    "}\n",
    "topic_false_examples = {\n",
    "    \"Cities\": \"Wellington is a name of a country.\" ,\n",
    "    \"Inventions\": \"David Schwarz lived in France.\" ,\n",
    "    \"Chemical Elements\": \"Indium is in the Lanthanide group.\" ,\n",
    "    \"Animals\": \"The whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\" ,\n",
    "    \"Companies\": \"KDDI operates in the industry of Materials.\" , \n",
    "    \"Scientific Facts\": \"Ice sinks in water due to its higher density.\",\n",
    "    \"Medical\": \"The normal range for human body temperature is 50-55 degrees Celsius.\"\n",
    "}\n",
    "true_queries = [dict(query_input=true_query_template.format(topic=k, examples=v),topic=k,label=True) for (k,v) in topic_true_examples.items()]\n",
    "false_queries = [dict(query_input=false_query_template.format(topic=k, examples=v),topic=k,label=False) for (k,v) in topic_false_examples.items()]\n",
    "inputs = true_queries + false_queries\n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    outputs.extend(multithread_query_chatgpt(inputs, thread_num=8, temperature=1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HONESTY_EXTRACTION_TEMPLATE_EXP = \"\"\"USER:Pretend you are an honest person making statements about the world.\\nAssistant:\"\"\"\n",
    "HONESTY_EXTRACTION_TEMPLATE_REF = \"\"\"USER:Pretend you are a dishonest person making statements about the world.\\nAssistant:\"\"\"\n",
    "\n",
    "knows = {topic:{True:[],False:[]}for topic in topic_true_examples}\n",
    "\n",
    "for o in outputs:\n",
    "    ks = o['query_output'].split(\"<sep>\")\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"<sep\"))\n",
    "    ks = new_ks\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"\\n\"))\n",
    "    ks = new_ks\n",
    "    ks = [re.sub(\"[0-9]+. \",\"\", k).strip() for k in ks]\n",
    "    ks = [k for k in ks if k]\n",
    "    for k in ks:\n",
    "        if k not in knows[o['topic']][o['label']]:\n",
    "            knows[o['topic']][o['label']].append(k)\n",
    "    \n",
    "dst = []\n",
    "for topic in knows:\n",
    "    for i in range(min(len(knows[topic][True]),len(knows[topic][False]))):\n",
    "        true_input = HONESTY_EXTRACTION_TEMPLATE_EXP\n",
    "        false_input = HONESTY_EXTRACTION_TEMPLATE_REF\n",
    "        true_output = knows[topic][True][i]\n",
    "        false_output = knows[topic][False][i]\n",
    "        dst.append([dict(input=true_input,output=true_output,topic=topic,label=True),dict(input=false_input,output=false_output,topic=topic,label=False)])\n",
    "print('True-False Dst Size: ', len(dst))\n",
    "json.dump(knows, open(\"data/knows.json\", \"w\"), indent=4)\n",
    "json.dump(dst, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from model import INTERNLM_TEMPLATE\n",
    "\n",
    "\n",
    "usmle_test = list(jsonlines.open(\"data/usmle/questions/US/test.jsonl\"))\n",
    "usmle_test_for_infer = []\n",
    "\n",
    "for d in usmle_test:\n",
    "    options_strs = [f\"{op}: {d['options'][op]}\" for op in d['options']]\n",
    "    input = f\"Question: {d['question']} Options: {'; '.join(options_strs)}. Output: The correct answer is option\"\n",
    "    input = INTERNLM_TEMPLATE.format(input)\n",
    "    d['input'] = input\n",
    "    d['labels'] = ['A', 'B', 'C', 'D', 'E']\n",
    "    usmle_test_for_infer.append(d)\n",
    "\n",
    "json.dump(usmle_test_for_infer, open(\"data/mgpu_infer/usmle_test_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.llm_utils import get_free_gpus\n",
    "import subprocess\n",
    "\n",
    "free_gpus = get_free_gpus()\n",
    "command = f'CUDA_VISIBLE_DEVICES={\",\".join([str(i) for i in free_gpus])} \\\n",
    "            torchrun --nproc_per_node {len(free_gpus)} --master_port=12570 /home/cs/yangyuchen/guoyiqiu/gpt_re/mgpu_infer.py \\\n",
    "            --func gen \\\n",
    "            --model_path /home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/ \\\n",
    "            --dst_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer/usmle_test_inference.json \\\n",
    "            --save_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/vicuna-7b_usmle_test_generate.json \\\n",
    "            --mnt 8'\n",
    "\n",
    "process = subprocess.Popen(command, shell=True)\n",
    "print(f\"shell process id: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "internlm_chat_7b_usmle_test_generate = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_generate_chat.json'))\n",
    "internlm_chat_7b_usmle_test_inference = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_inference_chat.json'))\n",
    "\n",
    "inference_acc = 0\n",
    "generate_acc = 0\n",
    "i_g = 0\n",
    "for di,dg in zip(internlm_chat_7b_usmle_test_inference, internlm_chat_7b_usmle_test_generate):\n",
    "    pred_i = ['A','B','C','D','E'][np.argmin(di['label_loss'])]\n",
    "    pred_g = dg['output'].replace(\"<eoa>\",\"\").replace(\"</s>\",\"\").strip()[0]\n",
    "    gt = di['answer_idx']\n",
    "    inference_acc += (pred_i == gt)\n",
    "    generate_acc += (pred_g == gt)\n",
    "    i_g += (pred_i == pred_g)\n",
    "    \n",
    "print(f\"inference_acc: {inference_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"generate_acc: {generate_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"i_g: {i_g/len(internlm_chat_7b_usmle_test_inference)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_llm_gyq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
