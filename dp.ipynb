{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Clash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if not result.stdout:\n",
    "    subprocess.Popen(\"~/tools/clash/clash\", shell=True)\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(f\"Clash is running, pid: {result.stdout}\")\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!killall clash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '你', '好', '呀', '？', '[SEP]']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.add_special_tokens=False\n",
    "tok.batch_decode(tok(\"你好呀？\")['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "src = torch.rand(10, 32, 128)\n",
    "out = transformer_encoder(src)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "from model import multithread_query_chatgpt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "true_query_template = \"Generate 10 statements about the topic {topic}. The statements should be true and brief and contain factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "false_query_template = \"Generate 10 false statements about the topic {topic}. The statements should be incorrect and brief and contain wrong factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "topic_true_examples = {\n",
    "    \"Cities\": \"Oranjestad is a city in Aruba.\" , \n",
    "    \"Inventions\": \"Grace Hopper invented the COBOL programming language.\" , \n",
    "    \"Chemical Elements\": \"Boron is used in the production of glass and ceramics.\" , \n",
    "    \"Animals\": \"The llama has a diet of herbivore.\" , \n",
    "    \"Companies\": \"Meta Platforms has headquarters in United States.\" , \n",
    "    \"Scientific Facts\": \"The Earth’s tides are primarily caused by the gravitational pull of the moon.\",\n",
    "    \"Medical\": \"Benign tumors typically grow slowly and do not invade surrounding tissues or spread to other areas.\"\n",
    "}\n",
    "topic_false_examples = {\n",
    "    \"Cities\": \"Wellington is a name of a country.\" ,\n",
    "    \"Inventions\": \"David Schwarz lived in France.\" ,\n",
    "    \"Chemical Elements\": \"Indium is in the Lanthanide group.\" ,\n",
    "    \"Animals\": \"The whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\" ,\n",
    "    \"Companies\": \"KDDI operates in the industry of Materials.\" , \n",
    "    \"Scientific Facts\": \"Ice sinks in water due to its higher density.\",\n",
    "    \"Medical\": \"The normal range for human body temperature is 50-55 degrees Celsius.\"\n",
    "}\n",
    "true_queries = [dict(query_input=true_query_template.format(topic=k, examples=v),topic=k,label=True) for (k,v) in topic_true_examples.items()]\n",
    "false_queries = [dict(query_input=false_query_template.format(topic=k, examples=v),topic=k,label=False) for (k,v) in topic_false_examples.items()]\n",
    "inputs = true_queries + false_queries\n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    outputs.extend(multithread_query_chatgpt(inputs, thread_num=8, temperature=1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "九院数预处理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import fitz\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import re\n",
    "\n",
    "def get_data(i):\n",
    "    broken_files = []\n",
    "    advice_path = os.path.join(base_dir,\"多科室医嘱\",i+\".json\")\n",
    "    with open(advice_path, \"r\", encoding=\"gbk\") as f:\n",
    "        try:\n",
    "            advice = json.load(f)['Data']\n",
    "        except Exception as e:\n",
    "            broken_files.append(advice_path)\n",
    "            # print(f\"{i}: broken advice {advice_path}.\")\n",
    "            advice=None\n",
    "    \n",
    "    all_checks = []\n",
    "    check_path = os.path.join(base_dir,\"多科室检验报告\",i)\n",
    "    for fname in os.listdir(check_path):\n",
    "        if not fname.endswith(\".html\"):\n",
    "            continue\n",
    "        try:\n",
    "            with open(os.path.join(check_path, fname), \"r\") as f:\n",
    "                soup = BeautifulSoup(f, features=\"lxml\")\n",
    "        except Exception as e:\n",
    "            broken_files.append(os.path.join(check_path, fname))\n",
    "            # print(f\"{i}: broken check: {fname}.\")\n",
    "            continue\n",
    "            \n",
    "        if not soup.table:\n",
    "            continue\n",
    "        check_title = soup.h4.get_text(strip=True).replace(\"共 1 项\",\"\").strip()\n",
    "        # print('check_title: ', check_title)\n",
    "        info = dict(标题=check_title)\n",
    "        info_text = soup.div.get_text(strip=True).replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "        # print('info_text: ', info_text)\n",
    "        info_keys = [\"工作组:\",\"送检医生:\",\"报告时间:\",\"检验医生:\",\"送检时间:\",\"标本名称:\",\"性别:\",\"年龄:\",\"诊断:\",\"生理周期:\",\"申请号:\"]\n",
    "        for k1,k2 in zip(info_keys[:-1],info_keys[1:]):\n",
    "            value = re.search(f\"{k1}(.*?){k2}\", info_text)\n",
    "            if value:\n",
    "                info[k1[:-1]] = value.group(1).strip()\n",
    "        # print('info: ', info)\n",
    "        \n",
    "        table_head = []\n",
    "        for row in soup.table.find_all('th'):\n",
    "            table_head.append(row.get_text(strip=True))\n",
    "        \n",
    "        table_data = []\n",
    "        for row in soup.table.find_all('tr'):\n",
    "            row_data = [i.get_text(strip=True) for i in row.find_all('td')]\n",
    "            if row_data:\n",
    "                table_data.append(row_data)\n",
    "        check_df = pd.DataFrame(table_data, columns=table_head)\n",
    "        check_dict = check_df[[c for c in check_df.columns if c]].to_dict(orient=\"records\")\n",
    "        all_checks.append(dict(filename=fname, info=info, values=check_dict))\n",
    "    \n",
    "    all_emrs = []\n",
    "    emr_path = os.path.join(base_dir,\"多科室EMR\",i)\n",
    "    for fname in os.listdir(emr_path):\n",
    "        if not fname.endswith(\".pdf\"):\n",
    "            continue\n",
    "        try:\n",
    "            texts = []\n",
    "            doc = fitz.open(os.path.join(emr_path,fname))\n",
    "            # print(os.path.join(emr_path,fname))\n",
    "            for page in doc:\n",
    "                texts.append(page.get_text())\n",
    "        except Exception as e:\n",
    "            broken_files.append(os.path.join(emr_path,fname))\n",
    "            # print(f\"{i}: broken emr: {fname}.\")\n",
    "            continue\n",
    "        finally:\n",
    "            all_emrs.append(dict(filename=fname, texts=texts))\n",
    "    \n",
    "    return dict(zid=i,advice=advice, checks=all_checks, emrs=all_emrs), broken_files\n",
    "\n",
    "base_dir = \"/home/cs/yangyuchen/guoyiqiu/ninth\"\n",
    "check_ids = os.listdir(os.path.join(base_dir,\"多科室检验报告\"))\n",
    "advice_ids = [i.replace(\".json\",\"\") for i in os.listdir(os.path.join(base_dir,\"多科室医嘱\"))]\n",
    "emr_ids = os.listdir(os.path.join(base_dir,\"多科室EMR\"))\n",
    "overlap_ids = sorted(list(set(check_ids) & set(advice_ids) & set(emr_ids)))\n",
    "\n",
    "all_data = []\n",
    "all_broken_files = []\n",
    "process_num = cpu_count()\n",
    "print(f\"Total {len(overlap_ids)} files. process_num={process_num}\")\n",
    "\n",
    "with Pool(process_num) as p:\n",
    "    for data, broken_files in tqdm(p.imap(get_data, overlap_ids), total=len(overlap_ids)):\n",
    "        all_data.append(data)\n",
    "        all_broken_files.extend(broken_files)\n",
    "\n",
    "all_data = sorted(all_data, key=lambda x: x[\"zid\"])\n",
    "all_data = [d for d in all_data if d[\"advice\"] and d[\"checks\"] and d[\"emrs\"]]\n",
    "json.dump(all_broken_files, open(\"/home/cs/yangyuchen/guoyiqiu/ninth/ninth_broken_files.json\", \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)\n",
    "json.dump(all_data[-10:], open(\"/home/cs/yangyuchen/guoyiqiu/ninth/ninth_data_sample.json\", \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)\n",
    "json.dump(all_data, open(\"/home/cs/yangyuchen/guoyiqiu/ninth/ninth_data.json\", \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pylcs\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "default_tmp = ('性别', '年龄', '入院时间', '出院时间', '门诊诊断', '入院诊断', '出院诊断', '入院时主要症状及体征', '主要化验结果', '特殊检查及重要会诊', '病程与治疗结果', '合并症', '出院时情况', '出院后建议', '治疗结果', \"主治医师\")\n",
    "tmp_orders = [(0,       1,        2,          3,        4,          5,          6,          7,                  8,              9,              10,             11,         12,          13,         14,         15),\n",
    "              (0,       1,        2,          3,        4,          5,          6,          7,                  10,              8,              9,             11,         12,          13,         14,         15)]\n",
    "    \n",
    "specaial_variants = dict(\n",
    "    入院时间=['入院日期',],\n",
    "    出院时间=['出院日期','死亡时间'],\n",
    "    门诊诊断=['门诊诊',],\n",
    "    入院诊断=['入院',],\n",
    "    出院诊断=['死亡诊断'],\n",
    "    入院时主要症状及体征=['入院情况'],\n",
    "    主要化验结果=['主要结果','检验结果'],\n",
    "    特殊检查及重要会诊=['特殊检查结果','特殊检验及重要会诊','动态心电图结果','术中冰冻结果','术中冰冻','病理结果'],\n",
    "    病程与治疗结果=['诊疗经过','经过','病程及治疗结果'],\n",
    "    出院时情况=['院时情况',\"死亡时情况\"],\n",
    "    出院后建议=['出院后用药及建议','出院后用药建议','其他'],\n",
    "    治疗结果=['治果'],\n",
    ")\n",
    "\n",
    "variants = lambda key :[key] + specaial_variants.get(key, []) + [(k[:i] + \"\\n\" + k[i:]).strip() for k in ([key] + specaial_variants.get(key, [])) for i in range(1,len(k))]\n",
    "is_checkout = lambda text: all(any(kv in text for kv in variants(key)) for key in default_tmp)\n",
    "\n",
    "\n",
    "data = json.load(open(\"../my_datasets/ninth/ninth_data.json\"))\n",
    "checkout_data = []\n",
    "num_error = 0\n",
    "not_checkout = 0\n",
    "for d in tqdm(data):\n",
    "    for emr in d['emrs']:\n",
    "        checkout_id = d['zid']+ \"/\" +emr['filename']\n",
    "        try:\n",
    "            if not emr['texts']:\n",
    "                continue\n",
    "            \n",
    "            texts = [re.sub(r'第(\\s)*\\d+(\\s)*页','',t) for t in emr['texts']]\n",
    "            text = ' '.join([t.replace(os.path.commonprefix(texts),'') for t in texts]) if len(texts) > 1 else texts[0]\n",
    "            \n",
    "            if not is_checkout(text):\n",
    "                if (\"出院小结\" in checkout_id and \"小时入出院记录\" not in text):\n",
    "                    not_checkout += 1\n",
    "                    # print(checkout_id)\n",
    "                    # print(text)\n",
    "                continue\n",
    "            \n",
    "            tmp = []\n",
    "            for key in default_tmp:\n",
    "                for kv in variants(key):\n",
    "                    if kv in text:\n",
    "                        tmp.append((kv,key))\n",
    "                        break\n",
    "            \n",
    "            for order in tmp_orders:\n",
    "                new_tmp = [tmp[i] for i in order]\n",
    "                new_tmp_str = ''.join([t[0] for t in new_tmp])\n",
    "                if pylcs.lcs_sequence_length(new_tmp_str,text) == len(new_tmp_str):\n",
    "                    tmp = new_tmp\n",
    "                    break\n",
    "            \n",
    "            info_dict = dict(文件ID=checkout_id)\n",
    "            for (kv,k),(nkv,nk) in zip(tmp[:-1],tmp[1:]):\n",
    "                # print(f'kv:{kv} nkv:{nkv}')\n",
    "                match = re.search(f\"{kv}(.*?){nkv}\", text, re.DOTALL)\n",
    "                span = match.span()\n",
    "                # print(f'span:{span}')\n",
    "                text = text[span[0]:]\n",
    "                info_dict[k] = match.group(1).replace(\"：\",\"\").strip()\n",
    "            \n",
    "            advice = info_dict['出院后建议']\n",
    "            advice = re.sub(\"预约[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"下次来院时间[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"健康宣教[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"请输入出院后建议[\\s\\S]*\",\"\",advice).strip()\n",
    "            info_dict['出院后建议'] = advice\n",
    "            \n",
    "            related_checks = []\n",
    "            time_span = []\n",
    "            default_time_stamp = ['2023','12','31','23','59']\n",
    "            for string in [info_dict['入院时间'],info_dict['出院时间']]:\n",
    "                # print('string: ', string)\n",
    "                time_stamp = re.findall(\"[\\d]+\",string)\n",
    "                # print('time_stamp: ', time_stamp)\n",
    "                time_stamp += default_time_stamp[len(time_stamp):]\n",
    "                time_stamp = ' '.join(time_stamp)\n",
    "                time_span.append(datetime.strptime(time_stamp, \"%Y %m %d %H %M\"))\n",
    "            # print('time_span: ', time_span)\n",
    "            \n",
    "            for check in d['checks']:\n",
    "                check_time = datetime.strptime(check['info']['报告时间'], \"%Y/%m/%d %H:%M\")\n",
    "                if time_span[0] < check_time < time_span[1]:\n",
    "                    check_dict = {}\n",
    "                    check_dict['header'] = check['info']['标本名称']\n",
    "                    check_dict['values'] = {}\n",
    "                    for c in check['values']:\n",
    "                        if c.get(\"异常标识\"):\n",
    "                            check_dict['values'][c['检验项']] = '[正常]' if c[\"异常标识\"] == \"N\" else '[异常]'\n",
    "                        elif c.get(\"结果标识\"):\n",
    "                            check_dict['values'][c['药品名称']] = '[正常]' if c[\"结果标识\"] == \"Z\" else '[异常]'\n",
    "                        else:\n",
    "                            print(c)\n",
    "                    related_checks.append(check_dict)\n",
    "            info_dict['完整化验结果'] = related_checks\n",
    "            checkout_data.append(info_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR{e}\\n{checkout_id}\")\n",
    "            num_error += 1\n",
    "        break\n",
    "print(f\"num error:{num_error}\")\n",
    "print(f\"not_checkout:{not_checkout}\")\n",
    "print(f\"success num :{len(checkout_data)}\")\n",
    "json.dump(checkout_data, open(\"../my_datasets/ninth/checkout_data.json\",'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ninth_data = json.load(open(\"../my_datasets/ninth/ninth_data.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'检验项': 'PT（全自动）', '检验结果值': '10.8', '单位': '秒', '参考范围': '9.8—12.7', '异常标识': 'N', '结果': '阴  性', '细菌名称': '正常菌群（草绿色链球菌和奈瑟菌生长）。', '药品名称': '需氧瓶三日培养未生长', '结果标识': 'Z', '折点': '阴性', '菌落数': '<10000CCU/ml'}\n"
     ]
    }
   ],
   "source": [
    "check_keys = {}\n",
    "for d in ninth_data:\n",
    "    for c in d['checks']:\n",
    "        for t in c['values']:\n",
    "            for k in t.keys():\n",
    "                if k not in check_keys and t[k]:\n",
    "                    check_keys[k] = t[k]\n",
    "print(check_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string:  <start>asdasa<start>\n",
      "asda<end>\n",
      "string:  (3, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'床位医师袁海花主任 周二下午 周四上午 专家门诊，王炯轶 周一下午 周四下午\\n专病门诊。\\n请输入出院后建议\\n健康'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string = re.search(\"<start>(.*?)<end>\",\"123<start>asdasa<start>\\nasda<end>sd\\n\\nhaha<end>\",re.DOTALL)\n",
    "print('string: ', string.group())\n",
    "print('string: ', string.span())\n",
    "\n",
    "text = \"床位医师袁海花主任 周二下午 周四上午 专家门诊，王炯轶 周一下午 周四下午\\n专病门诊。\\n请输入出院后建议\\n健康宣教内容\\n 下次来院时间2023 年 10 月 18521322595\\n预约 \\n□是否预约\"\n",
    "re.sub('宣教[\\s\\S]*','',text,re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效出院小结数量：53764\n",
      "门诊 not in 出院：19670\n",
      "门诊 != 出院：34400\n",
      "出院建议平均长度：102.0327542593557\n",
      "count    53764.000000\n",
      "mean       102.032754\n",
      "std         84.681880\n",
      "min          0.000000\n",
      "25%         48.000000\n",
      "50%         81.000000\n",
      "75%        127.000000\n",
      "max       1253.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../my_datasets/ninth/checkout_data.json\")\n",
    "print(f\"有效出院小结数量：{df.shape[0]}\")\n",
    "print(f\"门诊 not in 出院：{df.apply(lambda r : r if r['门诊诊断'] and r['出院诊断'] and (r['门诊诊断'] not in r['出院诊断']) else None,axis=1).dropna().shape[0]}\")\n",
    "print(f\"门诊 != 出院：{df.apply(lambda r : r if r['门诊诊断'] and r['出院诊断'] and (r['门诊诊断'] != r['出院诊断']) else None,axis=1).dropna().shape[0]}\")\n",
    "advice_len = df.apply(lambda r : len(r['出院后建议']) ,axis=1)\n",
    "print(f\"出院建议平均长度：{advice_len.mean()}\")\n",
    "advice_len.hist(bins=advice_len.max(),log=True)\n",
    "print(advice_len.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仁济住院数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_name_list = [name for name in os.listdir('./') if name.endswith('.csv')]\n",
    "dfs = {file_name:pd.read_csv(file_name, low_memory=False) for file_name in file_name_list}\n",
    "for df in dfs.values():\n",
    "    df.rename(columns={'jzzsy': '住院号'}, inplace=True)\n",
    "print(\"文件名:行数\")\n",
    "[(i,len(dfs[i])) for i in dfs]\n",
    "print(\"文件名:住院号数量\")\n",
    "[(i,len(set(dfs[i]['住院号']))) for i in dfs]\n",
    "intersection = set(dfs['202303_medical_history_enter.csv']['住院号'])\n",
    "black_list = set([\n",
    "    '202303_medical_history_leave_24h.csv',\n",
    "    '202303_medical_history_op_first_disease.csv',\n",
    "    '202303_medical_history_operation.csv',\n",
    "    '202303_medical_history_routine.csv'\n",
    "])\n",
    "union = set()\n",
    "for i in dfs:\n",
    "    union = union.union(set(dfs[i]['住院号']))\n",
    "\n",
    "for i in set(file_name_list) - black_list:\n",
    "    new_set = set(dfs[i]['住院号'])\n",
    "    intersection = new_set & intersection\n",
    "print(f\"住院号交集数量:{len(intersection)}, 重合比例:{len(intersection)}/{len(union)} {len(intersection)/len(union)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "advice = pd.read_csv(\"data/202303出院/202303出院有放射报告-医嘱.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "check = pd.read_csv(\"data/202303出院/202303出院有放射报告-检验.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "test = pd.read_csv(\"data/202303出院/202303出院有放射报告-检查.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "xlsx = pd.ExcelFile('data/202303出院/202303出院有放射报告-病史相关.xlsx')\n",
    "history = {sheet_name : xlsx.parse(sheet_name).rename(columns={'jzzsy': '住院号'}) for sheet_name in xlsx.sheet_names}\n",
    "\n",
    "\n",
    "zids = set(history['病案首页']['住院号'])\n",
    "chuyuan = []\n",
    "for zid in tqdm(zids):\n",
    "    d = {}\n",
    "    d['住院号'] = zid\n",
    "    for sheet_name in history:\n",
    "        sheet = history[sheet_name]\n",
    "        if \"住院号\" in sheet.columns:\n",
    "            d[sheet_name] = sheet[sheet['住院号'] == zid].to_dict(orient='records')\n",
    "    d['医嘱'] = advice[advice['住院号'] == zid].to_dict(orient='records')\n",
    "    d['检验'] = check[check['住院号'] == zid].to_dict(orient='records')\n",
    "    d['检查'] = test[test['住院号'] == zid].to_dict(orient='records')\n",
    "    chuyuan.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "chuyuan_sample = chuyuan[:20]\n",
    "json.dump(chuyuan_sample, open(\"data/chuyuan_data_sample.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)\n",
    "# json.dump(chuyuan, open(\"data/chuyuan_data.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chuyuan = json.load(open(\"data/chuyuan_data.json\"))\n",
    "check_nums = [len(d['检验']) for d in chuyuan]\n",
    "check_normal = [len([c for c in d['检验'] if c['结果值异常标志'] == 'NO']) for d in chuyuan]\n",
    "check_abnormal = [(i-j) for i,j in zip(check_nums, check_normal)]\n",
    "print(f\"average_check_num: {(sum(check_nums)/len(check_nums))}\\n\" \\\n",
    "    f\"average normal: {sum(check_normal)/len(check_normal)}\\n\" \\\n",
    "    f\"average abnormal: {sum(check_abnormal)/len(check_abnormal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Stdout2File\n",
    "from datetime import datetime\n",
    "\n",
    "int2date = lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M%S\").strftime(\"%Y/%m/%d %H:%M:%S\").replace(r\"2023/3\",r\"2023/03\")\n",
    "date2int = lambda x: int(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\").strftime(\"%Y%m%d%H%M%S\"))\n",
    "chuyuan_sample = json.load(open(\"data/chuyuan_data_sample.json\"))\n",
    "\n",
    "def time_stamps(d):\n",
    "    time_stamps = []\n",
    "    time_stamps.append((int2date(str(d['出院记录'][0]['入院日期时间'])),  f\"入院时间 {d['病案首页'][0]['入院诊断名称']}\"))\n",
    "    time_stamps.append((int2date(str(d['出院记录'][0]['出院日期时间'])[:14]),  '出院时间'))\n",
    "    if d.get(\"手术记录\"):\n",
    "        for i,op in enumerate(d['手术记录']):\n",
    "            time_stamps.append((int2date(str(op['手术开始日期时间'])),  f\"手术{i}:{op['手术及操作编码对应名称']} 开始时间\"))\n",
    "            if op.get('手术结束日期时间').strip():\n",
    "                time_stamps.append((int2date(str(op['手术结束日期时间'][:14])),  f\"手术{i}:{op['手术及操作编码对应名称']} 结束时间\"))\n",
    "    if d.get(\"日常病程记录\"):\n",
    "        for i,rec in enumerate(d['日常病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"日常病程{i} 记录时间:\"))\n",
    "    if d.get('首次病程记录'):\n",
    "        for i, rec in enumerate(d['首次病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"首次病程记录{i} 记录时间:\"))\n",
    "    if d.get('术后首次病程记录'):\n",
    "        for i, rec in enumerate(d['术后首次病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"术后首次病程记录{i} 记录时间:\"))\n",
    "    if d.get('医嘱'):\n",
    "        for i, rec in enumerate(d['医嘱']):\n",
    "            time_stamps.append((rec['开始时间'].replace(r\"2023/3\",r\"2023/03\"),  f\"医嘱{i}:{rec['医嘱名称']} 开始时间:\"))\n",
    "            if rec.get('结束时间'):\n",
    "                time_stamps.append((rec['结束时间'].replace(r\"2023/3\",r\"2023/03\"),  f\"医嘱{i}:{rec['医嘱名称']} 结束时间:\"))\n",
    "    if d.get('检验'):\n",
    "        for i, rec in enumerate(d['检验']):\n",
    "            time_stamps.append((rec['报告日期'].replace(r\"2023/3\",r\"2023/03\"),  f\"检验{i}:{rec['报告名称']} {rec['检验项目']} 报告日期:\"))\n",
    "    if d.get('检查'):\n",
    "        for i, rec in enumerate(d['检查']):\n",
    "            time_stamps.append((rec['报告日期'].replace(r\"2023/3\",r\"2023/03\"), f\"检查{i}:{rec['报告名称']} 报告日期:\"))\n",
    "    sorted_time_stamps = sorted(time_stamps, key=lambda x: date2int(x[0]))\n",
    "    return sorted_time_stamps\n",
    "\n",
    "with Stdout2File(\"data/chuyuan_time_stamps.txt\"):\n",
    "    for d in chuyuan_sample:\n",
    "        for s in time_stamps(d):\n",
    "            print(s)\n",
    "        print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "date2int = lambda x: int(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\").strftime(\"%Y%m%d%H%M%S\"))\n",
    "data = json.load(open(\"data/chuyuan/chuyuan_data.json\"))\n",
    "\n",
    "def advice_type(advice):\n",
    "    if advice.get(\"规格\") and advice['单次用量']!=1.0:\n",
    "        return \"药物\"\n",
    "    # if \"会诊\" in advice['医嘱名称']:\n",
    "    #     return \"会诊\"\n",
    "    keywords = [\"平扫\",\"脑电\",\"心电\",\"CT\",\"检查\",\"分析\",\"试验\",\"检测\",\"MRI\",\"静脉血\",\"超声\"]\n",
    "    for k in keywords:\n",
    "        if k in advice['医嘱名称']:\n",
    "            return \"检查\"\n",
    "    return False\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for d in tqdm(data):\n",
    "    if (not d.get(\"入院记录\") and not d.get(\"24小时出入院记录\")) or (not d.get(\"医嘱\") or not d.get(\"检验\")):\n",
    "        continue\n",
    "    text_dict =dict(\n",
    "        病人年龄=d['检验'][0]['年龄'],\n",
    "        病人性别=d['病案首页'][0]['性别'],\n",
    "        入院诊断名称=d['病案首页'][0]['入院诊断名称'],\n",
    "        主诉=d['入院记录'][0]['主诉'] if d.get('入院记录') else d['24小时出入院记录'][0]['主诉'],\n",
    "    )\n",
    "    \n",
    "    checks = []\n",
    "    for c in d['检验']:\n",
    "        try:\n",
    "            c['报告日期'] = date2int(c['报告日期'])\n",
    "            checks.append(c)\n",
    "        except:\n",
    "            continue\n",
    "    checks = sorted(checks, key=lambda x: x['报告日期'])\n",
    "    operations = sorted(d['手术记录'], key=lambda x: x['手术开始日期时间'])\n",
    "    # tests = sorted(d['检查'], key=lambda x: x['报告日期'])\n",
    "    \n",
    "    advices = []\n",
    "    for a in d['医嘱']:\n",
    "        try:\n",
    "            a['开始时间'] = date2int(a['开始时间'])\n",
    "            advices.append(a)\n",
    "        except:\n",
    "            continue\n",
    "    advices = sorted(advices, key=lambda x: x['开始时间'])\n",
    "    \n",
    "    # 如果有手术记录，过滤掉在第一次手术之后的所有信息\n",
    "    if operations:\n",
    "        first_op_time = operations[0]['手术开始日期时间']\n",
    "        checks = [c for c in checks if c['报告日期'] < first_op_time]\n",
    "        advices = [a for a in advices if a['开始时间'] < first_op_time]\n",
    "    \n",
    "    if not checks or not advices:\n",
    "        continue\n",
    "    \n",
    "    # 过滤掉在第一次检验报告出来之前的医嘱\n",
    "    advices = [a for a in advices if a['开始时间'] > checks[0]['报告日期']]\n",
    "    \n",
    "    # 过滤掉在最后一次检验报告出来之前的医嘱\n",
    "    # advices = [a for a in advices if a['开始时间'] > checks[-1]['报告日期']]\n",
    "    \n",
    "    # 只保留药物，会诊，检查三种医嘱\n",
    "    advices = [a for a in advices if advice_type(a)]\n",
    "    \n",
    "    # 对医嘱按时间聚类\n",
    "    new_advices = [dict(开始时间=a['开始时间'], 医嘱名称=a['医嘱名称'], 医嘱类型=advice_type(a)) for a in advices]\n",
    "    check_times = sorted(list(set([c['报告日期'] for c in checks])))\n",
    "    advice_groups = [[a for a in new_advices if check_times[i] < a['开始时间'] < check_times[i+1]] for i in range(len(check_times)-1)]\n",
    "    advice_groups = [a for a in advice_groups if a]\n",
    "\n",
    "    # 生成医嘱训练数据\n",
    "    text = \",\".join([f\"{k}: {v}\" for (k,v) in text_dict.items()])\n",
    "    for advice_group in advice_groups:\n",
    "        check_list = [dict(检验项目=c['检验项目'], 检验结果=c['检验结果'], 正常值范围=c['正常值范围'], 结果值异常标志=c['结果值异常标志'] if c['结果值异常标志']!=\"NO\" else \"正常\") for c in checks if c['报告日期'] < advice_group[0]['开始时间']]\n",
    "        advice_text = \",\".join([f\"{a['医嘱类型']}:{a['医嘱名称']}\" for a in advice_group])\n",
    "        train_data.append(dict(text=text,data=check_list,output=advice_text))\n",
    "\n",
    "json.dump(train_data, open(\"data/chuyuan/chuyuan_train.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_data[:50], open(\"data/chuyuan/chuyuan_train_sample.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HONESTY_EXTRACTION_TEMPLATE_EXP = \"\"\"USER:Pretend you are an honest person making statements about the world.\\nAssistant:\"\"\"\n",
    "HONESTY_EXTRACTION_TEMPLATE_REF = \"\"\"USER:Pretend you are a dishonest person making statements about the world.\\nAssistant:\"\"\"\n",
    "\n",
    "knows = {topic:{True:[],False:[]}for topic in topic_true_examples}\n",
    "\n",
    "for o in outputs:\n",
    "    ks = o['query_output'].split(\"<sep>\")\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"<sep\"))\n",
    "    ks = new_ks\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"\\n\"))\n",
    "    ks = new_ks\n",
    "    ks = [re.sub(\"[0-9]+. \",\"\", k).strip() for k in ks]\n",
    "    ks = [k for k in ks if k]\n",
    "    for k in ks:\n",
    "        if k not in knows[o['topic']][o['label']]:\n",
    "            knows[o['topic']][o['label']].append(k)\n",
    "    \n",
    "dst = []\n",
    "for topic in knows:\n",
    "    for i in range(min(len(knows[topic][True]),len(knows[topic][False]))):\n",
    "        true_input = HONESTY_EXTRACTION_TEMPLATE_EXP\n",
    "        false_input = HONESTY_EXTRACTION_TEMPLATE_REF\n",
    "        true_output = knows[topic][True][i]\n",
    "        false_output = knows[topic][False][i]\n",
    "        dst.append([dict(input=true_input,output=true_output,topic=topic,label=True),dict(input=false_input,output=false_output,topic=topic,label=False)])\n",
    "print('True-False Dst Size: ', len(dst))\n",
    "json.dump(knows, open(\"data/knows.json\", \"w\"), indent=4)\n",
    "json.dump(dst, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from model import INTERNLM_TEMPLATE\n",
    "\n",
    "\n",
    "usmle_test = list(jsonlines.open(\"data/usmle/questions/US/test.jsonl\"))\n",
    "usmle_test_for_infer = []\n",
    "\n",
    "for d in usmle_test:\n",
    "    options_strs = [f\"{op}: {d['options'][op]}\" for op in d['options']]\n",
    "    input = f\"Question: {d['question']} Options: {'; '.join(options_strs)}. Output: The correct answer is option\"\n",
    "    input = INTERNLM_TEMPLATE.format(input)\n",
    "    d['input'] = input\n",
    "    d['labels'] = ['A', 'B', 'C', 'D', 'E']\n",
    "    usmle_test_for_infer.append(d)\n",
    "\n",
    "json.dump(usmle_test_for_infer, open(\"data/mgpu_infer/usmle_test_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from model import *\n",
    "\n",
    "\n",
    "tqa = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/TruthfulQA-main/data/mc_task.json\"))\n",
    "print(tqa[0])\n",
    "tqa_dst = [dict(input=INTERNLM_TEMPLATE.format(d['question']),labels=list(d['mc1_targets'].keys()),gt=list(d['mc1_targets'].values()).index(1)) for d in tqa]\n",
    "print(tqa_dst[0])\n",
    "json.dump(tqa_dst, open(\"data/mgpu_infer/truthfulqa_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "medqa_test = json.load(open(\"data/usmle/questions/US/test.json\"))\n",
    "print('medqa_test: ', medqa_test[0])\n",
    "medqa_test_infer = []\n",
    "for d in medqa_test:\n",
    "    options = d['options']\n",
    "    option_text = \";\".join([f\"{k}: {options[k]}\" for k in options])\n",
    "    # input_text = INTERNLM_TEMPLATE.format(\"Question:\"+d['question']+\"; Options:\"+option_text) + \"The correct answer is option\"\n",
    "    input_text = INTERNLM_TEMPLATE.format(\"Options:\"+option_text) + \"The correct answer is option\"\n",
    "    medqa_test_infer.append(dict(input=input_text, labels=list(options.keys()), gt=d['answer']))\n",
    "print('medqa_test_infer: ', medqa_test_infer[0])\n",
    "json.dump(medqa_test_infer, open(\"data/mgpu_infer/medqa_test_noquestion_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.llm_utils import get_free_gpus\n",
    "import subprocess\n",
    "\n",
    "free_gpus = get_free_gpus()\n",
    "command = f'CUDA_VISIBLE_DEVICES={\",\".join([str(i) for i in free_gpus])} \\\n",
    "            torchrun --nproc_per_node {len(free_gpus)} --master_port=12570 /home/cs/yangyuchen/guoyiqiu/gpt_re/mgpu_infer.py \\\n",
    "            --func infer \\\n",
    "            --model_path /home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms \\\n",
    "            --dst_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer/medqa_test_noquestion_inference.json \\\n",
    "            --save_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_noquestion_inference.json \\\n",
    "            --mnt 8 '\n",
    "\n",
    "process = subprocess.Popen(command, shell=True)\n",
    "print(f\"shell process id: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "internlm_chat_7b_usmle_test_generate = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_generate.json'))\n",
    "internlm_chat_7b_usmle_test_inference = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_inference.json'))\n",
    "\n",
    "inference_acc = 0\n",
    "generate_acc = 0\n",
    "i_g = 0\n",
    "for di,dg in zip(internlm_chat_7b_usmle_test_inference, internlm_chat_7b_usmle_test_generate):\n",
    "    pred_i = ['A','B','C','D','E'][np.argmin(di['label_loss'])]\n",
    "    pred_g = dg['output'].replace(\"<eoa>\",\"\").replace(\"</s>\",\"\").strip()[0]\n",
    "    gt = di['answer_idx']\n",
    "    inference_acc += (pred_i == gt)\n",
    "    generate_acc += (pred_g == gt)\n",
    "    i_g += (pred_i == pred_g)\n",
    "    \n",
    "print(f\"inference_acc: {inference_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"generate_acc: {generate_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"i_g: {i_g/len(internlm_chat_7b_usmle_test_inference)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "medqa_test_withquestion_result = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_inference.json\"))\n",
    "acc_w = [['A','B','C','D','E'][np.argmin(d['label_loss'])]==d['gt'] for d in medqa_test_withquestion_result]\n",
    "print(f\"acc_w: {sum(acc_w)/len(acc_w)}\")\n",
    "medqa_test_noquestion_result = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_noquestion_inference.json\"))\n",
    "acc_wo = [['A','B','C','D','E'][np.argmin(d['label_loss'])]==d['gt'] for d in medqa_test_noquestion_result]\n",
    "print(f\"acc_wo: {sum(acc_wo)/len(acc_wo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_llm_gyq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
