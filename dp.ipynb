{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Clash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if not result.stdout:\n",
    "    subprocess.Popen(\"~/tools/clash/clash\", shell=True)\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(f\"Clash is running, pid: {result.stdout}\")\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget www.google.com\n",
    "!rm -f index.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!killall clash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_dir:  /mnt/petrelfs/guoyiqiu/coding/my_models/internlm-7b\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "model_name = \"internlm/internlm-7b\"\n",
    "# model_name = \"baichuan-inc/Baichuan2-7B-Base\"\n",
    "local_dir = os.path.join(os.environ['my_models_dir'], model_name.split(\"/\")[-1])\n",
    "print('local_dir: ', local_dir)\n",
    "\n",
    "# from huggingface_hub import snapshot_download as huggingface_snapshot_download\n",
    "# huggingface_snapshot_download(model_name, cache_dir=local_dir, local_dir=local_dir, local_dir_use_symlinks=False, ignore_patterns=[\"*.h5\",\"*safetensors\",\"*msgpack\"], etag_timeout=60, force_download=True, resume_download=False)\n",
    "\n",
    "# from modelscope import snapshot_download as modelscope_snapshot_download; \n",
    "# modelscope_snapshot_download('Shanghai_AI_Laboratory/internlm-7b', revision='v1.0.2',)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True, use_fast=True)\n",
    "tok.batch_decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "lst = arr.tolist()\n",
    "\n",
    "print(lst)  # [1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from model import multithread_query_chatgpt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = multithread_query_chatgpt([dict(query_input=\"hello\")])\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_query_template = \"Generate 10 statements about the topic {topic}. The statements should be true and brief and contain factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "false_query_template = \"Generate 10 false statements about the topic {topic}. The statements should be incorrect and brief and contain wrong factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sep>.\"\n",
    "\n",
    "topic_true_examples = {\n",
    "    \"Cities\": \"Oranjestad is a city in Aruba.\" , \n",
    "    \"Inventions\": \"Grace Hopper invented the COBOL programming language.\" , \n",
    "    \"Chemical Elements\": \"Boron is used in the production of glass and ceramics.\" , \n",
    "    \"Animals\": \"The llama has a diet of herbivore.\" , \n",
    "    \"Companies\": \"Meta Platforms has headquarters in United States.\" , \n",
    "    \"Scientific Facts\": \"The Earth’s tides are primarily caused by the gravitational pull of the moon.\",\n",
    "    \"Medical\": \"Benign tumors typically grow slowly and do not invade surrounding tissues or spread to other areas.\"\n",
    "}\n",
    "topic_false_examples = {\n",
    "    \"Cities\": \"Wellington is a name of a country.\" ,\n",
    "    \"Inventions\": \"David Schwarz lived in France.\" ,\n",
    "    \"Chemical Elements\": \"Indium is in the Lanthanide group.\" ,\n",
    "    \"Animals\": \"The whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\" ,\n",
    "    \"Companies\": \"KDDI operates in the industry of Materials.\" , \n",
    "    \"Scientific Facts\": \"Ice sinks in water due to its higher density.\",\n",
    "    \"Medical\": \"The normal range for human body temperature is 50-55 degrees Celsius.\"\n",
    "}\n",
    "true_queries = [dict(query_input=true_query_template.format(topic=k, examples=v),topic=k,label=True) for (k,v) in topic_true_examples.items()]\n",
    "false_queries = [dict(query_input=false_query_template.format(topic=k, examples=v),topic=k,label=False) for (k,v) in topic_false_examples.items()]\n",
    "inputs = true_queries + false_queries\n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    outputs.extend(multithread_query_chatgpt(inputs, thread_num=8, model_name='gpt-4-1106-preview'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "九院数预处理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import fitz\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import re\n",
    "\n",
    "def get_data(i):\n",
    "    broken_files = []\n",
    "    advice_path = os.path.join(base_dir,\"多科室医嘱\",i+\".json\")\n",
    "    with open(advice_path, \"r\", encoding=\"gbk\") as f:\n",
    "        try:\n",
    "            advice = json.load(f)['Data']\n",
    "        except Exception as e:\n",
    "            broken_files.append(advice_path)\n",
    "            # print(f\"{i}: broken advice {advice_path}.\")\n",
    "            advice=None\n",
    "    \n",
    "    all_checks = []\n",
    "    check_path = os.path.join(base_dir,\"多科室检验报告\",i)\n",
    "    for fname in os.listdir(check_path):\n",
    "        if not fname.endswith(\".html\"):\n",
    "            continue\n",
    "        try:\n",
    "            with open(os.path.join(check_path, fname), \"r\") as f:\n",
    "                soup = BeautifulSoup(f, features=\"lxml\")\n",
    "        except Exception as e:\n",
    "            broken_files.append(os.path.join(check_path, fname))\n",
    "            # print(f\"{i}: broken check: {fname}.\")\n",
    "            continue\n",
    "            \n",
    "        if not soup.table:\n",
    "            continue\n",
    "        check_title = soup.h4.get_text(strip=True).replace(\"共 1 项\",\"\").strip()\n",
    "        # print('check_title: ', check_title)\n",
    "        info = dict(标题=check_title)\n",
    "        info_text = soup.div.get_text(strip=True).replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "        # print('info_text: ', info_text)\n",
    "        info_keys = [\"工作组:\",\"送检医生:\",\"报告时间:\",\"检验医生:\",\"送检时间:\",\"标本名称:\",\"性别:\",\"年龄:\",\"诊断:\",\"生理周期:\",\"申请号:\"]\n",
    "        for k1,k2 in zip(info_keys[:-1],info_keys[1:]):\n",
    "            value = re.search(f\"{k1}(.*?){k2}\", info_text)\n",
    "            if value:\n",
    "                info[k1[:-1]] = value.group(1).strip()\n",
    "        # print('info: ', info)\n",
    "        \n",
    "        table_head = []\n",
    "        for row in soup.table.find_all('th'):\n",
    "            table_head.append(row.get_text(strip=True))\n",
    "        \n",
    "        table_data = []\n",
    "        for row in soup.table.find_all('tr'):\n",
    "            row_data = [i.get_text(strip=True) for i in row.find_all('td')]\n",
    "            if row_data:\n",
    "                table_data.append(row_data)\n",
    "        check_df = pd.DataFrame(table_data, columns=table_head)\n",
    "        check_dict = check_df[[c for c in check_df.columns if c]].to_dict(orient=\"records\")\n",
    "        all_checks.append(dict(filename=fname, info=info, values=check_dict))\n",
    "    \n",
    "    all_emrs = []\n",
    "    emr_path = os.path.join(base_dir,\"多科室EMR\",i)\n",
    "    for fname in os.listdir(emr_path):\n",
    "        if not fname.endswith(\".pdf\"):\n",
    "            continue\n",
    "        try:\n",
    "            texts = []\n",
    "            doc = fitz.open(os.path.join(emr_path,fname))\n",
    "            # print(os.path.join(emr_path,fname))\n",
    "            for page in doc:\n",
    "                texts.append(page.get_text())\n",
    "        except Exception as e:\n",
    "            broken_files.append(os.path.join(emr_path,fname))\n",
    "            # print(f\"{i}: broken emr: {fname}.\")\n",
    "            continue\n",
    "        finally:\n",
    "            all_emrs.append(dict(filename=fname, texts=texts))\n",
    "    \n",
    "    return dict(zid=i,advice=advice, checks=all_checks, emrs=all_emrs), broken_files\n",
    "\n",
    "base_dir = os.path.join(os.environ['my_datasets_dir'], \"ninth\")\n",
    "check_ids = os.listdir(os.path.join(base_dir,\"多科室检验报告\"))\n",
    "advice_ids = [i.replace(\".json\",\"\") for i in os.listdir(os.path.join(base_dir,\"多科室医嘱\"))]\n",
    "emr_ids = os.listdir(os.path.join(base_dir,\"多科室EMR\"))\n",
    "overlap_ids = sorted(list(set(check_ids) & set(advice_ids) & set(emr_ids)))\n",
    "\n",
    "all_data = []\n",
    "all_broken_files = []\n",
    "process_num = cpu_count()\n",
    "print(f\"Total {len(overlap_ids)} files. process_num={process_num}\")\n",
    "\n",
    "with Pool(process_num) as p:\n",
    "    for data, broken_files in tqdm(p.imap(get_data, overlap_ids), total=len(overlap_ids)):\n",
    "        all_data.append(data)\n",
    "        all_broken_files.extend(broken_files)\n",
    "\n",
    "all_data = sorted(all_data, key=lambda x: x[\"zid\"])\n",
    "all_data = [d for d in all_data if d[\"advice\"] and d[\"checks\"] and d[\"emrs\"]]\n",
    "json.dump(all_broken_files, open(os.path.join(base_dir, \"ninth_broken_files.json\"), \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)\n",
    "json.dump(all_data[-10:], open(os.path.join(base_dir, \"ninth_data_sample.json\"), \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)\n",
    "json.dump(all_data, open(os.path.join(base_dir, \"ninth_data.json\"), \"w\", encoding=\"utf8\"), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pylcs\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "\n",
    "default_tmp = ('性别', '年龄', '入院时间', '出院时间', '门诊诊断', '入院诊断', '出院诊断', '入院时主要症状及体征', '主要化验结果', '特殊检查及重要会诊', '病程与治疗结果', '合并症', '出院时情况', '出院后建议', '治疗结果', \"主治医师\")\n",
    "tmp_orders = [(0,       1,        2,          3,        4,          5,          6,          7,                  8,              9,              10,             11,         12,          13,         14,         15),\n",
    "              (0,       1,        2,          3,        4,          5,          6,          7,                  10,              8,              9,             11,         12,          13,         14,         15)]\n",
    "    \n",
    "specaial_variants = dict(\n",
    "    入院时间=['入院日期',],\n",
    "    出院时间=['出院日期','死亡时间'],\n",
    "    门诊诊断=['门诊诊',],\n",
    "    入院诊断=['入院',],\n",
    "    出院诊断=['死亡诊断'],\n",
    "    入院时主要症状及体征=['入院情况'],\n",
    "    主要化验结果=['主要结果','检验结果'],\n",
    "    特殊检查及重要会诊=['特殊检查结果','特殊检验及重要会诊','动态心电图结果','术中冰冻结果','术中冰冻','病理结果'],\n",
    "    病程与治疗结果=['诊疗经过','经过','病程及治疗结果'],\n",
    "    出院时情况=['院时情况',\"死亡时情况\"],\n",
    "    出院后建议=['出院后用药及建议','出院后用药建议','其他'],\n",
    "    治疗结果=['治果'],\n",
    ")\n",
    "\n",
    "variants = lambda key :[key] + specaial_variants.get(key, []) + [(k[:i] + \"\\n\" + k[i:]).strip() for k in ([key] + specaial_variants.get(key, [])) for i in range(1,len(k))]\n",
    "is_checkout = lambda text: all(any(kv in text for kv in variants(key)) for key in default_tmp)\n",
    "\n",
    "\n",
    "\n",
    "my_datasets_dir = os.environ['my_datasets_dir']\n",
    "input_path = os.path.join(my_datasets_dir,\"ninth/ninth_data.json\")\n",
    "output_path = os.path.join(my_datasets_dir, \"ninth/checkout_data_with_checks.json\")\n",
    "\n",
    "data = json.load(open(input_path))\n",
    "checkout_data = []\n",
    "num_error = 0\n",
    "not_checkout = 0\n",
    "for d in tqdm(data):\n",
    "    for emr in d['emrs']:\n",
    "        checkout_id = d['zid']+ \"/\" +emr['filename']\n",
    "        try:\n",
    "            if not emr['texts']:\n",
    "                continue\n",
    "            \n",
    "            texts = [re.sub(r'第(\\s)*\\d+(\\s)*页','',t) for t in emr['texts']]\n",
    "            text = ' '.join([t.replace(os.path.commonprefix(texts),'') for t in texts]) if len(texts) > 1 else texts[0]\n",
    "            \n",
    "            if not is_checkout(text):\n",
    "                if (\"出院小结\" in checkout_id and \"小时入出院记录\" not in text):\n",
    "                    not_checkout += 1\n",
    "                    # print(checkout_id)\n",
    "                    # print(text)\n",
    "                continue\n",
    "            \n",
    "            tmp = []\n",
    "            for key in default_tmp:\n",
    "                for kv in variants(key):\n",
    "                    if kv in text:\n",
    "                        tmp.append((kv,key))\n",
    "                        break\n",
    "            \n",
    "            for order in tmp_orders:\n",
    "                new_tmp = [tmp[i] for i in order]\n",
    "                new_tmp_str = ''.join([t[0] for t in new_tmp])\n",
    "                if pylcs.lcs_sequence_length(new_tmp_str,text) == len(new_tmp_str):\n",
    "                    tmp = new_tmp\n",
    "                    break\n",
    "            \n",
    "            info_dict = dict(文件ID=checkout_id)\n",
    "            for (kv,k),(nkv,nk) in zip(tmp[:-1],tmp[1:]):\n",
    "                # print(f'kv:{kv} nkv:{nkv}')\n",
    "                match = re.search(f\"{kv}(.*?){nkv}\", text, re.DOTALL)\n",
    "                span = match.span()\n",
    "                # print(f'span:{span}')\n",
    "                text = text[span[0]:]\n",
    "                info_dict[k] = match.group(1).replace(\"：\",\"\").strip()\n",
    "            \n",
    "            advice = info_dict['出院后建议']\n",
    "            advice = re.sub(\"预约[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"下次来院时间[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"健康宣教[\\s\\S]*\",\"\",advice)\n",
    "            advice = re.sub(\"请输入出院后建议[\\s\\S]*\",\"\",advice).strip()\n",
    "            info_dict['出院后建议'] = advice\n",
    "            \n",
    "            related_checks = []\n",
    "            time_span = []\n",
    "            default_time_stamp = ['2023','12','31','23','59']\n",
    "            for string in [info_dict['入院时间'],info_dict['出院时间']]:\n",
    "                # print('string: ', string)\n",
    "                time_stamp = re.findall(\"[\\d]+\",string)\n",
    "                # print('time_stamp: ', time_stamp)\n",
    "                time_stamp += default_time_stamp[len(time_stamp):]\n",
    "                time_stamp = ' '.join(time_stamp)\n",
    "                time_span.append(datetime.strptime(time_stamp, \"%Y %m %d %H %M\"))\n",
    "            # print('time_span: ', time_span)\n",
    "            \n",
    "            key_set = set(['检验项', '药品名称', '细菌名称'])\n",
    "            value_set = set(['异常标识', '结果标识', '结果'])\n",
    "            \n",
    "            value_map = [('^N$','[正常]'),(\"^敏感[\\s\\S]*\",\"[敏感]\"),(\"^耐药[\\s\\S]*\",\"[耐药]\"),(\"^中介[\\s\\S]*\",\"[中介]\"),(\"^$\",'[异常]'),(\"阴.*性.*\",\"[阴性]\"),(\"阳.*性.*\",\"[阳性]\"),(\"S\",\"[阴性]\"),(\"R\",\"[阳性]\"),(\"T\",\"[中介]\"),(\"弱阳性\",\"[弱阳性]\"),(\"极弱阳性\",\"[极弱阳性]\")]\n",
    "            for check in d['checks']:\n",
    "                check_time = datetime.strptime(check['info']['报告时间'], \"%Y/%m/%d %H:%M\")\n",
    "                if time_span[0] < check_time < time_span[1]:\n",
    "                    check_dict = {}\n",
    "                    check_dict['header'] = check['info']['标本名称']\n",
    "                    check_dict['values'] = {}\n",
    "                    key_name = '检验项'\n",
    "                    value_name = '异常标识'\n",
    "                    if not check['values']:\n",
    "                        continue\n",
    "                    for key in key_set:\n",
    "                        if key in check['values'][0]:\n",
    "                            key_name = key\n",
    "                            break\n",
    "                    for value in value_set:\n",
    "                        if value in check['values'][0]:\n",
    "                            value_name = value\n",
    "                            break\n",
    "                    for c in check['values']:\n",
    "                        key_item = c[key_name]\n",
    "                        value_item = ''\n",
    "                        if c[value_name] is None:\n",
    "                            continue\n",
    "                        \n",
    "                        for pattern,flag in value_map:\n",
    "                            if re.match(pattern, c[value_name]):\n",
    "                                value_item = flag\n",
    "                                break\n",
    "                        if not key_item or not value_item:\n",
    "                            print(c)\n",
    "                            continue\n",
    "                        check_dict['values'][key_item] = value_item\n",
    "                    related_checks.append(check_dict)\n",
    "            if not related_checks:\n",
    "                continue\n",
    "            info_dict['完整化验结果'] = related_checks\n",
    "            checkout_data.append(info_dict)\n",
    "        except Exception as e:\n",
    "            tb_str = traceback.format_tb(e.__traceback__)\n",
    "            print(f\"ERROR: {checkout_id} LINE:{tb_str} {e}\")\n",
    "            num_error += 1\n",
    "        break\n",
    "print(f\"num error:{num_error}\")\n",
    "print(f\"not_checkout:{not_checkout}\")\n",
    "print(f\"success num :{len(checkout_data)}\")\n",
    "json.dump(checkout_data, open(output_path,'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full df size:  43687\n",
      "filterd by 门诊and出院: 15489 left\n",
      "filterd by length: 15167 left\n",
      "filterd by length 2: 10914 left\n",
      "count    10914.000000\n",
      "mean       209.888767\n",
      "std         99.048961\n",
      "min         38.000000\n",
      "25%        142.000000\n",
      "50%        188.000000\n",
      "75%        252.000000\n",
      "max        765.000000\n",
      "Name: num_tokens, dtype: float64\n",
      "count    10914.000000\n",
      "mean       963.299432\n",
      "std        307.687371\n",
      "min         73.000000\n",
      "25%        757.000000\n",
      "50%        926.000000\n",
      "75%       1184.750000\n",
      "max       1599.000000\n",
      "Name: num_tokens, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm_tok = AutoTokenizer.from_pretrained(os.path.join(os.environ['my_models_dir'],'internlm-7b'), trust_remote_code=True)\n",
    "data_path = os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_with_checks.json\")\n",
    "eval_ratio = 0.01\n",
    "max_length = 768\n",
    "max_length2 = 1600\n",
    "\n",
    "df = pd.read_json(data_path)\n",
    "print('full df size: ', df.shape[0])\n",
    "\n",
    "data_df = df.apply(lambda r: r if (r['门诊诊断'] and r['出院诊断'] and r['门诊诊断'] not in r['出院诊断']) else None,axis=1).dropna()\n",
    "data_df = data_df.apply(lambda r: pd.Series(dict(\n",
    "    input=\"性别:\"+r['性别']+\"\\n年龄:\"+r['年龄']+\"\\n入院时主要症状及体征:\"+r['入院时主要症状及体征'].replace(\"\\n\",\"\")+\"\\n特殊检查及重要会诊:\"+r['特殊检查及重要会诊'].replace(\"\\n\",\"\"),\n",
    "    data=r['完整化验结果'],\n",
    "    output=\"出院诊断:\"+r['出院诊断'])),axis=1)\n",
    "# data_df['input'] = data_df.apply(lambda r: r['input'].replace(r['output','未知']), axis=1)\n",
    "print(f\"filterd by 门诊and出院: {data_df.shape[0]} left\")\n",
    "\n",
    "data_df['num_tokens'] = data_df.apply(lambda r : len(llm_tok(r['input']+r['output'])['input_ids']),axis=1)\n",
    "data_df = data_df[data_df['num_tokens'] < max_length]\n",
    "print(f\"filterd by length: {data_df.shape[0]} left\")\n",
    "\n",
    "data_df = data_df.sample(frac=1)\n",
    "data_df_text_dicts = data_df.apply(lambda r: pd.Series(dict(input=r['input']+'\\n完整化验结果: '+', '.join([f\"{k}:{v.replace('[','').replace(']','')}\" for dict in r['data'] for (k,v) in dict['values'].items()]),output=r['output'])), axis=1)\n",
    "data_df_text_dicts['num_tokens'] = data_df_text_dicts.apply(lambda r : len(llm_tok(r['input']+r['output'])['input_ids']),axis=1)\n",
    "\n",
    "data_df = data_df[data_df_text_dicts['num_tokens'] < max_length2]\n",
    "data_df_text_dicts = data_df_text_dicts[data_df_text_dicts['num_tokens'] < max_length2]\n",
    "data_df_no_dicts = data_df.drop(columns=['data'])\n",
    "print(f'filterd by length 2: {data_df.shape[0]} left')\n",
    "\n",
    "train_df = data_df.iloc[:int(len(data_df)*(1-eval_ratio))]\n",
    "eval_df = data_df.iloc[int(len(data_df)*(1-eval_ratio)):]\n",
    "train_df_text_dicts = data_df_text_dicts.iloc[:int(len(data_df_text_dicts)*(1-eval_ratio))]\n",
    "eval_df_text_dicts = data_df_text_dicts.iloc[int(len(data_df_text_dicts)*(1-eval_ratio)):]\n",
    "train_df_no_dicts = data_df_no_dicts.iloc[:int(len(data_df_no_dicts)*(1-eval_ratio))]\n",
    "eval_df_no_dicts = data_df_no_dicts.iloc[int(len(data_df_no_dicts)*(1-eval_ratio)):]\n",
    "\n",
    "json.dump(train_df.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_train.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(eval_df.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_eval.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(train_df_no_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_train_no_dicts.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(eval_df_no_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_eval_no_dicts.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(train_df_text_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_train_text_dicts.json\"),'w'), ensure_ascii=False)\n",
    "json.dump(eval_df_text_dicts.to_dict(orient=\"records\"), open(os.path.join(os.environ['my_datasets_dir'], \"ninth/checkout_data_eval_text_dicts.json\"),'w'), ensure_ascii=False)\n",
    "\n",
    "print(data_df['num_tokens'].describe())\n",
    "print(data_df_text_dicts['num_tokens'].describe())\n",
    "data_df_text_dicts['num_tokens'].hist(bins=1000,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../my_datasets/ninth/checkout_data.json\")\n",
    "print(f\"有效出院小结数量：{df.shape[0]}\")\n",
    "print(f\"门诊 not in 出院：{df.apply(lambda r : r if r['门诊诊断'] and r['出院诊断'] and (r['门诊诊断'] not in r['出院诊断']) else None,axis=1).dropna().shape[0]}\")\n",
    "print(f\"门诊 != 出院：{df.apply(lambda r : r if r['门诊诊断'] and r['出院诊断'] and (r['门诊诊断'] != r['出院诊断']) else None,axis=1).dropna().shape[0]}\")\n",
    "advice_len = df.apply(lambda r : len(r['出院后建议']) ,axis=1)\n",
    "print(f\"出院建议平均长度：{advice_len.mean()}\")\n",
    "advice_len.hist(bins=advice_len.max(),log=True)\n",
    "print(advice_len.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仁济住院数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_name_list = [name for name in os.listdir('./') if name.endswith('.csv')]\n",
    "dfs = {file_name:pd.read_csv(file_name, low_memory=False) for file_name in file_name_list}\n",
    "for df in dfs.values():\n",
    "    df.rename(columns={'jzzsy': '住院号'}, inplace=True)\n",
    "print(\"文件名:行数\")\n",
    "[(i,len(dfs[i])) for i in dfs]\n",
    "print(\"文件名:住院号数量\")\n",
    "[(i,len(set(dfs[i]['住院号']))) for i in dfs]\n",
    "intersection = set(dfs['202303_medical_history_enter.csv']['住院号'])\n",
    "black_list = set([\n",
    "    '202303_medical_history_leave_24h.csv',\n",
    "    '202303_medical_history_op_first_disease.csv',\n",
    "    '202303_medical_history_operation.csv',\n",
    "    '202303_medical_history_routine.csv'\n",
    "])\n",
    "union = set()\n",
    "for i in dfs:\n",
    "    union = union.union(set(dfs[i]['住院号']))\n",
    "\n",
    "for i in set(file_name_list) - black_list:\n",
    "    new_set = set(dfs[i]['住院号'])\n",
    "    intersection = new_set & intersection\n",
    "print(f\"住院号交集数量:{len(intersection)}, 重合比例:{len(intersection)}/{len(union)} {len(intersection)/len(union)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "advice = pd.read_csv(\"data/202303出院/202303出院有放射报告-医嘱.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "check = pd.read_csv(\"data/202303出院/202303出院有放射报告-检验.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "test = pd.read_csv(\"data/202303出院/202303出院有放射报告-检查.csv\", low_memory=False, encoding='gbk').rename(columns={'jzzsy': '住院号'})\n",
    "xlsx = pd.ExcelFile('data/202303出院/202303出院有放射报告-病史相关.xlsx')\n",
    "history = {sheet_name : xlsx.parse(sheet_name).rename(columns={'jzzsy': '住院号'}) for sheet_name in xlsx.sheet_names}\n",
    "\n",
    "\n",
    "zids = set(history['病案首页']['住院号'])\n",
    "chuyuan = []\n",
    "for zid in tqdm(zids):\n",
    "    d = {}\n",
    "    d['住院号'] = zid\n",
    "    for sheet_name in history:\n",
    "        sheet = history[sheet_name]\n",
    "        if \"住院号\" in sheet.columns:\n",
    "            d[sheet_name] = sheet[sheet['住院号'] == zid].to_dict(orient='records')\n",
    "    d['医嘱'] = advice[advice['住院号'] == zid].to_dict(orient='records')\n",
    "    d['检验'] = check[check['住院号'] == zid].to_dict(orient='records')\n",
    "    d['检查'] = test[test['住院号'] == zid].to_dict(orient='records')\n",
    "    chuyuan.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "chuyuan_sample = chuyuan[:20]\n",
    "json.dump(chuyuan_sample, open(\"data/chuyuan_data_sample.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)\n",
    "# json.dump(chuyuan, open(\"data/chuyuan_data.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chuyuan = json.load(open(\"data/chuyuan_data.json\"))\n",
    "check_nums = [len(d['检验']) for d in chuyuan]\n",
    "check_normal = [len([c for c in d['检验'] if c['结果值异常标志'] == 'NO']) for d in chuyuan]\n",
    "check_abnormal = [(i-j) for i,j in zip(check_nums, check_normal)]\n",
    "print(f\"average_check_num: {(sum(check_nums)/len(check_nums))}\\n\" \\\n",
    "    f\"average normal: {sum(check_normal)/len(check_normal)}\\n\" \\\n",
    "    f\"average abnormal: {sum(check_abnormal)/len(check_abnormal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Stdout2File\n",
    "from datetime import datetime\n",
    "\n",
    "int2date = lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M%S\").strftime(\"%Y/%m/%d %H:%M:%S\").replace(r\"2023/3\",r\"2023/03\")\n",
    "date2int = lambda x: int(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\").strftime(\"%Y%m%d%H%M%S\"))\n",
    "chuyuan_sample = json.load(open(\"data/chuyuan_data_sample.json\"))\n",
    "\n",
    "def time_stamps(d):\n",
    "    time_stamps = []\n",
    "    time_stamps.append((int2date(str(d['出院记录'][0]['入院日期时间'])),  f\"入院时间 {d['病案首页'][0]['入院诊断名称']}\"))\n",
    "    time_stamps.append((int2date(str(d['出院记录'][0]['出院日期时间'])[:14]),  '出院时间'))\n",
    "    if d.get(\"手术记录\"):\n",
    "        for i,op in enumerate(d['手术记录']):\n",
    "            time_stamps.append((int2date(str(op['手术开始日期时间'])),  f\"手术{i}:{op['手术及操作编码对应名称']} 开始时间\"))\n",
    "            if op.get('手术结束日期时间').strip():\n",
    "                time_stamps.append((int2date(str(op['手术结束日期时间'][:14])),  f\"手术{i}:{op['手术及操作编码对应名称']} 结束时间\"))\n",
    "    if d.get(\"日常病程记录\"):\n",
    "        for i,rec in enumerate(d['日常病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"日常病程{i} 记录时间:\"))\n",
    "    if d.get('首次病程记录'):\n",
    "        for i, rec in enumerate(d['首次病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"首次病程记录{i} 记录时间:\"))\n",
    "    if d.get('术后首次病程记录'):\n",
    "        for i, rec in enumerate(d['术后首次病程记录']):\n",
    "            time_stamps.append((int2date(str(rec['记录日期'])),  f\"术后首次病程记录{i} 记录时间:\"))\n",
    "    if d.get('医嘱'):\n",
    "        for i, rec in enumerate(d['医嘱']):\n",
    "            time_stamps.append((rec['开始时间'].replace(r\"2023/3\",r\"2023/03\"),  f\"医嘱{i}:{rec['医嘱名称']} 开始时间:\"))\n",
    "            if rec.get('结束时间'):\n",
    "                time_stamps.append((rec['结束时间'].replace(r\"2023/3\",r\"2023/03\"),  f\"医嘱{i}:{rec['医嘱名称']} 结束时间:\"))\n",
    "    if d.get('检验'):\n",
    "        for i, rec in enumerate(d['检验']):\n",
    "            time_stamps.append((rec['报告日期'].replace(r\"2023/3\",r\"2023/03\"),  f\"检验{i}:{rec['报告名称']} {rec['检验项目']} 报告日期:\"))\n",
    "    if d.get('检查'):\n",
    "        for i, rec in enumerate(d['检查']):\n",
    "            time_stamps.append((rec['报告日期'].replace(r\"2023/3\",r\"2023/03\"), f\"检查{i}:{rec['报告名称']} 报告日期:\"))\n",
    "    sorted_time_stamps = sorted(time_stamps, key=lambda x: date2int(x[0]))\n",
    "    return sorted_time_stamps\n",
    "\n",
    "with Stdout2File(\"data/chuyuan_time_stamps.txt\"):\n",
    "    for d in chuyuan_sample:\n",
    "        for s in time_stamps(d):\n",
    "            print(s)\n",
    "        print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "date2int = lambda x: int(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\").strftime(\"%Y%m%d%H%M%S\"))\n",
    "data = json.load(open(\"data/chuyuan/chuyuan_data.json\"))\n",
    "\n",
    "def advice_type(advice):\n",
    "    if advice.get(\"规格\") and advice['单次用量']!=1.0:\n",
    "        return \"药物\"\n",
    "    # if \"会诊\" in advice['医嘱名称']:\n",
    "    #     return \"会诊\"\n",
    "    keywords = [\"平扫\",\"脑电\",\"心电\",\"CT\",\"检查\",\"分析\",\"试验\",\"检测\",\"MRI\",\"静脉血\",\"超声\"]\n",
    "    for k in keywords:\n",
    "        if k in advice['医嘱名称']:\n",
    "            return \"检查\"\n",
    "    return False\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for d in tqdm(data):\n",
    "    if (not d.get(\"入院记录\") and not d.get(\"24小时出入院记录\")) or (not d.get(\"医嘱\") or not d.get(\"检验\")):\n",
    "        continue\n",
    "    text_dict =dict(\n",
    "        病人年龄=d['检验'][0]['年龄'],\n",
    "        病人性别=d['病案首页'][0]['性别'],\n",
    "        入院诊断名称=d['病案首页'][0]['入院诊断名称'],\n",
    "        主诉=d['入院记录'][0]['主诉'] if d.get('入院记录') else d['24小时出入院记录'][0]['主诉'],\n",
    "    )\n",
    "    \n",
    "    checks = []\n",
    "    for c in d['检验']:\n",
    "        try:\n",
    "            c['报告日期'] = date2int(c['报告日期'])\n",
    "            checks.append(c)\n",
    "        except:\n",
    "            continue\n",
    "    checks = sorted(checks, key=lambda x: x['报告日期'])\n",
    "    operations = sorted(d['手术记录'], key=lambda x: x['手术开始日期时间'])\n",
    "    # tests = sorted(d['检查'], key=lambda x: x['报告日期'])\n",
    "    \n",
    "    advices = []\n",
    "    for a in d['医嘱']:\n",
    "        try:\n",
    "            a['开始时间'] = date2int(a['开始时间'])\n",
    "            advices.append(a)\n",
    "        except:\n",
    "            continue\n",
    "    advices = sorted(advices, key=lambda x: x['开始时间'])\n",
    "    \n",
    "    # 如果有手术记录，过滤掉在第一次手术之后的所有信息\n",
    "    if operations:\n",
    "        first_op_time = operations[0]['手术开始日期时间']\n",
    "        checks = [c for c in checks if c['报告日期'] < first_op_time]\n",
    "        advices = [a for a in advices if a['开始时间'] < first_op_time]\n",
    "    \n",
    "    if not checks or not advices:\n",
    "        continue\n",
    "    \n",
    "    # 过滤掉在第一次检验报告出来之前的医嘱\n",
    "    advices = [a for a in advices if a['开始时间'] > checks[0]['报告日期']]\n",
    "    \n",
    "    # 过滤掉在最后一次检验报告出来之前的医嘱\n",
    "    # advices = [a for a in advices if a['开始时间'] > checks[-1]['报告日期']]\n",
    "    \n",
    "    # 只保留药物，会诊，检查三种医嘱\n",
    "    advices = [a for a in advices if advice_type(a)]\n",
    "    \n",
    "    # 对医嘱按时间聚类\n",
    "    new_advices = [dict(开始时间=a['开始时间'], 医嘱名称=a['医嘱名称'], 医嘱类型=advice_type(a)) for a in advices]\n",
    "    check_times = sorted(list(set([c['报告日期'] for c in checks])))\n",
    "    advice_groups = [[a for a in new_advices if check_times[i] < a['开始时间'] < check_times[i+1]] for i in range(len(check_times)-1)]\n",
    "    advice_groups = [a for a in advice_groups if a]\n",
    "\n",
    "    # 生成医嘱训练数据\n",
    "    text = \",\".join([f\"{k}: {v}\" for (k,v) in text_dict.items()])\n",
    "    for advice_group in advice_groups:\n",
    "        check_list = [dict(检验项目=c['检验项目'], 检验结果=c['检验结果'], 正常值范围=c['正常值范围'], 结果值异常标志=c['结果值异常标志'] if c['结果值异常标志']!=\"NO\" else \"正常\") for c in checks if c['报告日期'] < advice_group[0]['开始时间']]\n",
    "        advice_text = \",\".join([f\"{a['医嘱类型']}:{a['医嘱名称']}\" for a in advice_group])\n",
    "        train_data.append(dict(text=text,data=check_list,output=advice_text))\n",
    "\n",
    "json.dump(train_data, open(\"data/chuyuan/chuyuan_train.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_data[:50], open(\"data/chuyuan/chuyuan_train_sample.json\", 'w',encoding='utf-8',),ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HONESTY_EXTRACTION_TEMPLATE_EXP = \"\"\"USER:Pretend you are an honest person making statements about the world.\\nAssistant:\"\"\"\n",
    "HONESTY_EXTRACTION_TEMPLATE_REF = \"\"\"USER:Pretend you are a dishonest person making statements about the world.\\nAssistant:\"\"\"\n",
    "\n",
    "knows = {topic:{True:[],False:[]}for topic in topic_true_examples}\n",
    "\n",
    "for o in outputs:\n",
    "    ks = o['query_output'].split(\"<sep>\")\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"<sep\"))\n",
    "    ks = new_ks\n",
    "    new_ks = []\n",
    "    for k in ks:\n",
    "        new_ks.extend(k.split(\"\\n\"))\n",
    "    ks = new_ks\n",
    "    ks = [re.sub(\"[0-9]+. \",\"\", k).strip() for k in ks]\n",
    "    ks = [k for k in ks if k]\n",
    "    for k in ks:\n",
    "        if k not in knows[o['topic']][o['label']]:\n",
    "            knows[o['topic']][o['label']].append(k)\n",
    "    \n",
    "dst = []\n",
    "for topic in knows:\n",
    "    for i in range(min(len(knows[topic][True]),len(knows[topic][False]))):\n",
    "        true_input = HONESTY_EXTRACTION_TEMPLATE_EXP\n",
    "        false_input = HONESTY_EXTRACTION_TEMPLATE_REF\n",
    "        true_output = knows[topic][True][i]\n",
    "        false_output = knows[topic][False][i]\n",
    "        dst.append([dict(input=true_input,output=true_output,topic=topic,label=True),dict(input=false_input,output=false_output,topic=topic,label=False)])\n",
    "print('True-False Dst Size: ', len(dst))\n",
    "json.dump(knows, open(\"data/knows.json\", \"w\"), indent=4)\n",
    "json.dump(dst, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from model import INTERNLM_TEMPLATE\n",
    "\n",
    "\n",
    "usmle_test = list(jsonlines.open(\"data/usmle/questions/US/test.jsonl\"))\n",
    "usmle_test_for_infer = []\n",
    "\n",
    "for d in usmle_test:\n",
    "    options_strs = [f\"{op}: {d['options'][op]}\" for op in d['options']]\n",
    "    input = f\"Question: {d['question']} Options: {'; '.join(options_strs)}. Output: The correct answer is option\"\n",
    "    input = INTERNLM_TEMPLATE.format(input)\n",
    "    d['input'] = input\n",
    "    d['labels'] = ['A', 'B', 'C', 'D', 'E']\n",
    "    usmle_test_for_infer.append(d)\n",
    "\n",
    "json.dump(usmle_test_for_infer, open(\"data/mgpu_infer/usmle_test_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from model import *\n",
    "import os\n",
    "\n",
    "tqa = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/TruthfulQA-main/data/mc_task.json\"))\n",
    "print(tqa[0])\n",
    "tqa_dst = [dict(input=INTERNLM_TEMPLATE.format(d['question']),labels=list(d['mc1_targets'].keys()),gt=list(d['mc1_targets'].values()).index(1)) for d in tqa]\n",
    "print(tqa_dst[0])\n",
    "json.dump(tqa_dst, open(\"data/mgpu_infer/truthfulqa_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "medqa_test = json.load(open(\"data/usmle/questions/US/test.json\"))\n",
    "print('medqa_test: ', medqa_test[0])\n",
    "medqa_test_infer = []\n",
    "for d in medqa_test:\n",
    "    options = d['options']\n",
    "    option_text = \";\".join([f\"{k}: {options[k]}\" for k in options])\n",
    "    # input_text = INTERNLM_TEMPLATE.format(\"Question:\"+d['question']+\"; Options:\"+option_text) + \"The correct answer is option\"\n",
    "    input_text = INTERNLM_TEMPLATE.format(\"Options:\"+option_text) + \"The correct answer is option\"\n",
    "    medqa_test_infer.append(dict(input=input_text, labels=list(options.keys()), gt=d['answer']))\n",
    "print('medqa_test_infer: ', medqa_test_infer[0])\n",
    "json.dump(medqa_test_infer, open(\"data/mgpu_infer/medqa_test_noquestion_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.llm_utils import get_free_gpus\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "free_gpus = get_free_gpus()\n",
    "command = f'CUDA_VISIBLE_DEVICES={\",\".join([str(i) for i in free_gpus])} \\\n",
    "            torchrun --nproc_per_node {len(free_gpus)} --master_port=12570 /home/cs/yangyuchen/guoyiqiu/gpt_re/mgpu_infer.py \\\n",
    "            --func infer \\\n",
    "            --model_path /home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms \\\n",
    "            --dst_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer/medqa_test_noquestion_inference.json \\\n",
    "            --save_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_noquestion_inference.json \\\n",
    "            --mnt 8 '\n",
    "\n",
    "process = subprocess.Popen(command, shell=True)\n",
    "print(f\"shell process id: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "internlm_chat_7b_usmle_test_generate = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_generate.json'))\n",
    "internlm_chat_7b_usmle_test_inference = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_inference.json'))\n",
    "\n",
    "inference_acc = 0\n",
    "generate_acc = 0\n",
    "i_g = 0\n",
    "for di,dg in zip(internlm_chat_7b_usmle_test_inference, internlm_chat_7b_usmle_test_generate):\n",
    "    pred_i = ['A','B','C','D','E'][np.argmin(di['label_loss'])]\n",
    "    pred_g = dg['output'].replace(\"<eoa>\",\"\").replace(\"</s>\",\"\").strip()[0]\n",
    "    gt = di['answer_idx']\n",
    "    inference_acc += (pred_i == gt)\n",
    "    generate_acc += (pred_g == gt)\n",
    "    i_g += (pred_i == pred_g)\n",
    "    \n",
    "print(f\"inference_acc: {inference_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"generate_acc: {generate_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"i_g: {i_g/len(internlm_chat_7b_usmle_test_inference)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "medqa_test_withquestion_result = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_inference.json\"))\n",
    "acc_w = [['A','B','C','D','E'][np.argmin(d['label_loss'])]==d['gt'] for d in medqa_test_withquestion_result]\n",
    "print(f\"acc_w: {sum(acc_w)/len(acc_w)}\")\n",
    "medqa_test_noquestion_result = json.load(open(\"/home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/llama-2-13b-chat_medqa_test_noquestion_inference.json\"))\n",
    "acc_wo = [['A','B','C','D','E'][np.argmin(d['label_loss'])]==d['gt'] for d in medqa_test_noquestion_result]\n",
    "print(f\"acc_wo: {sum(acc_wo)/len(acc_wo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_llm_gyq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
