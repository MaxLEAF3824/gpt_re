{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM控制面板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from IPython.display import display\n",
    "\n",
    "model_list = [\n",
    "        (\"gpt2\", \"/home/cs/yangyuchen/guoyiqiu/my_models/gpt2\"),\n",
    "        (\"llama_7b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-7b-hf\"),\n",
    "        (\"llama_13b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-13b\"),\n",
    "        (\"llama-2-7b-chat\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-2-7b-chat-hugging\"),\n",
    "        (\"llama-2-13b-chat\", \"/home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms\"),\n",
    "        (\"vicuna_7b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/\"),\n",
    "        (\"internlm-chat-7b\", \"/home/cs/yangyuchen/yushengliao/Medical_LLM/internlm-chat-7b\"),\n",
    "        (\"internlm-chat-20b\", \"/home/cs/yangyuchen/guoyiqiu/my_models/internlm-chat-20b\"),\n",
    "    ]\n",
    "\n",
    "panel = LLMPanel(model_list, chat_template=INTERNLM_TEMPLATE)\n",
    "display(panel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utokens_tab, entropy_tab = panel.mt.vis_sentence(f'<|User|>:Pretend you’re an honest person making statements about the world.<eoh>\\n<|Bot|>:stimulus')\n",
    "utokens_tab.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "usmle_test = list(jsonlines.open(\"data/usmle/questions/US/test.jsonl\"))\n",
    "usmle_test_for_infer = []\n",
    "\n",
    "for d in usmle_test:\n",
    "    options_strs = [f\"{op}: {d['options'][op]}\" for op in d['options']]\n",
    "    input = f\"Question: {d['question']} Options: {'; '.join(options_strs)}. Output: The correct answer is option\"\n",
    "    input = INTERNLM_TEMPLATE.format(input)\n",
    "    d['input'] = input\n",
    "    d['labels'] = ['A', 'B', 'C', 'D', 'E']\n",
    "    usmle_test_for_infer.append(d)\n",
    "\n",
    "json.dump(usmle_test_for_infer, open(\"data/mgpu_infer/usmle_test_inference.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.llm_utils import get_free_gpus\n",
    "import subprocess\n",
    "\n",
    "free_gpus = get_free_gpus()\n",
    "command = f'CUDA_VISIBLE_DEVICES={\",\".join([str(i) for i in free_gpus])} \\\n",
    "            torchrun --nproc_per_node {len(free_gpus)} --master_port=12570 /home/cs/yangyuchen/guoyiqiu/gpt_re/mgpu_infer.py \\\n",
    "            --func gen \\\n",
    "            --model_path /home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/ \\\n",
    "            --dst_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer/usmle_test_inference.json \\\n",
    "            --save_path /home/cs/yangyuchen/guoyiqiu/gpt_re/data/mgpu_infer_output/vicuna-7b_usmle_test_generate.json \\\n",
    "            --mnt 8'\n",
    "\n",
    "process = subprocess.Popen(command, shell=True)\n",
    "print(f\"shell process id: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "internlm_chat_7b_usmle_test_generate = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_generate_chat.json'))\n",
    "internlm_chat_7b_usmle_test_inference = json.load(open('data/mgpu_infer_output/internlm-chat-7b_usmle_test_inference_chat.json'))\n",
    "\n",
    "inference_acc = 0\n",
    "generate_acc = 0\n",
    "i_g = 0\n",
    "for di,dg in zip(internlm_chat_7b_usmle_test_inference, internlm_chat_7b_usmle_test_generate):\n",
    "    pred_i = ['A','B','C','D','E'][np.argmin(di['label_loss'])]\n",
    "    pred_g = dg['output'].replace(\"<eoa>\",\"\").replace(\"</s>\",\"\").strip()[0]\n",
    "    gt = di['answer_idx']\n",
    "    inference_acc += (pred_i == gt)\n",
    "    generate_acc += (pred_g == gt)\n",
    "    i_g += (pred_i == pred_g)\n",
    "    \n",
    "print(f\"inference_acc: {inference_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"generate_acc: {generate_acc/len(internlm_chat_7b_usmle_test_inference)}\")\n",
    "print(f\"i_g: {i_g/len(internlm_chat_7b_usmle_test_inference)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import re\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "os.environ['http_proxy'] = '127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = '127.0.0.1:7890'\n",
    "\n",
    "true_query_template = \"Generate 40 statements about the topic {topic}. The statements should be true and brief and contain factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sob> and <eob>.\"\n",
    "\n",
    "false_query_template = \"Generate 40 false statements about the topic {topic}. The statements should be incorrect and brief and contain wrong factual knowledge. You can use the following statements as examples: {examples}. The statements should be split by <sob> and <eob>.\"\n",
    "\n",
    "topic_true_examples = {\n",
    "    \"Cities\": \"Oranjestad is a city in Aruba.\" , \n",
    "    \"Inventions\": \"Grace Hopper invented the COBOL programming language.\" , \n",
    "    \"Chemical Elements\": \"Boron is used in the production of glass and ceramics.\" , \n",
    "    \"Animals\": \"The llama has a diet of herbivore.\" , \n",
    "    \"Companies\": \"Meta Platforms has headquarters in United States.\" , \n",
    "    \"Scientific Facts\": \"The Earth’s tides are primarily caused by the gravitational pull of the moon.\",\n",
    "    \"Medical\": \"Benign tumors typically grow slowly and do not invade surrounding tissues or spread to other areas.\"\n",
    "}\n",
    "topic_false_examples = {\n",
    "    \"Cities\": \"Wellington is a name of a country.\" ,\n",
    "    \"Inventions\": \"David Schwarz lived in France.\" ,\n",
    "    \"Chemical Elements\": \"Indium is in the Lanthanide group.\" ,\n",
    "    \"Animals\": \"The whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\" ,\n",
    "    \"Companies\": \"KDDI operates in the industry of Materials.\" , \n",
    "    \"Medical\": \"The normal range for human body temperature is 50-55 degrees Celsius.\"\n",
    "}\n",
    "true_queries = [dict(query_input=true_query_template.format(topic=k, examples=v)) for (k,v) in topic_true_examples.items()]\n",
    "false_queries = [dict(query_input=false_query_template.format(topic=k, examples=v)) for (k,v) in topic_false_examples.items()]\n",
    "\n",
    "true_knowledge = multithread_query_chatgpt(true_queries)\n",
    "false_knowledge = multithread_query_chatgpt(false_queries)\n",
    "\n",
    "\n",
    "true_knows = []\n",
    "false_knows = []\n",
    "\n",
    "\n",
    "for d in true_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=True) for k in knows if k]\n",
    "    true_knows.extend(knows)\n",
    "    \n",
    "\n",
    "for d in false_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=False) for k in knows if k]\n",
    "    false_knows.extend(knows)\n",
    "    \n",
    "all_knows = true_knows + false_knows\n",
    "json.dump(all_knows, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "true_knows = []\n",
    "false_knows = []\n",
    "\n",
    "\n",
    "for d in true_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=True) for k in knows if k]\n",
    "    true_knows.extend(knows)\n",
    "    \n",
    "\n",
    "for d in false_knowledge:\n",
    "    knows = d['query_output'].split(\"<sob>\")\n",
    "    knows = [re.sub(\"[0-9]+. \",\"\", k).replace(\"<eob>\",\"\").strip() for k in knows]\n",
    "    knows = [dict(input=k,label=False) for k in knows if k]\n",
    "    false_knows.extend(knows)\n",
    "    \n",
    "all_knows = true_knows + false_knows\n",
    "json.dump(all_knows, open(\"data/true_false_dataset.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "HONESTY_EXTRACTION_TEMPLATE_EXP = \"\"\"<|User|>:Pretend you’re an honest person making statements about the world.<eoh>\\n<|Bot|>:{stimulus}\"\"\"\n",
    "HONESTY_EXTRACTION_TEMPLATE_REF = \"\"\"<|User|>:Pretend you’re a dishonest person making statements about the world.<eoh>\\n<|Bot|>:{stimulus}\"\"\"\n",
    "tf_dataset = json.load(open(\"data/true_false_dataset.json\"))\n",
    "\n",
    "def pca():\n",
    "    pass\n",
    "\n",
    "def make_function_stimuli_dataset(tmp_t, tmp_f, dst):\n",
    "    stimuli_dst = []\n",
    "    dst_t = [d for d in dst if d['label']]\n",
    "    dst_f = [d for d in dst if not d['label']]\n",
    "    length = min(len(dst_t),len(dst_f))\n",
    "    dst_t = dst_t[:length]\n",
    "    dst_f = dst_f[:length]\n",
    "    for t,f in zip(dst_t,dst_f):\n",
    "        stimuli_dst.append([dict(input=tmp_t.format(stimulus=t['input']),label=True), dict(input=tmp_f.format(stimulus=f['input']),label=False)])\n",
    "    return stimuli_dst\n",
    "\n",
    "def lat(mt, stimuli_dst, k):\n",
    "    hook_configs = [LLMHookConfig(module_name='block', layer=l) for l in mt.n_layer]\n",
    "    with LLMHooker(mt, hook_configs) as hooker:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
