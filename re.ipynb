{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM控制面板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e445e11002a64450969b8906d64512cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLMPanel(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', '/home/cs/yangyuchen/guoyiq…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933dea3d666a43299030174cec7c07aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model import *\n",
    "from IPython.display import display\n",
    "\n",
    "model_list = {\n",
    "    \"gpt2\": \"/home/cs/yangyuchen/guoyiqiu/my_models/gpt2\",\n",
    "    \"llama_13b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-13b\",\n",
    "    \"llama-2-7b-chat\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-2-7b-chat-hugging\",\n",
    "    \"llama-2-13b-chat\": \"/home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms\",\n",
    "    \"vicuna_7b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/\",\n",
    "    \"internlm-chat-7b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/internlm-chat-7b\",\n",
    "    \"internlm-chat-20b\": \"/home/cs/yangyuchen/guoyiqiu/my_models/internlm-chat-20b\",\n",
    "    \"vicuna-33b-v1.3\": \"/home/cs/yangyuchen/guoyiqiu/my_models/models--lmsys--vicuna-33b-v1.3/snapshots/ef8d6becf883fb3ce52e3706885f761819477ab4\",\n",
    "}\n",
    "\n",
    "panel = LLMPanel(model_list, chat_template=INTERNLM_TEMPLATE)\n",
    "display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAT on honesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting train neuron_acts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec8ed1627354dc8988f2be08de4be0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing reading_vectors\n",
      "collecting test neuron_acts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19aae37f0494f85aa9ee719d1566590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores\n",
      "importance: 0.0322\n",
      "acc: 0.0408,\n",
      "mean_diff: -0.3358\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyecharts.options as opts\n",
    "from pyecharts.charts import Bar, Line, Scatter\n",
    "import json\n",
    "from model import *\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "    \n",
    "\n",
    "def calcu_score(neuron_act, reading_vectors, mean_vector, std_vector):\n",
    "    \"\"\"\n",
    "    reading_vectors: [layer, hidden_size]\n",
    "    mean_vector: [layer * hidden_size]\n",
    "    std_vector: [layer * hidden_size]\n",
    "    \"\"\"\n",
    "    neuron_act = neuron_act.transpose(0,1) # [seq_len, layer, hidden_size]\n",
    "    seq_len, layer, hidden_size = neuron_act.shape\n",
    "    neuron_act = neuron_act.reshape(seq_len, layer * hidden_size) # [seq_len, layer * hidden_size]\n",
    "    neuron_act = (neuron_act - mean_vector) / std_vector\n",
    "    neuron_act = neuron_act.reshape(seq_len, layer, hidden_size) # [seq_len, layer, hidden_size]\n",
    "    scores = (neuron_act * reading_vectors.repeat(seq_len, 1, 1)).sum(-1) # [seq_len, layer]\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def compute_reading_vectors(neuron_acts):\n",
    "    diff = []\n",
    "    for (act1,act2) in neuron_acts:\n",
    "        l = min(act1.shape[1],act2.shape[1])\n",
    "        act1 = act1[:,:l,:]\n",
    "        act2 = act2[:,:l,:]\n",
    "        diff.append(act1 - act2)\n",
    "    diff = torch.cat(diff, dim=1).transpose(0,1) # [sum(seq_len), layer, hidden_size]\n",
    "    sample_size,n_layer,hidden_size = diff.shape\n",
    "    diff = diff.reshape(sample_size, -1).numpy() # [sample_size, layer * hidden_size]\n",
    "    mean_vector = np.mean(diff, axis=0)\n",
    "    std_vector = np.std(diff, axis=0)\n",
    "    diff = (diff - mean_vector) / std_vector\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(diff)\n",
    "    reading_vectors = pca.components_[0].reshape(n_layer, hidden_size) # [layer, hidden_size]\n",
    "    # print('reading_vectors: ', reading_vectors.shape)\n",
    "    reading_vectors = torch.from_numpy(reading_vectors)\n",
    "    mean_vector = torch.from_numpy(mean_vector)\n",
    "    std_vector = torch.from_numpy(std_vector)\n",
    "    return reading_vectors, mean_vector, std_vector, pca.explained_variance_ratio_[0]\n",
    "\n",
    "\n",
    "def collect_neuron_acts(mt, stimuli_dst, capture_window, local_bsz=32):\n",
    "    hook_configs = [LLMHookerConfig(module_name='block', layer=l) for l in range(mt.n_layer)]\n",
    "    neuron_acts = []\n",
    "    data_bsz = local_bsz // 2\n",
    "    stimuli_dst = [stimuli_dst[i:i+data_bsz] for i in range(0, len(stimuli_dst), data_bsz)]\n",
    "    for batch_pairs in tqdm(stimuli_dst):\n",
    "        pairs = []\n",
    "        for pair in batch_pairs:\n",
    "            pairs += pair\n",
    "        prompt_lens = [len(mt.tok(s['input'])['input_ids']) for s in pairs]\n",
    "        seq_lens = [len(mt.tok(s['input']+s['output'])['input_ids']) for s in pairs]\n",
    "        with PaddingSide(mt.tok, 'right'):\n",
    "            input_ids = mt.tok([s['input']+s['output'] for s in pairs], return_tensors='pt', padding=True)['input_ids']\n",
    "        with torch.no_grad(), LLMHooker(mt, hook_configs) as hooker:\n",
    "            mt.model(input_ids=input_ids.to(mt.model.device))\n",
    "            sentences_repr = torch.stack([h.outputs[0] for h in hooker.hooks]).transpose(0,1) # [bsz, layer, seq_len, hidden_size]\n",
    "        batch_neuron_acts = []\n",
    "        for i,repr in enumerate(sentences_repr):\n",
    "            prompt_len = prompt_lens[i]\n",
    "            seq_len = seq_lens[i]\n",
    "            start = prompt_len + capture_window[0] if capture_window[0] >= 0 else seq_len - capture_window[0]\n",
    "            end = prompt_len + capture_window[1] if capture_window[1] >= 0 else seq_len - capture_window[1] + 1\n",
    "            batch_neuron_acts.append(repr[:,start:end,:])\n",
    "        batch_neuron_acts = [[batch_neuron_acts[i],batch_neuron_acts[i+1]] for i in range(0, len(batch_neuron_acts), 2)]\n",
    "        neuron_acts.extend(batch_neuron_acts)\n",
    "    return neuron_acts # [layer, window_size, hidden_size]\n",
    "\n",
    "\n",
    "def full_pipeline(mt, train_dst, test_dst, capture_window, compare_window, local_bsz=64):\n",
    "    print('collecting train neuron_acts')\n",
    "    neuron_acts = collect_neuron_acts(mt, train_dst, capture_window=capture_window,local_bsz=local_bsz)\n",
    "    print('computing reading_vectors')\n",
    "    reading_vectors, mean_vector, std_vector, importance = compute_reading_vectors(neuron_acts)\n",
    "    print('collecting test neuron_acts')\n",
    "    test_neuron_acts = collect_neuron_acts(mt, test_dst, capture_window=compare_window,local_bsz=local_bsz)\n",
    "    print('calculating scores')\n",
    "    scores = [[calcu_score(tna, reading_vectors, mean_vector, std_vector),calcu_score(fna, reading_vectors, mean_vector, std_vector)] for (tna, fna) in test_neuron_acts]\n",
    "    mean_diff = np.mean([s[1]-s[0] for s in scores])\n",
    "    acc = sum([1 for s in scores if s[0]<s[1]])/len(scores)\n",
    "    return acc, mean_diff, importance\n",
    "\n",
    "\n",
    "\n",
    "tf_dst = json.load(open(\"data/true_false_dataset.json\"))\n",
    "random.shuffle(tf_dst)\n",
    "\n",
    "# IID_Hard\n",
    "reserved_topic = \"Medical\"\n",
    "train_dst_iid_hard = [[td,fd] for (td,fd) in tf_dst if td['topic']!= \"Medical\"]\n",
    "test_dst_iid_hard = [[td,fd] for (td,fd) in tf_dst if td['topic']== \"Medical\"]\n",
    "# IID_Weak\n",
    "train_dst_iid_weak = tf_dst[:len(train_dst_iid_hard)]\n",
    "test_dst_iid_weak = tf_dst[len(train_dst_iid_hard):]\n",
    "# OOD\n",
    "reserved_topic = \"Medical\"\n",
    "prompt = \"USER:Tell me a fact.\\nAssistant:\"\n",
    "train_dst_ood = [[td,fd] for (td,fd) in tf_dst if td['topic']!= \"Medical\"]\n",
    "test_dst_ood = [[dict(input=prompt,output=td['output'],topic=td['topic'],label=True),\n",
    "                 dict(input=prompt,output=fd['output'],topic=fd['topic'],label=False)] \n",
    "                for (td,fd) in tf_dst if td['topic']== \"Medical\"]\n",
    "\n",
    "config = {\n",
    "    \"mt\": panel.mt,\n",
    "    \"train_dst\": train_dst_iid_hard,\n",
    "    \"test_dst\": test_dst_iid_hard,\n",
    "    \"capture_window\": (0,-1),\n",
    "    \"compare_window\": (0,-1),\n",
    "    \"local_bsz\": 64,\n",
    "}\n",
    "\n",
    "acc, mean_diff, importance = full_pipeline(**config)\n",
    "print(f\"importance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
