{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM控制面板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75849dc8516e43dea96b4be7e4362873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLMPanel(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', '/home/cs/yangyuchen/guoyiq…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f64599e2e640d58a78edc58324cb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is ready. Time cost: 45.28s\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from IPython.display import display\n",
    "\n",
    "model_list = {\n",
    "    \"gpt2\": \"/home/cs/yangyuchen/guoyiqiu/my_models/gpt2\",\n",
    "    \"llama_13b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-13b\",\n",
    "    \"llama-2-7b-chat\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-2-7b-chat-hugging\",\n",
    "    \"llama-2-13b-chat\": \"/home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms\",\n",
    "    \"vicuna_7b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/\",\n",
    "    \"internlm-chat-7b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/internlm-chat-7b\",\n",
    "    \"internlm-chat-20b\": \"/home/cs/yangyuchen/guoyiqiu/my_models/internlm-chat-20b\",\n",
    "    \"vicuna-33b-v1.3\": \"/home/cs/yangyuchen/guoyiqiu/my_models/models--lmsys--vicuna-33b-v1.3/snapshots/ef8d6becf883fb3ce52e3706885f761819477ab4\",\n",
    "}\n",
    "\n",
    "panel = LLMPanel(model_list, chat_template=INTERNLM_TEMPLATE)\n",
    "display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honesty Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import *\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "def calcu_score(neuron_act, reading_vectors, mean_vector, std_vector):\n",
    "    \"\"\"\n",
    "    reading_vectors: [layer, hidden_size]\n",
    "    mean_vector: [layer * hidden_size]\n",
    "    std_vector: [layer * hidden_size]\n",
    "    \"\"\"\n",
    "    neuron_act = neuron_act.transpose(0,1) # [seq_len, layer, hidden_size]\n",
    "    seq_len, layer, hidden_size = neuron_act.shape\n",
    "    neuron_act = neuron_act.reshape(seq_len, layer * hidden_size) # [seq_len, layer * hidden_size]\n",
    "    neuron_act = (neuron_act - mean_vector) / std_vector\n",
    "    neuron_act = neuron_act.reshape(seq_len, layer, hidden_size) # [seq_len, layer, hidden_size]\n",
    "    scores = (neuron_act * reading_vectors.repeat(seq_len, 1, 1)).sum(-1) # [seq_len, layer]\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def compute_reading_vectors(neuron_acts, sign=False):\n",
    "    '''\n",
    "    neuron_acts : List[Tuple[torch.Tensor [layer, window_size, hidden_size]]]\n",
    "    '''\n",
    "    diff = []\n",
    "    for (act1,act2) in neuron_acts:\n",
    "        l = min(act1.shape[1],act2.shape[1])\n",
    "        diff.append(act1[:,:l,:] - act2[:,:l,:])\n",
    "    diff = torch.cat(diff, dim=1).transpose(0,1) # [sum(seq_len), layer, hidden_size]\n",
    "    sample_size, n_layer, hidden_size = diff.shape\n",
    "    diff = diff.reshape(sample_size, -1) # [sample_size, layer * hidden_size]\n",
    "    mean_vector = torch.mean(diff, dim=0)\n",
    "    std_vector = torch.std(diff, dim=0)\n",
    "    diff = (diff.numpy() - mean_vector.numpy()) / std_vector.numpy()\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(diff)\n",
    "    reading_vectors = pca.components_[0].reshape(n_layer, hidden_size) # [layer, hidden_size]\n",
    "    # print('reading_vectors: ', reading_vectors.shape)\n",
    "    reading_vectors = torch.from_numpy(reading_vectors) # [layer, hidden_size]\n",
    "    if sign:\n",
    "        acts_t = torch.cat()\n",
    "    return reading_vectors, mean_vector, std_vector, pca.explained_variance_ratio_[0]\n",
    "\n",
    "\n",
    "def collect_neuron_acts(mt, dst, capture_window, layers, local_bsz=32):\n",
    "    data_bsz = local_bsz // 2\n",
    "    dst = [dst[i:i+data_bsz] for i in range(0, len(dst), data_bsz)]\n",
    "    neuron_acts = []\n",
    "    for batch_pairs in tqdm(dst):\n",
    "        pairs = []\n",
    "        for pair in batch_pairs:\n",
    "            pairs += pair\n",
    "        prompt_lens = [len(mt.tok(s['input'])['input_ids']) for s in pairs]\n",
    "        seq_lens = [len(mt.tok(s['input']+s['output'])['input_ids']) for s in pairs]\n",
    "        with PaddingSide(mt.tok, 'right'):\n",
    "            input_ids = mt.tok([s['input']+s['output'] for s in pairs], return_tensors='pt', padding=True)['input_ids']\n",
    "        hook_configs = [LLMHookerConfig(module_name='block', layer=l) for l in layers]\n",
    "        with torch.no_grad(), LLMHooker(mt, hook_configs) as hooker:\n",
    "            mt.model(input_ids=input_ids.to(mt.model.device))\n",
    "            sentences_repr = torch.stack([h.outputs[0] for h in hooker.hooks]).transpose(0,1) # [bsz, layer, seq_len, hidden_size]\n",
    "        batch_neuron_acts = []\n",
    "        for i,repr in enumerate(sentences_repr):\n",
    "            prompt_len = prompt_lens[i]\n",
    "            seq_len = seq_lens[i]\n",
    "            start = prompt_len + capture_window[0] if capture_window[0] >= 0 else seq_len + capture_window[0]\n",
    "            end = prompt_len + capture_window[1] if capture_window[1] > 0 else seq_len + capture_window[1]\n",
    "            batch_neuron_acts.append(repr[:,start:end,:])\n",
    "        batch_neuron_acts = [[batch_neuron_acts[i],batch_neuron_acts[i+1]] for i in range(0, len(batch_neuron_acts), 2)]\n",
    "        neuron_acts.extend(batch_neuron_acts)\n",
    "    return neuron_acts # [layer, window_size, hidden_size]\n",
    "\n",
    "\n",
    "def full_pipeline(mt, train_dst, test_dst, capture_window, compare_window, layers=None, local_bsz=64):\n",
    "    if layers is None:\n",
    "        layers = list(range(mt.n_layer))\n",
    "    neuron_acts = collect_neuron_acts(mt, train_dst, capture_window, layers, local_bsz=local_bsz)\n",
    "    rv, mv, sv, importance = compute_reading_vectors(neuron_acts)\n",
    "    test_neuron_acts = collect_neuron_acts(mt, test_dst, compare_window, layers, local_bsz=local_bsz)\n",
    "    scores = [[calcu_score(tna, rv, mv, sv),calcu_score(fna, rv, mv, sv)] for (tna, fna) in test_neuron_acts]\n",
    "    mean_diff = np.mean([s[0]-s[1] for s in scores])\n",
    "    acc = sum([1 for s in scores if s[0]>s[1]])/len(scores)\n",
    "    return acc, mean_diff, importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"re.ipynb\"\n",
    "\n",
    "mt = LLM.from_pretrained(model_path=\"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-2-7b-chat-hugging\").cuda(get_free_gpus([0]))\n",
    "\n",
    "tf_dst = json.load(open(\"data/true_false_dataset.json\"))\n",
    "# IID_Hard\n",
    "reserved_topic = \"Medical\"\n",
    "train_dst_iid_hard = [[td,fd] for (td,fd) in tf_dst if td['topic'] != \"Medical\"]\n",
    "test_dst_iid_hard = [[td,fd] for (td,fd) in tf_dst if td['topic'] == \"Medical\"]\n",
    "# IID_Weak\n",
    "random.shuffle(tf_dst)\n",
    "train_dst_iid_weak = tf_dst[:len(train_dst_iid_hard)]\n",
    "test_dst_iid_weak = tf_dst[len(train_dst_iid_hard):]\n",
    "# OOD\n",
    "reserved_topic = \"Medical\"\n",
    "prompt = \"USER:Tell me a fact.\\nAssistant:\"\n",
    "train_dst_ood = [[td,fd] for (td,fd) in tf_dst if td['topic'] != \"Medical\"]\n",
    "test_dst_ood = [[dict(input=prompt,output=td['output'],topic=td['topic'],label=True),\n",
    "                 dict(input=prompt,output=fd['output'],topic=fd['topic'],label=False)] \n",
    "                for (td,fd) in tf_dst if td['topic'] == \"Medical\"]\n",
    "\n",
    "# Layer Sweep\n",
    "config = {\n",
    "    \"capture_window\": (0,0),\n",
    "    \"compare_window\": (0,0),\n",
    "    \"local_bsz\": 32,\n",
    "}\n",
    "\n",
    "wandb.init(config=config, \n",
    "           project=\"lat layer sweep\", \n",
    "           name=\"vicuna_33b_seed42\",\n",
    "           dir=\"output/lat_layer_sweep\",\n",
    "           job_type=\"inference\")\n",
    "\n",
    "for l in range(mt.n_layer):\n",
    "    layers = [l]\n",
    "    acc, mean_diff, importance = full_pipeline(mt=mt, train_dst=train_dst_iid_weak, test_dst=test_dst_iid_weak,layers=layers, **config)\n",
    "    print(f\"iid weak\\nimportance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")\n",
    "    wandb.log({\"iid weak\": acc if acc>0.5 else 1-acc, \"mean_diff\": abs(mean_diff), \"importance\": importance}, step=l)\n",
    "    acc, mean_diff, importance = full_pipeline(mt=mt, train_dst=train_dst_iid_hard, test_dst=test_dst_iid_hard,layers=layers, **config)\n",
    "    print(f\"iid hard\\nimportance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")\n",
    "    wandb.log({\"iid hard\": acc if acc>0.5 else 1-acc, \"mean_diff\": abs(mean_diff), \"importance\": importance}, step=l)\n",
    "    acc, mean_diff, importance = full_pipeline(mt=mt, train_dst=train_dst_ood, test_dst=test_dst_ood, layers=layers, **config)\n",
    "    print(f\"ood\\nimportance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")\n",
    "    wandb.log({\"ood\": acc if acc>0.5 else 1-acc, \"mean_diff\": abs(mean_diff), \"importance\": importance}, step=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honesty Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
