{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM控制面板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/guoyiqiu/miniconda3/envs/med_llm_gyq/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU device on this machine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de56fc71beb449249a7411f5b0be3a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLMPanel(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', '../my_models/gpt2'), ('lla…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is ready. Time cost: 2.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from IPython.display import display\n",
    "\n",
    "model_list = {\n",
    "    \"gpt2\": \"../my_models/gpt2\",\n",
    "    \"llama_13b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-13b\",\n",
    "    \"llama-2-7b-chat\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-2-7b-chat-hugging\",\n",
    "    \"llama-2-13b-chat\": \"/home/cs/yangyuchen/guoyiqiu/my_models/Llama-2-13b-chat-ms\",\n",
    "    \"vicuna_7b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/vicuna-7b/\",\n",
    "    \"internlm-chat-7b\": \"/home/cs/yangyuchen/yushengliao/Medical_LLM/internlm-chat-7b\",\n",
    "    \"internlm-chat-20b\": \"/home/cs/yangyuchen/guoyiqiu/my_models/internlm-chat-20b\",\n",
    "    \"vicuna-33b-v1.3\": \"/home/cs/yangyuchen/guoyiqiu/my_models/models--lmsys--vicuna-33b-v1.3/snapshots/ef8d6becf883fb3ce52e3706885f761819477ab4\",\n",
    "}\n",
    "\n",
    "panel = LLMPanel(model_list, chat_template=ChatTemplate.INTERNLM_TEMPLATE)\n",
    "display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_out1:  tensor([[100, 200, 300,  84,  12,  75,  84,  12]])\n",
      "gen_out2:  tensor([[ 200,  200,  300,   84, 1087,   68,  388,  796]])\n",
      "target_embedding:  torch.Size([1, 768])\n",
      "embedding is called!\n",
      "original_input:tensor([[100, 200, 300]]) torch.Size([1, 3])\n",
      "original_output:torch.Size([1, 3, 768])\n",
      "modified_output:torch.Size([1, 3, 768])\n",
      "embedding is called!\n",
      "original_input:tensor([[84]]) torch.Size([1, 1])\n",
      "original_output:torch.Size([1, 1, 768])\n",
      "modified_output:torch.Size([1, 1, 768])\n",
      "embedding is called!\n",
      "original_input:tensor([[1087]]) torch.Size([1, 1])\n",
      "original_output:torch.Size([1, 1, 768])\n",
      "modified_output:torch.Size([1, 1, 768])\n",
      "embedding is called!\n",
      "original_input:tensor([[68]]) torch.Size([1, 1])\n",
      "original_output:torch.Size([1, 1, 768])\n",
      "modified_output:torch.Size([1, 1, 768])\n",
      "embedding is called!\n",
      "original_input:tensor([[388]]) torch.Size([1, 1])\n",
      "original_output:torch.Size([1, 1, 768])\n",
      "modified_output:torch.Size([1, 1, 768])\n",
      "gen_out3:  tensor([[ 100,  200,  300,   84, 1087,   68,  388,  796]])\n",
      "emb_called_count:  5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "input_ids1 = torch.tensor([100,200,300]).unsqueeze(0)\n",
    "gen_out1 = panel.mt.model.generate(input_ids=input_ids1, max_new_tokens=5)\n",
    "print('gen_out1: ', gen_out1)\n",
    "\n",
    "input_ids2 = torch.tensor([200,200,300]).unsqueeze(0)\n",
    "gen_out2 = panel.mt.model.generate(input_ids=input_ids2, max_new_tokens=5)\n",
    "print('gen_out2: ', gen_out2)\n",
    "\n",
    "source_id = 100\n",
    "target_id = 200\n",
    "target_embedding = panel.mt.embedding(torch.tensor(target_id).unsqueeze(0))\n",
    "print('target_embedding: ', target_embedding.shape)\n",
    "\n",
    "emb_called_count = 0\n",
    "\n",
    "def edit_embedding(module, input, output):\n",
    "    global emb_called_count\n",
    "    print(\"embedding is called!\")\n",
    "    emb_called_count += 1\n",
    "    input = input[0]\n",
    "    print(f\"original_input:{input} {input.shape}\")\n",
    "    print(f\"original_output:{output.shape}\")\n",
    "    source_id_idx = input == source_id\n",
    "    output[source_id_idx,:] = target_embedding\n",
    "    print(f\"modified_output:{output.shape}\")\n",
    "    return output\n",
    "\n",
    "with LLMHooker(panel.mt, LLMHookerConfig('embedding', retain_output=False, edit_output=edit_embedding)) as hooker:\n",
    "    try:\n",
    "        gen_out3 = panel.mt.model.generate(input_ids=input_ids1, max_new_tokens=5)\n",
    "        print('gen_out3: ', gen_out3)\n",
    "    finally:\n",
    "        print('emb_called_count: ', emb_called_count)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(torch.tensor([1,2,3]).unsqueeze(0).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honesty Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import *\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "def calcu_score(neuron_act, reading_vectors, mean_vector, std_vector):\n",
    "    \"\"\"\n",
    "    reading_vectors: [layer, hidden_size]\n",
    "    mean_vector: [layer * hidden_size]\n",
    "    std_vector: [layer * hidden_size]\n",
    "    \"\"\"\n",
    "    neuron_act = neuron_act.transpose(0,1) # [seq_len, layer, hidden_size]\n",
    "    seq_len, layer, hidden_size = neuron_act.shape\n",
    "    neuron_act = neuron_act.reshape(seq_len, layer * hidden_size) # [seq_len, layer * hidden_size]\n",
    "    neuron_act = (neuron_act - mean_vector) / std_vector\n",
    "    neuron_act = neuron_act.reshape(seq_len, layer, hidden_size) # [seq_len, layer, hidden_size]\n",
    "    scores = (neuron_act * reading_vectors.repeat(seq_len, 1, 1)).sum(-1) # [seq_len, layer]\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def compute_reading_vectors(neuron_acts, sign=False):\n",
    "    '''\n",
    "    neuron_acts : List[Tuple[torch.Tensor [layer, window_size, hidden_size]]]\n",
    "    '''\n",
    "    diff = []\n",
    "    for (act1,act2) in neuron_acts:\n",
    "        l = min(act1.shape[1],act2.shape[1])\n",
    "        diff.append(act1[:,:l,:] - act2[:,:l,:])\n",
    "    diff = torch.cat(diff, dim=1).transpose(0,1) # [sum(seq_len), layer, hidden_size]\n",
    "    sample_size, n_layer, hidden_size = diff.shape\n",
    "    diff = diff.reshape(sample_size, -1) # [sample_size, layer * hidden_size]\n",
    "    mean_vector = torch.mean(diff, dim=0) # [layer * hidden_size]\n",
    "    std_vector = torch.std(diff, dim=0) # [layer * hidden_size]\n",
    "    diff = (diff.numpy() - mean_vector.numpy()) / std_vector.numpy()\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(diff)\n",
    "    reading_vectors = pca.components_[0].reshape(n_layer, hidden_size) # [layer, hidden_size]\n",
    "    # print('reading_vectors: ', reading_vectors.shape)\n",
    "    reading_vectors = torch.from_numpy(reading_vectors) # [layer, hidden_size]\n",
    "    if sign:\n",
    "        acts_t = torch.cat([act1 for (act1,act2) in neuron_acts], dim=1).transpose(0,1) # [sum(seq_len), layer, hidden_size]\n",
    "        acts_f = torch.cat([act2 for (act1,act2) in neuron_acts], dim=1).transpose(0,1)\n",
    "        acts_t = ((acts_t.reshape(acts_t.shape[0], -1) - mean_vector) / std_vector).reshape(acts_t.shape)\n",
    "        acts_f = ((acts_f.reshape(acts_f.shape[0], -1) - mean_vector) / std_vector).reshape(acts_f.shape)\n",
    "        t_score = (acts_t * reading_vectors.repeat(acts_t.shape[0], 1, 1)).mean()\n",
    "        f_score = (acts_f * reading_vectors.repeat(acts_f.shape[0], 1, 1)).mean()\n",
    "        if t_score < f_score:\n",
    "            reading_vectors = -reading_vectors\n",
    "    return reading_vectors, mean_vector, std_vector, pca.explained_variance_ratio_[0]\n",
    "\n",
    "\n",
    "def collect_neuron_acts(mt, dst, capture_window=(0,0), layers=None, local_bsz=32):\n",
    "    data_bsz = local_bsz // 2\n",
    "    dst = [dst[i:i+data_bsz] for i in range(0, len(dst), data_bsz)]\n",
    "    neuron_acts = []\n",
    "    for batch_pairs in tqdm(dst):\n",
    "        pairs = []\n",
    "        for pair in batch_pairs:\n",
    "            pairs += pair\n",
    "        prompt_lens = [len(mt.tok(s['input'])['input_ids']) for s in pairs]\n",
    "        seq_lens = [len(mt.tok(s['input']+s['output'])['input_ids']) for s in pairs]\n",
    "        with PaddingSide(mt.tok, 'right'):\n",
    "            input_ids = mt.tok([s['input']+s['output'] for s in pairs], return_tensors='pt', padding=True)['input_ids']\n",
    "        hook_configs = [LLMHookerConfig(module_name='block', layer=l) for l in layers]\n",
    "        with torch.no_grad(), LLMHooker(mt, hook_configs) as hooker:\n",
    "            mt.model(input_ids=input_ids.to(mt.model.device))\n",
    "            sentences_repr = torch.stack([h.outputs[0] for h in hooker.hooks]).transpose(0,1) # [bsz, layer, seq_len, hidden_size]\n",
    "        batch_neuron_acts = []\n",
    "        for i,repr in enumerate(sentences_repr):\n",
    "            prompt_len = prompt_lens[i]\n",
    "            seq_len = seq_lens[i]\n",
    "            start = prompt_len + capture_window[0] if capture_window[0] >= 0 else seq_len + capture_window[0]\n",
    "            end = prompt_len + capture_window[1] if capture_window[1] > 0 else seq_len + capture_window[1]\n",
    "            batch_neuron_acts.append(repr[:,start:end,:])\n",
    "        batch_neuron_acts = [[batch_neuron_acts[i],batch_neuron_acts[i+1]] for i in range(0, len(batch_neuron_acts), 2)]\n",
    "        neuron_acts.extend(batch_neuron_acts)\n",
    "    return neuron_acts # [layer, window_size, hidden_size]\n",
    "\n",
    "\n",
    "def evaluate_detection(mt, train_dst, test_dst, capture_window, compare_window, layers, local_bsz=64):\n",
    "    neuron_acts = collect_neuron_acts(mt, train_dst, capture_window, layers, local_bsz=local_bsz)\n",
    "    rv, mv, sv, importance = compute_reading_vectors(neuron_acts, sign=True)\n",
    "    test_neuron_acts = collect_neuron_acts(mt, test_dst, compare_window, layers, local_bsz=local_bsz)\n",
    "    scores = [[calcu_score(tna, rv, mv, sv),calcu_score(fna, rv, mv, sv)] for (tna, fna) in test_neuron_acts]\n",
    "    mean_diff = np.mean([s[0]-s[1] for s in scores])\n",
    "    acc = sum([1 for s in scores if s[0]>s[1]])/len(scores)\n",
    "    return acc, mean_diff, importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"re.ipynb\"\n",
    "\n",
    "mt = LLM.from_pretrained(model_path=\"/home/cs/yangyuchen/yushengliao/Medical_LLM/llama-2-7b-chat-hugging\").cuda(get_free_gpus()[0])\n",
    "\n",
    "tf_dst = json.load(open(\"data/true_false_dataset.json\"))\n",
    "\n",
    "# IID_Hard\n",
    "reserved_topic = \"Medical\"\n",
    "train_dst_iid_hard = [[td,fd] for (td,fd) in tf_dst if td['topic'] != \"Medical\"]\n",
    "test_dst_iid_hard = [[td,fd] for (td,fd) in tf_dst if td['topic'] == \"Medical\"]\n",
    "\n",
    "# IID_Weak\n",
    "random.shuffle(tf_dst)\n",
    "train_dst_iid_weak = tf_dst[:len(train_dst_iid_hard)]\n",
    "test_dst_iid_weak = tf_dst[len(train_dst_iid_hard):]\n",
    "\n",
    "# OOD\n",
    "reserved_topic = \"Medical\"\n",
    "prompt = \"USER:Tell me a fact.\\nAssistant:\"\n",
    "train_dst_ood = [[td,fd] for (td,fd) in tf_dst if td['topic'] != \"Medical\"]\n",
    "test_dst_ood = [[dict(input=prompt,output=td['output'],topic=td['topic'],label=True),\n",
    "                 dict(input=prompt,output=fd['output'],topic=fd['topic'],label=False)] \n",
    "                for (td,fd) in tf_dst if td['topic'] == \"Medical\"]\n",
    "\n",
    "# Layer Sweep\n",
    "def layer_sweep():\n",
    "    # Layer Sweep\n",
    "    config = {\n",
    "        \"capture_window\": (0,0),\n",
    "        \"compare_window\": (0,0),\n",
    "        \"local_bsz\": 32,\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config, \n",
    "            project=\"lat layer sweep\", \n",
    "            name=\"vicuna_33b_seed42\",\n",
    "            dir=\"output/lat_layer_sweep\",\n",
    "            job_type=\"inference\")\n",
    "\n",
    "    for l in range(mt.n_layer):\n",
    "        layers = [l]\n",
    "        acc, mean_diff, importance = evaluate_detection(mt=mt, train_dst=train_dst_iid_weak, test_dst=test_dst_iid_weak,layers=layers, **config)\n",
    "        print(f\"iid weak\\nimportance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")\n",
    "        wandb.log({\"iid weak\": acc if acc>0.5 else 1-acc, \"mean_diff\": abs(mean_diff), \"importance\": importance}, step=l)\n",
    "        acc, mean_diff, importance = evaluate_detection(mt=mt, train_dst=train_dst_iid_hard, test_dst=test_dst_iid_hard,layers=layers, **config)\n",
    "        print(f\"iid hard\\nimportance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")\n",
    "        wandb.log({\"iid hard\": acc if acc>0.5 else 1-acc, \"mean_diff\": abs(mean_diff), \"importance\": importance}, step=l)\n",
    "        acc, mean_diff, importance = evaluate_detection(mt=mt, train_dst=train_dst_ood, test_dst=test_dst_ood, layers=layers, **config)\n",
    "        print(f\"ood\\nimportance: {importance:.4f}\\nacc: {acc:.4f},\\nmean_diff: {mean_diff:.4f}\")\n",
    "        wandb.log({\"ood\": acc if acc>0.5 else 1-acc, \"mean_diff\": abs(mean_diff), \"importance\": importance}, step=l)\n",
    "\n",
    "layer_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honesty Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdd458a31eba722ab7b98fdf16d48af2e10cc66366abf20948be409b0a7311f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
